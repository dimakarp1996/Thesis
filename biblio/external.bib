@article{hochreiter_1997,
	title        = {Long short-term memory.},
	author       = {Hochreiter, S and Schmidhuber, J},
	year         = 1997,
	journal      = {Neural Computation},
	volume       = 9,
	number       = 8,
	pages        = {1735--1780},
	doi          = {10.1162/neco.1997.9.8.1735},
	url          = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	urldate      = {2018-08-14},
	pmid         = 9377276,
	abstract     = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}
@article{turing_1950,
	title        = {Computing Machinery and Intelligence},
	author       = {Turing, A. M.},
	year         = 1950,
	journal      = {Mind; a quarterly review of psychology and philosophy},
	volume       = {LIX},
	number       = 236,
	pages        = 433,
	doi          = {10.1093/mind/{LIX}.236.433},
	issn         = {0026-4423},
	url          = {http://mind.oxfordjournals.org/cgi/doi/10.1093/mind/LIX.236.433},
	urldate      = {2021-05-06}
}
@article{elman_1990,
	title        = {Finding Structure in Time},
	author       = {Elman, Jeffrey L.},
	year         = 1990,
	month        = 3,
	journal      = {Cognitive science},
	volume       = 14,
	number       = 2,
	pages        = {179--211},
	doi          = {10.1207/s15516709cog1402\_1},
	issn         = {03640213},
	url          = {http://doi.wiley.com/10.1207/s15516709cog1402\_1},
	urldate      = {2022-11-04}
}
@article{graves_2016,
	title        = {Hybrid computing using a neural network with dynamic external memory.},
	author       = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
	year         = 2016,
	month        = 10,
	day          = 27,
	journal      = {Nature},
	volume       = 538,
	number       = 7626,
	pages        = {471--476},
	doi          = {10.1038/nature20101},
	url          = {http://dx.doi.org/10.1038/nature20101},
	urldate      = {2017-05-08},
	pmid         = 27732574,
	abstract     = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer ({DNC}), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a {DNC} can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a {DNC} can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that {DNCs} have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read-write memory.}
}
@inproceedings{chen_2016,
	title        = {{XGBoost}: A Scalable Tree Boosting System},
	author       = {Chen, Tianqi and Guestrin, Carlos},
	year         = 2016,
	month        = 8,
	day          = 13,
	booktitle    = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining - {KDD} '16},
	publisher    = {{ACM} Press},
	address      = {New York, New York, {USA}},
	pages        = {785--794},
	doi          = {10.1145/2939672.2939785},
	isbn         = 9781450342322,
	url          = {http://dl.acm.org/citation.cfm?doid=2939672.2939785},
	urldate      = {2022-10-27},
	abstract     = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called {XGBoost}, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, {XGBoost} scales beyond billions of examples using far fewer resources than existing systems.}
}
@article{weizenbaum_1966,
	title        = {{ELIZA}---a computer program for the study of natural language communication between man and machine},
	author       = {Weizenbaum, Joseph},
	year         = 1966,
	month        = 1,
	day          = 1,
	journal      = {Communications of the {ACM}},
	volume       = 9,
	number       = 1,
	pages        = {36--45},
	doi          = {10.1145/365153.365168},
	issn         = {00010782},
	url          = {http://portal.acm.org/citation.cfm?doid=365153.365168},
	urldate      = {2022-10-27}
}
@inproceedings{pennington_2014,
	title        = {Glove: global vectors for word representation},
	author       = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year         = 2014,
	booktitle    = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {1532--1543},
	doi          = {10.3115/v1/D14-1162},
	url          = {http://aclweb.org/anthology/D14-1162},
	urldate      = {2022-10-27},
	abstract     = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.}
}
@inproceedings{collobert_2008,
	title        = {A unified architecture for natural language processing: Deep neural networks with multitask learning},
	author       = {Collobert, Ronan and Weston, Jason},
	year         = 2008,
	month        = 7,
	day          = 5,
	booktitle    = {Proceedings of the 25th international conference on Machine learning - {ICML} '08},
	publisher    = {{ACM} Press},
	address      = {New York, New York, {USA}},
	pages        = {160--167},
	doi          = {10.1145/1390156.1390177},
	isbn         = 9781605582054,
	url          = {http://portal.acm.org/citation.cfm?doid=1390156.1390177},
	urldate      = {2022-11-04}
}
@inproceedings{he_2016,
	title        = {Deep residual learning for image recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	month        = 6,
	day          = 27,
	booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher    = {IEEE},
	pages        = {770--778},
	doi          = {10.1109/{CVPR}.2016.90},
	isbn         = {978-1-4673-8851-1},
	url          = {http://ieeexplore.ieee.org/document/7780459/},
	urldate      = {2020-08-06},
	abstract     = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers\textemdash8× deeper than {VGG} nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions1, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.}
}
@incollection{caruana_1993,
	title        = {Multitask Learning: A Knowledge-Based Source of Inductive Bias},
	author       = {Caruana, Richard A.},
	year         = 1993,
	booktitle    = {Machine learning proceedings 1993},
	publisher    = {Elsevier},
	pages        = {41--48},
	doi          = {10.1016/B978-1-55860-307-3.50012-5},
	isbn         = 9781558603073,
	url          = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603073500125},
	urldate      = {2022-11-04}
}
@inproceedings{graves_2013,
	title        = {Hybrid speech recognition with Deep Bidirectional {LSTM}},
	author       = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
	year         = 2013,
	month        = 12,
	day          = 8,
	booktitle    = {2013 {IEEE} Workshop on Automatic Speech Recognition and Understanding},
	publisher    = {IEEE},
	pages        = {273--278},
	doi          = {10.1109/{ASRU}.2013.6707742},
	isbn         = {978-1-4799-2756-2},
	url          = {http://ieeexplore.ieee.org/document/6707742/},
	urldate      = {2022-11-04},
	abstract     = {Deep Bidirectional {LSTM} ({DBLSTM}) recurrent neural networks have recently been shown to give state-of-the-art performance on the {TIMIT} speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of {DBLSTM} as an acoustic model in a standard neural network-{HMM} hybrid system. We find that a {DBLSTM}-{HMM} hybrid gives equally good results on {TIMIT} as the previous work. It also outperforms both {GMM} and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with {DBLSTM} appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.}
}
@inproceedings{papineni_2001,
	title        = {{BLEU}: A method for automatic evaluation of machine translation},
	author       = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year         = 2001,
	booktitle    = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - {ACL} '02},
	publisher    = {Association for Computational Linguistics},
	address      = {Morristown, {NJ}, {USA}},
	pages        = 311,
	doi          = {10.3115/1073083.1073135},
	url          = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
	urldate      = {2022-11-04}
}
@inproceedings{peters_2018,
	title        = {Deep contextualized word representations},
	author       = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {2227--2237},
	doi          = {10.18653/v1/N18-1202},
	url          = {http://aclweb.org/anthology/N18-1202},
	urldate      = {2022-10-27},
	abstract     = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g. syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e. to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
}
@book{na_2018,
	title        = {The {NIPS} '17 competition: building intelligent systems},
	year         = 2018,
	publisher    = {Springer International Publishing},
	address      = {Cham},
	series       = {The springer series on challenges in machine learning},
	doi          = {10.1007/978-3-319-94042-7},
	isbn         = {978-3-319-94041-0},
	issn         = {2520-{131X}},
	url          = {http://link.springer.com/10.1007/978-3-319-94042-7},
	urldate      = {2022-10-27},
	editor       = {Escalera, Sergio and Weimer, Markus}
}
@inproceedings{miller_2016,
	title        = {Key-Value Memory Networks for Directly Reading Documents},
	author       = {Miller, Alexander and Fisch, Adam and Dodge, Jesse and Karimi, Amir-Hossein and Bordes, Antoine and Weston, Jason},
	year         = 2016,
	booktitle    = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {1400--1409},
	doi          = {10.18653/v1/D16-1147},
	url          = {http://aclweb.org/anthology/D16-1147},
	urldate      = {2022-11-04},
	abstract     = {Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering ({QA}) has been directed towards using Knowledge Bases ({KBs}) instead, which has proven effective. Unfortunately {KBs} often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using {KBs}, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, {WikiMovies}, a {QA} dataset that contains raw text alongside a preprocessed {KB}, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing {WikiQA} benchmark.}
}
@article{vrandei_2014,
	title        = {Wikidata: A Free Collaborative Knowledgebase},
	author       = {Vrandečić, Denny and Krötzsch, Markus},
	year         = 2014,
	month        = 9,
	day          = 23,
	journal      = {Communications of the {ACM}},
	volume       = 57,
	number       = 10,
	pages        = {78--85},
	doi          = {10.1145/2629489},
	issn         = {00010782},
	url          = {http://dl.acm.org/citation.cfm?doid=2661061.2629489},
	urldate      = {2022-10-27},
	abstract     = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.}
}
@inproceedings{kalchbrenner_2014,
	title        = {A convolutional neural network for modelling sentences},
	author       = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
	year         = 2014,
	booktitle    = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {655--665},
	doi          = {10.3115/v1/P14-1062},
	url          = {http://aclweb.org/anthology/P14-1062},
	urldate      = {2022-11-04},
	abstract     = {The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ({DCNN}) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the {DCNN} in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.}
}
@article{hochreiter_1998,
	title        = {The vanishing gradient problem during learning recurrent neural nets and problem solutions},
	author       = {Hochreiter, Sepp},
	year         = 1998,
	month        = 4,
	journal      = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	volume       = {06},
	number       = {02},
	pages        = {107--116},
	doi          = {10.1142/S0218488598000094},
	issn         = {0218-4885},
	url          = {http://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
	urldate      = {2022-11-20}
}
@inproceedings{zhu_2015,
	title        = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
	author       = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
	year         = 2015,
	month        = 12,
	day          = 7,
	booktitle    = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher    = {IEEE},
	pages        = {19--27},
	doi          = {10.1109/{ICCV}.2015.11},
	isbn         = {978-1-4673-8391-2},
	url          = {http://ieeexplore.ieee.org/document/7410368/},
	urldate      = {2022-11-20},
	abstract     = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware {CNN} to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.}
}
@inproceedings{lample_2016,
	title        = {Neural architectures for named entity recognition},
	author       = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
	year         = 2016,
	booktitle    = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {260--270},
	doi          = {10.18653/v1/N16-1030},
	url          = {http://aclweb.org/anthology/N16-1030},
	urldate      = {2022-10-27}
}
@inproceedings{zhang_2018,
	title        = {Personalizing Dialogue Agents: I have a dog, do you have pets too?},
	author       = {Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
	year         = 2018,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {2204--2213},
	doi          = {10.18653/v1/P18-1205},
	url          = {http://aclweb.org/anthology/P18-1205},
	urldate      = {2022-10-27},
	abstract     = {Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.}
}
@inproceedings{khatri_2018,
	title        = {Contextual topic modeling for dialog systems},
	author       = {Khatri, Chandra and Goel, Rahul and Hedayatnia, Behnam and Metanillou, Angeliki and Venkatesh, Anushree and Gabriel, Raefer and Mandal, Arindam},
	year         = 2018,
	month        = 12,
	day          = 18,
	booktitle    = {2018 {IEEE} Spoken Language Technology Workshop ({SLT})},
	publisher    = {IEEE},
	pages        = {892--899},
	doi          = {10.1109/{SLT}.2018.8639552},
	isbn         = {978-1-5386-4334-1},
	url          = {https://ieeexplore.ieee.org/document/8639552/},
	urldate      = {2022-10-27},
	abstract     = {Accurate prediction of conversation topics can be a valuable signal for creating coherent and engaging dialog systems. In this work, we focus on context-aware topic classification methods for identifying topics in free-form human-chatbot dialogs. We extend previous work on neural topic classification and unsupervised topic keyword detection by incorporating conversational context and dialog act features. On annotated data, we show that incorporating context and dialog acts leads to relative gains in topic classification accuracy by 35\% and on unsupervised keyword detection recall by 11\% for conversational interactions where topics frequently span multiple utterances. We show that topical metrics such as topical depth is highly correlated with dialog evaluation metrics such as coherence and engagement implying that conversational topic models can predict user satisfaction. Our work for detecting conversation topics and keywords can be used to guide chatbots towards coherent dialog.}
}
@inproceedings{wang_2018,
	title        = {{GLUE}: A multi-task benchmark and analysis platform for natural language understanding},
	author       = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 {EMNLP} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {353--355},
	doi          = {10.18653/v1/W18-5446},
	url          = {http://aclweb.org/anthology/W18-5446},
	urldate      = {2022-10-27}
}
@article{larsson_2000,
	title        = {Information state and dialogue management in the {TRINDI} dialogue move engine toolkit},
	author       = {Larsson, {STAFFAN} and Traum, {DAVID} R.},
	year         = 2000,
	month        = 9,
	journal      = {Natural language engineering},
	volume       = 6,
	number       = {3\&4},
	pages        = {323--340},
	doi          = {10.1017/S1351324900002539},
	issn         = 13513249,
	url          = {http://www.journals.cambridge.org/abstract\_S1351324900002539},
	urldate      = {2022-10-27}
}
@inproceedings{mintz_2009,
	title        = {Distant supervision for relation extraction without labeled data},
	author       = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
	year         = 2009,
	month        = 8,
	day          = 2,
	booktitle    = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}: Volume 2 - {ACL}-{IJCNLP} '09},
	publisher    = {Association for Computational Linguistics},
	address      = {Morristown, {NJ}, {USA}},
	pages        = 1003,
	doi          = {10.3115/1690219.1690287},
	isbn         = 9781932432466,
	url          = {http://portal.acm.org/citation.cfm?doid=1690219.1690287},
	urldate      = {2022-11-04}
}
@inproceedings{mtdnn,
	title        = {Multi-Task Deep Neural Networks for Natural Language Understanding},
	author       = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
	year         = 2019,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {4487--4496},
	doi          = {10.18653/v1/P19-1441},
	url          = {https://www.aclweb.org/anthology/P19-1441},
	urldate      = {2022-10-27}
}
@inproceedings{logeswaran_2019,
	title        = {Zero-Shot Entity Linking by Reading Entity Descriptions},
	author       = {Logeswaran, Lajanugen and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina and Devlin, Jacob and Lee, Honglak},
	year         = 2019,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {3449--3460},
	doi          = {10.18653/v1/P19-1335},
	url          = {https://www.aclweb.org/anthology/P19-1335},
	urldate      = {2022-10-27}
}
@inproceedings{rashkin_2019,
	title        = {Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset},
	author       = {Rashkin, Hannah and Smith, Eric Michael and Li, Margaret and Boureau, Y-Lan},
	year         = 2019,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {5370--5381},
	doi          = {10.18653/v1/P19-1534},
	url          = {https://www.aclweb.org/anthology/P19-1534},
	urldate      = {2022-10-27},
	abstract     = {One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others' feelings in a conversation, this is a significant challenge for {AI} systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and {EmpatheticDialogues}, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model.}
}
@inproceedings{reimers_2019,
	title        = {Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
	author       = {Reimers, Nils and Gurevych, Iryna},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {3973--3983},
	doi          = {10.18653/v1/D19-1410},
	url          = {https://www.aclweb.org/anthology/D19-1410},
	urldate      = {2022-10-27}
}
@article{zhou_2020,
	title        = {The design and implementation of xiaoice, an empathetic social chatbot},
	author       = {Zhou, Li and Gao, Jianfeng and Li, Di and Shum, Heung-Yeung},
	year         = 2020,
	month        = 3,
	journal      = {Computational Linguistics},
	volume       = 46,
	number       = 1,
	pages        = {53--93},
	doi          = {10.1162/coli\_a\_00368},
	issn         = {0891-2017},
	url          = {https://www.mitpressjournals.org/doi/abs/10.1162/coli\_a\_00368},
	urldate      = {2022-10-27},
	abstract     = {This article describes the development of Microsoft {XiaoIce}, the most popular social chatbot in the world. {XiaoIce} is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human–machine social chat as decision-making over Markov Decision Processes, and optimize {XiaoIce} for long-term user engagement, measured in expected Conversation-turns Per Session ({CPS}). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how {XiaoIce} dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, {XiaoIce} has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that {XiaoIce} has achieved an average {CPS} of 23, which is significantly higher than that of other chatbots and even human conversations.}
}
@article{roy_2021,
	title        = {Efficient Content-Based Sparse Attention with Routing Transformers},
	author       = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
	year         = 2021,
	month        = 2,
	journal      = {Transactions of the Association for Computational Linguistics},
	volume       = 9,
	pages        = {53--68},
	doi          = {10.1162/tacl\_a\_00353},
	issn         = {2307-{387X}},
	url          = {https://direct.mit.edu/tacl/article/97776},
	urldate      = {2022-10-27},
	abstract     = {Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O( n 1.5 d) from O( n 2 d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on {ImageNet}-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released {PG}-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow. 1}
}
@book{chomsky_1957,
	title        = {Syntactic Structures},
	author       = {Chomsky, Noam},
	year         = 1957,
	month        = 12,
	day          = 31,
	publisher    = {De Gruyter},
	doi          = {10.1515/9783112316009},
	isbn         = 9783112316009,
	url          = {https://www.degruyter.com/document/doi/10.1515/9783112316009/html},
	urldate      = {2022-11-04}
}
@article{devlin_2018,
	title        = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year         = 2018,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1810.04805},
	url          = {https://arxiv.org/abs/1810.04805},
	urldate      = {2022-03-05},
	abstract     = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@article{zhang_2019,
	title        = {{DialoGPT}: Large-Scale Generative Pre-training for Conversational Response Generation},
	author       = {Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill},
	year         = 2019,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1911.00536},
	url          = {https://arxiv.org/abs/1911.00536},
	urldate      = {2022-10-27},
	abstract     = {We present a large, tunable neural conversational response generation model, {DialoGPT} (dialogue generative pre-trained transformer). Trained on {147M} conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, {DialoGPT} extends the Hugging Face {PyTorch} transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage {DialoGPT} generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.}
}
@article{vaswani_2017,
	title        = {Attention Is All You Need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1706.03762},
	url          = {https://arxiv.org/abs/1706.03762},
	urldate      = {2022-11-15},
	abstract     = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.}
}
@article{kingma_2014,
	title        = {Adam: A Method for Stochastic Optimization},
	author       = {Kingma, Diederik P. and Ba, Jimmy},
	year         = 2014,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1412.6980},
	url          = {https://arxiv.org/abs/1412.6980},
	urldate      = {2022-05-09},
	abstract     = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.}
}
@inproceedings{lewis_2020,
	title        = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
	author       = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
	year         = 2020,
	booktitle    = {Proceedings of the 58th annual meeting of the association for computational linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {7871--7880},
	doi          = {10.18653/v1/2020.acl-main.703},
	url          = {https://www.aclweb.org/anthology/2020.acl-main.703},
	urldate      = {2022-10-27}
}
@inproceedings{bosselut_2019,
	title        = {{COMET}: commonsense transformers for automatic knowledge graph construction},
	author       = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
	year         = 2019,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {4762--4779},
	doi          = {10.18653/v1/P19-1470},
	url          = {https://www.aclweb.org/anthology/P19-1470},
	urldate      = {2022-10-27}
}
@article{mikolov_2013,
	title        = {Efficient Estimation of Word Representations in Vector Space},
	author       = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year         = 2013,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1301.3781},
	url          = {https://arxiv.org/abs/1301.3781},
	urldate      = {2022-11-04},
	abstract     = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.}
}
@article{sutskever_2014,
	title        = {Sequence to Sequence Learning with Neural Networks},
	author       = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	year         = 2014,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1409.3215},
	url          = {https://arxiv.org/abs/1409.3215},
	urldate      = {2022-08-30},
	abstract     = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difficult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a fixed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}'14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}'s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difficulty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}'s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.}
}
@article{shavrina_2020,
	title        = {{RussianSuperGLUE}: A Russian Language Understanding Evaluation Benchmark},
	author       = {Shavrina, Tatiana and Fenogenova, Alena and Anton, Emelyanov and Denis, Shevelev and Ekaterina, Artemova and Malykh, Valentin and Vladislav, Mikhailov and Maria, Tikhonova and Chertok, Andrey and Evlampiev, Andrey},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2010.15925},
	urldate      = {2022-10-27}
}
@misc{massive,
	title        = {MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages},
	author       = {Jack FitzGerald and Christopher Hench and Charith Peris and Scott Mackie and Kay Rottmann and Ana Sanchez and Aaron Nash and Liam Urbach and Vishesh Kakarala and Richa Singh and Swetha Ranganath and Laurie Crist and Misha Britan and Wouter Leeuwis and Gokhan Tur and Prem Natarajan},
	year         = 2022,
	eprint       = {2204.08582},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{ru_emotions,
	title        = {Data-Driven Model for Emotion Detection in Russian Texts},
	author       = {Sboev, Alexander and Naumov, Aleksandr and Rybka, Roman},
	year         = 2021,
	journal      = {Procedia Computer Science},
	publisher    = {Elsevier},
	volume       = 190,
	pages        = {637--642},
	url          = {https://huggingface.co/datasets/cedr}
}
@article{khot2018scitail,
	title        = {SciTaiL: A Textual Entailment Dataset from Science Question Answering},
	author       = {Khot, Tushar and Sabharwal, Ashish and Clark, Peter},
	year         = 2018,
	month        = {Apr.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 32,
	number       = 1,
	doi          = {10.1609/aaai.v32i1.12022},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/12022},
	abstractnote = {&lt;p&gt; We present a new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem. SciTail is the first entailment set that is created solely from natural sentences that already exist independently ``in the wild’’ rather than sentences authored specifically for the entailment task. Different from existing entailment datasets, we create hypotheses from science questions and the corresponding answer candidates, and premises from relevant web sentences retrieved from a large corpus. These sentences are often linguistically challenging. This, combined with the high lexical similarity of premise and hypothesis for both entailed and non-entailed pairs, makes this new entailment task particularly difficult. The resulting challenge is evidenced by state-of-the-art textual entailment systems achieving mediocre performance on SciTail, especially in comparison to a simple majority class baseline. As a step forward, we demonstrate that one can improve accuracy on SciTail by 5\% using a new neural model that exploits linguistic structure. &lt;/p&gt;}
}
@inproceedings{ksquad,
	title        = {Exploring the Bert Cross-Lingual Transfer for Reading Comprehension},
	author       = {Konovalov, Vasily and Gulyaev, Pavel and Sorokin, Alexey and Kuratov, Yury and Burtsev, Mikhail},
	year         = 2020,
	booktitle    = {Komp'juternaja Lingvistika i Intellektual'nye Tehnologiithis},
	pages        = {445--453},
	doi          = {10.28995/2075-7182-2020-19-445-453},
	isbn         = {978-5-7281-2947-9},
	url          = {http://www.dialog-21.ru/media/5100/konovalovvpplusetal-118.pdf}
}
@article{xglue,
	title        = {XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},
	author       = {Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos and Rangan Majumder and Ming Zhou},
	year         = 2020,
	journal      = {arXiv},
	volume       = {abs/2004.01401}
}
@misc{https://doi.org/10.48550/arxiv.1812.08989,
	title        = {The Design and Implementation of XiaoIce, an Empathetic Social Chatbot},
	author       = {Zhou, Li and Gao, Jianfeng and Li, Di and Shum, Heung-Yeung},
	year         = 2018,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.1812.08989},
	url          = {https://arxiv.org/abs/1812.08989},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Human-Computer Interaction (cs.HC), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{mlsum,
	title        = {MLSUM: The Multilingual Summarization Corpus},
	author       = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.14900}
}
@inproceedings{ag_news_and_yahoo,
	title        = {Character-level Convolutional Networks for Text Classification},
	author       = {Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
	year         = 2015,
	booktitle    = {NIPS}
}
@article{summarizer,
	title        = {Variations of the Similarity Function of TextRank for Automated Summarization},
	author       = {Federico Barrios and Federico L{\'{o}}pez and Luis Argerich and Rosa Wachenchauzer},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1602.03606},
	url          = {http://arxiv.org/abs/1602.03606},
	archiveprefix = {arXiv},
	eprint       = {1602.03606},
	timestamp    = {Wed, 07 Jun 2017 14:40:43 +0200},
	biburl       = {https://dblp.org/rec/bib/journals/corr/BarriosLAW16},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wochat,
	title        = {Collecting {Better} {Training} {Data} using {Biased} {Agent} {Policies} in {Negotiation} {Dialogues}},
	author       = {Konovalov, Vasily and Melamud, Oren and Artstein, Ron and Dagan, Ido},
	year         = 2016,
	month        = sep,
	booktitle    = {Proceedings of {WOCHAT}, the {Second} {Workshop} on {Chatbots} and {Conversational} {Agent} {Technologies}},
	publisher    = {Zerotype},
	address      = {Los Angeles},
	url          = {http://workshop.colips.org/wochat/documents/RP-270.pdf},
	abstract     = {When naturally occurring data is characterized by a highly skewed class distribution, supervised learning often benefits from reducing this skew. Human-agent dialogue data is commonly highly skewed when using standard agent policies. Hence, we suggest that agent policies need to be reconsidered in the context of training data collection. Specifically, in this work we implemented biased agent policies that are optimized for data collection in the negotiation domain. Empirical evaluations show that our method is successful in collecting a reasonably balanced corpus in the highly skewed Job-Candidate domain. Furthermore, using this balanced corpus to train a negotiation intent classifier yields notable performance improvements relative to naturally distributed data.},
	keywords     = {Virtual Humans}
}
@inproceedings{squad,
	title        = {Exploring the Bert Cross-Lingual Transfer for Reading Comprehension},
	author       = {Konovalov, Vasily and Gulyaev, Pavel and Sorokin, Alexey and Kuratov, Yury and Burtsev, Mikhail},
	year         = 2020,
	booktitle    = {Komp'juternaja Lingvistika i Intellektual'nye Tehnologiithis},
	pages        = {445--453},
	doi          = {10.28995/2075-7182-2020-19-445-453},
	isbn         = {978-5-7281-2947-9},
	url          = {http://www.dialog-21.ru/media/5100/konovalovvpplusetal-118.pdf}
}
@misc{aaai,
	title        = {Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker},
	author       = {Gulyaev, Pavel and Elistratova, Eugenia and Konovalov, Vasily and Kuratov, Yuri and Pugachev, Leonid and Burtsev, Mikhail},
	year         = 2020,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2002.02450},
	url          = {https://arxiv.org/abs/2002.02450},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@inproceedings{negochat,
	title        = {The Negochat Corpus of Human-agent Negotiation Dialogues},
	author       = {Konovalov, Vasily  and Artstein, Ron  and Melamud, Oren  and Dagan, Ido},
	year         = 2016,
	month        = may,
	booktitle    = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)},
	publisher    = {European Language Resources Association (ELRA)},
	address      = {Portoro{\v{z}}, Slovenia},
	pages        = {3141--3145},
	url          = {https://aclanthology.org/L16-1501},
	abstract     = {Annotated in-domain corpora are crucial to the successful development of dialogue systems of automated agents, and in particular for developing natural language understanding (NLU) components of such systems. Unfortunately, such important resources are scarce. In this work, we introduce an annotated natural language human-agent dialogue corpus in the negotiation domain. The corpus was collected using Amazon Mechanical Turk following the {`}Wizard-Of-Oz{'} approach, where a {`}wizard{'} human translates the participants{'} natural language utterances in real time into a semantic language. Once dialogue collection was completed, utterances were annotated with intent labels by two independent annotators, achieving high inter-annotator agreement. Our initial experiments with an SVM classifier show that automatically inferring such labels from the utterances is far from trivial. We make our corpus publicly available to serve as an aid in the development of dialogue systems for negotiation agents, and suggest that analogous corpora can be created following our methodology and using our available source code. To the best of our knowledge this is the first publicly available negotiation dialogue corpus.}
}
@misc{chatbotru,
	title        = {Chatbot-ru: Russian intent and topic classification dataset},
	author       = {Ilya Koziev},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/Koziev/chatbot/blob/master/data/intents.txt}},
	commit       = {1fefb34ca2af56155b64a2b2a54b2b81a8399658}
}
@misc{multilingual_bert,
	title        = {Official description of the multilingual BERT models from Google Research},
	author       = {Jacob Devlin, Slav Petrov},
	year         = 2019,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/google-research/bert/blob/master/multilingual.md}},
	commit       = {cc7051dc592802f501e8a6f71f8fb3cf9de95dc9}
}
@inproceedings{dp_topics,
	title        = {DeepPavlov Topics: Topic Classification Dataset for Conversational Domain in English},
	author       = {Sagyndyk, Beksultan and Baymurzina, Dilyara and Burtsev, Mikhail},
	year         = 2023,
	booktitle    = {Advances in Neural Computation, Machine Learning, and Cognitive Research VI},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {371--380},
	isbn         = {978-3-031-19032-2},
	editor       = {Kryzhanovsky, Boris and Dunin-Barkowski, Witali and Redko, Vladimir and Tiumentsev, Yury},
	abstract     = {This paper presents ``DeepPavlov Topics``, a new dataset for topic classification in conversational domain. The dataset was collected and filtered automatically from web-sites and open datasets. We identify 33 topics, and present full (4.2M samples) and down-sampled (2.2M samples) versions of the ``DeepPavlov Topics''. The proposed topics are aimed to cover conversational domain in details but maintain interpretability. We report baseline classification results trained in multi-label setup to allow multiple classes per text during inference. We also release pre-trained models for topic classification including distilled and multi-lingual versions.}
}
@article{distilbert,
	title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper and lighter},
	author       = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1910.01108},
	url          = {http://arxiv.org/abs/1910.01108},
	eprinttype   = {arXiv},
	eprint       = {1910.01108},
	timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{hupd,
	title        = {The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications},
	author       = {Suzgun, Mirac and Melas-Kyriazi, Luke and Sarkar, Suproteem K. and Kominers, Scott Duke and Shieber, Stuart M.},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2207.04043},
	url          = {https://arxiv.org/abs/2207.04043}
}
@inproceedings{lexglue,
	title        = {LexGLUE: A Benchmark Dataset for Legal Language Understanding in English},
	author       = {Chalkidis, Ilias and Jana, Abhik and Hartung, Dirk and Bommarito, Michael and Androutsopoulos, Ion and Katz, Daniel Martin and Aletras, Nikolaos},
	year         = 2022,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
	address      = {Dubln, Ireland}
}
@misc{lextreme,
	title        = {LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain},
	author       = {Joel Niklaus and Veton Matoshi and Pooja Rani and Andrea Galassi and Matthias Stürmer and Ilias Chalkidis},
	year         = 2023,
	eprint       = {2301.13126},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@article{guardian_authorship,
	title        = {On the robustness of authorship attribution based on character n-gram features},
	author       = {Stamatatos, Efstathios},
	year         = 2013,
	month        = {01},
	journal      = {Journal of Law and Policy},
	volume       = 21,
	pages        = {421--439}
}
@misc{blbooksgenre,
	title        = {19th Century Books - metadata with additional crowdsourced annotations},
	author       = {{British Library} and  Morris, Victoria and van Strien, Daniel and Tolfo, Giorgia and Afric, Lora and Robertson, Stewart and Tiney, Patricia and Dogterom, Annelies and Wollner, Ildi},
	year         = 2021,
	url          = {https://doi.org/10.23636/BKHQ-0312}
}
@inproceedings{amazon_reviews,
	title        = {The Multilingual Amazon Reviews Corpus},
	author       = {Keung, Phillip and Lu, Yichao and Szarvas, György and Smith, Noah A.},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}
}
@misc{kill_me_please,
	title        = {Dataset from Russian website "Kill me please"},
	year         = 2022,
	note         = {Accessed: 2023-02-17},
	howpublished = {\url{https://huggingface.co/datasets/takiholadi/kill-me-please-dataset}}
}
@misc{healthcare_facilities_reviews,
	title        = {Dataset of Russian reviews about medical facilities},
	author       = {Pavel Blinov},
	year         = 2022,
	note         = {Accessed: 2023-02-17},
	howpublished = {\url{https://huggingface.co/datasets/blinoff/healthcare\_facilities\_reviews}}
}
@misc{yandex_q,
	title        = {Yandex Que Service},
	year         = 2023,
	note         = {Accessed: 2023-02-17},
	howpublished = {\url{https://yandex.ru/q/}}
}
@misc{sbert_large_nlu_ru,
	title        = {Training of natural language model with BERT and Tensorflow},
	author       = {SberDevices team},
	year         = 2020,
	note         = {HuggingFace model link: \url{https://huggingface.co/sberbank-ai/sbert\_large\_nlu\_ru}. Accessed: 2023-02-17.},
	howpublished = {\url{https://habr.com/ru/company/sberdevices/blog/527576}}
}
@inproceedings{buryat,
	title        = {Learning word embeddings for low resource languages: the case of Buryat},
	author       = {Konovalov, VP and Tumunbayarova, ZB},
	year         = 2018,
	booktitle    = {Komp'juternaja Lingvistika i Intellektual'nye Tehnologii},
	pages        = {331--341},
	url          = {http://www.dialog-21.ru/media/4528/konovalovvp\_tumunbayarovazb.pdf}
}
@article{rubert,
	title        = {Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language},
	author       = {Yuri Kuratov and Mikhail Y. Arkhipov},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1905.07213},
	url          = {http://arxiv.org/abs/1905.07213},
	eprinttype   = {arXiv},
	eprint       = {1905.07213},
	timestamp    = {Tue, 17 Aug 2021 08:52:50 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1905-07213.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{distilrubert,
	title        = {Knowledge Distillation of Russian Language Models with Reduction of Vocabulary},
	author       = {Kolesnikova, Alina and Kuratov, Yuri and Konovalov, Vasily and Burtsev, Mikhail},
	year         = 2022,
	publisher    = {arXiv},
	doi          = {10.48550/ARXIV.2205.02340},
	url          = {https://arxiv.org/abs/2205.02340},
	copyright    = {arXiv.org perpetual, non-exclusive license},
	keywords     = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@misc{sbert_base,
	title        = {ruT5, ruRoBERTa, ruBERT: How we trained a series of models for the Russian-language},
	author       = {SberDevices},
	year         = 2021,
	note         = {HuggingFace model link: \url{https://huggingface.co/sberbank-ai/ruBert-base}. Accessed: 2023-02-17.},
	howpublished = {\url{https://habr.com/ru/company/sberbank/blog/567776/}}
}
@inproceedings{huggingface_transformers,
	title        = {Transformers: State-of-the-Art Natural Language Processing},
	author       = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
	year         = 2020,
	month        = oct,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {38--45},
	url          = {https://www.aclweb.org/anthology/2020.emnlp-demos.6}
}
@article{dbpedia,
	title        = {DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia},
	author       = {Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, Sören and Bizer, Christian},
	year         = 2014,
	month        = {01},
	journal      = {Semantic Web Journal},
	volume       = 6,
	pages        = {},
	doi          = {10.3233/SW-140134}
}
@article{GradTS,
	title        = {GradTS: {A} Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks},
	author       = {Weicheng Ma and Renze Lou and Kai Zhang and Lili Wang and Soroush Vosoughi},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2109.05748},
	url          = {https://arxiv.org/abs/2109.05748},
	eprinttype   = {arXiv},
	eprint       = {2109.05748},
	timestamp    = {Tue, 21 Sep 2021 17:46:04 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2109-05748.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{GhostBERT2021,
	title        = {GhostBERT: Generate More Features with Cheap Operations for BERT},
	author       = {Huang, Zhiqi and Hou, Lu and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
	year         = 2021,
	month        = {01},
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
	pages        = {6512--6523},
	doi          = {10.18653/v1/2021.acl-long.509}
}
@inproceedings{ru_sentiment,
	title        = {Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks},
	author       = {Sergey Smetanin and Michail Komarov},
	year         = 2019,
	month        = {07},
	booktitle    = {2019 IEEE 21st Conference on Business Informatics (CBI)},
	volume       = {01},
	number       = {},
	pages        = {482--486},
	doi          = {10.1109/CBI.2019.00062},
	issn         = {2378-1963},
	url          = {https://github.com/sismetanin/rureviews}
}
@inproceedings{TaskEmbedded2021,
	title        = {Multitask Learning Using {BERT} with Task-Embedded Attention},
	author       = {Maziarka, Lukasz and Danel, Tomasz},
	year         = 2021,
	month        = 7,
	day          = 18,
	booktitle    = {2021 International Joint Conference on Neural Networks ({IJCNN})},
	publisher    = {IEEE},
	pages        = {1--6},
	doi          = {10.1109/{IJCNN52387}.2021.9533990},
	isbn         = {978-1-6654-3900-8},
	url          = {https://ieeexplore.ieee.org/document/9533990/},
	urldate      = {2022-10-27}
}
@inproceedings{el-nouby2021xcit,
	title        = {XCiT: Cross-Covariance Image Transformers},
	author       = {Ali, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Herve},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 34,
	pages        = {20014--20027},
	url          = {https://proceedings.neurips.cc/paper/2021/file/a655fbe4b8d7439994aa37ddad80de56-Paper.pdf},
	editor       = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan}
}
@article{sentiment,
	title        = {DynaSent: {A} Dynamic Benchmark for Sentiment Analysis},
	author       = {Christopher Potts and Zhengxuan Wu and Atticus Geiger and Douwe Kiela},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2012.15349},
	url          = {https://arxiv.org/abs/2012.15349},
	eprinttype   = {arXiv},
	eprint       = {2012.15349},
	timestamp    = {Fri, 08 Jan 2021 17:23:09 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2012-15349.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{toxic,
	title        = {Measuring and Mitigating Unintended Bias in Text Classification( paper for Wiki Talk dataset, cleaned version of the dataset retrieved from \url{https://huggingface.co/datasets/OxAISH-AL-LLM/wiki\_toxic})},
	author       = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
	location     = {New Orleans, LA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {AIES '18},
	pages        = {67–73},
	doi          = {10.1145/3278721.3278729},
	isbn         = 9781450360128,
	url          = {https://doi.org/10.1145/3278721.3278729},
	abstract     = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
	numpages     = 7,
	keywords     = {text classification, fairness, algorithmic bias, natural language processing, machine learning}
}
@article{ru_toxic,
	title        = {Russian toxicity dataset from 2ch.hk. Dataset retrieved from \url{https://github.com/s-nlp/rudetoxifier}},
	author       = {Daryna Dementieva and Daniil Moskovskiy and Varvara Logacheva and David Dale and Olga Kozlova and Nikita Semenov and Alexander Panchenko},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2105.09052},
	url          = {https://arxiv.org/abs/2105.09052},
	eprinttype   = {arXiv},
	eprint       = {2105.09052},
	timestamp    = {Thu, 14 Oct 2021 09:16:24 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2105-09052.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{slurp,
	title        = {{SLURP}: A Spoken Language Understanding Resource Package},
	author       = {Bastianelli, Emanuele  and Vanzo, Andrea  and Swietojanski, Pawel  and Rieser, Verena},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {7252--7262},
	doi          = {10.18653/v1/2020.emnlp-main.588},
	url          = {https://aclanthology.org/2020.emnlp-main.588},
	abstract     = {Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.}
}
@inproceedings{stickland_2019,
	title        = {{BERT} and {PALs}: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning},
	author       = {Stickland, Asa Cooper and Murray, Iain},
	year         = 2019,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	volume       = 97,
	pages        = {5986:5995},
	url          = {https://arxiv.org/abs/1902.02671},
	urldate      = {2022-10-27}
}
@article{arachie_2021,
	title        = {Constrained Labeling for Weakly Supervised Learning},
	author       = {Arachie, Chidubem and Huang, Bert},
	year         = 2021,
	journal      = {{arXiv} preprint {arXiv}:2009.07360},
	urldate      = {2022-10-27}
}
@article{pilault_2020,
	title        = {Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in {NLP} Using Fewer Parameters \& Less Data},
	author       = {Pilault, Jonathan and Elhattami, Amine and Pal, Christopher},
	year         = 2020,
	urldate      = {2022-10-27}
}
@article{worsham_2020,
	title        = {Multi-task learning for natural language processing in the 2020s: where are we going?},
	author       = {Worsham, Joseph and Kalita, Jugal},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2007.16008},
	url          = {https://arxiv.org/abs/2007.16008},
	urldate      = {2022-10-27}
}
@article{chen_2021,
	title        = {Multi-Task Learning in Natural Language Processing: An Overview},
	author       = {Chen, Shijie and Zhang, Yu and Yang, Qiang},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2109.09138},
	url          = {https://arxiv.org/abs/2109.09138},
	urldate      = {2022-10-27}
}
@article{caruana_1997,
	title        = {Multitask learning},
	author       = {Caruana, Rich},
	year         = 1997,
	journal      = {Machine learning},
	publisher    = {Springer},
	volume       = 28,
	number       = 1,
	pages        = {41--75},
	urldate      = {2022-10-27}
}
@article{mtdnn_improve,
	title        = {Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding},
	author       = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1904.09482},
	url          = {http://arxiv.org/abs/1904.09482},
	urldate      = {2022-10-27}
}
@article{wang_2021,
	title        = {Entailment as Few-Shot Learner},
	author       = {Wang, Sinong and Fang, Han and Khabsa, Madian and Mao, Hanzi and Ma, Hao},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2104.14690},
	url          = {https://arxiv.org/abs/2104.14690},
	urldate      = {2022-10-27}
}
@inproceedings{maziarka_2021,
	title        = {Multitask Learning Using {BERT} with Task-Embedded Attention},
	author       = {Maziarka, Lukasz and Danel, Tomasz},
	year         = 2021,
	month        = 7,
	day          = 18,
	booktitle    = {2021 International Joint Conference on Neural Networks ({IJCNN})},
	publisher    = {IEEE},
	pages        = {1--6},
	doi          = {10.1109/{IJCNN52387}.2021.9533990},
	isbn         = {978-1-6654-3900-8},
	url          = {https://ieeexplore.ieee.org/document/9533990/},
	urldate      = {2022-10-27}
}
@inproceedings{wang_2018a,
	title        = {Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning},
	author       = {Wang, Weichao and Feng, Shi and Gao, Wei and Wang, Daling and Zhang, Yifei},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {338--348},
	doi          = {10.18653/v1/D18-1031},
	url          = {http://aclweb.org/anthology/D18-1031},
	urldate      = {2022-10-27}
}
@inproceedings{zhao_2020,
	title        = {{SpanMlt}: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction},
	author       = {Zhao, He and Huang, Longtao and Zhang, Rong and Lu, Quan and xue, hui},
	year         = 2020,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {3239--3248},
	doi          = {10.18653/v1/2020.acl-main.296},
	url          = {https://www.aclweb.org/anthology/2020.acl-main.296},
	urldate      = {2022-10-27}
}
@inproceedings{magooda_2021,
	title        = {Exploring Multitask Learning for Low-Resource Abstractive Summarization},
	author       = {Magooda, Ahmed and Litman, Diane and Elaraby, Mohamed},
	year         = 2021,
	booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP} 2021},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {1652--1661},
	doi          = {10.18653/v1/2021.findings-emnlp.142},
	url          = {https://aclanthology.org/2021.findings-emnlp.142},
	urldate      = {2022-10-27}
}
@inproceedings{huang_2021,
	title        = {{GhostBERT}: Generate More Features with Cheap Operations for {BERT}},
	author       = {Huang, Zhiqi and Hou, Lu and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
	year         = 2021,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {6512--6523},
	doi          = {10.18653/v1/2021.acl-long.509},
	url          = {https://aclanthology.org/2021.acl-long.509},
	urldate      = {2022-10-27}
}
@article{wang_2019,
	title        = {{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
	author       = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1905.00537},
	url          = {http://arxiv.org/abs/1905.00537},
	urldate      = {2022-10-27}
}
@inproceedings{aroyehun_2018,
	title        = {Aggression Detection in Social Media: Using Deep Neural Networks,Data Augmentation, and Pseudo Labeling.Proceedings of the First Workshop on Trolling},
	author       = {Aroyehun, Segun Taofeek and Gelbukh, Alexander},
	year         = 2018,
	booktitle    = {Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying},
	pages        = {90:97},
	url          = {https://www.aclweb.org/anthology/W18-4411.pdf},
	urldate      = {2022-10-27}
}
@inproceedings{pentina_2017,
	title        = {Multi-Task Learning with Labeled and Unlabeled Tasks},
	author       = {Pentina, Anastasia and Lampert, Christoph H.},
	year         = 2017,
	booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
	volume       = 70,
	pages        = {2807:2816},
	url          = {http://proceedings.mlr.press/v70/pentina17a.html},
	urldate      = {2022-10-27}
}
@inproceedings{gottumukkala_2020,
	title        = {Dynamic Sampling Strategies for Multi-Task Reading Comprehension},
	author       = {Gottumukkala, Ananth and Dua, Dheeru and Singh, Sameer and Gardner, Matt},
	year         = 2020,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {920--924},
	doi          = {10.18653/v1/2020.acl-main.86},
	url          = {https://www.aclweb.org/anthology/2020.acl-main.86},
	urldate      = {2022-10-27}
}
@article{kuratov_2019,
	title        = {Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language},
	author       = {Kuratov, Yuri and Arkhipov, Mikhail Y.},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1905.07213},
	url          = {http://arxiv.org/abs/1905.07213},
	urldate      = {2022-10-27}
}
@inproceedings{mller_2020,
	title        = {When Does Label Smoothing Help?},
	author       = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
	year         = 2020,
	booktitle    = {Proceedings of the 33th {NeurIPS}},
	url          = {https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf},
	urldate      = {2022-10-27}
}
@article{choromanski_2020,
	title        = {Rethinking Attention with Performers},
	author       = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlós, Tamás and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy J. and Weller, Adrian},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2009.14794},
	url          = {https://arxiv.org/abs/2009.14794},
	urldate      = {2022-10-27}
}
@article{kitaev_2020,
	title        = {Reformer: The Efficient Transformer},
	author       = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2001.04451},
	url          = {https://arxiv.org/abs/2001.04451},
	urldate      = {2022-10-27}
}
@article{wang_2020,
	title        = {Linformer: Self-Attention with Linear Complexity},
	author       = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2006.04768},
	url          = {https://arxiv.org/abs/2006.04768},
	urldate      = {2022-10-27}
}
@article{jaszczur_2021,
	title        = {Sparse is Enough in Scaling Transformers},
	author       = {Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, Lukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2111.12763},
	url          = {https://arxiv.org/abs/2111.12763},
	urldate      = {2022-10-27}
}
@article{pilault_2020a,
	title        = {Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in {NLP} Using Fewer Parameters \& Less Data},
	author       = {Pilault, Jonathan and Elhattami, Amine and Pal, Christopher J.},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2009.09139},
	url          = {https://arxiv.org/abs/2009.09139},
	urldate      = {2022-10-27}
}
@article{lu_2019,
	title        = {12-in-1: Multi-Task Vision and Language Representation Learning},
	author       = {Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1912.02315},
	url          = {http://arxiv.org/abs/1912.02315},
	urldate      = {2022-10-27}
}
@inproceedings{sundararaman_2021,
	title        = {Learning task sampling policy for multitask learning},
	author       = {Sundararaman, Dhanasekar and Tsai, Henry and Lee, Kuang-Huei and Turc, Iulia and Carin, Lawrence},
	year         = 2021,
	booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP} 2021},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {4410--4415},
	doi          = {10.18653/v1/2021.findings-emnlp.375},
	url          = {https://aclanthology.org/2021.findings-emnlp.375},
	urldate      = {2022-10-27}
}
@article{ma_2021,
	title        = {{GradTS}: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks},
	author       = {Ma, Weicheng and Lou, Renze and Zhang, Kai and Wang, Lili and Vosoughi, Soroush},
	year         = 2021,
	journal      = {CoRR},
	volume       = {abs/2109.05748},
	url          = {https://arxiv.org/abs/2109.05748},
	urldate      = {2022-10-27}
}
@book{chafe_1994,
	title        = {Discourse, consciousness, and time: The flow and displacement of conscious experience in speaking and writing},
	author       = {Chafe, Wallace},
	year         = 1994,
	publisher    = {University of Chicago Press},
	urldate      = {2022-10-27}
}
@article{nogueira_2019,
	title        = {Passage Re-ranking with {BERT}},
	author       = {Nogueira, Rodrigo and Cho, Kyunghyun},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1901.04085},
	urldate      = {2022-10-27}
}
@article{scenariosa,
	title        = {'{ScenarioSA}: A Large Scale Conversational Database for Interactive Sentiment Analysis'},
	author       = {Yazhou Zhang, Lingling Song, Dawei Song and others},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1907.05562},
	urldate      = {2022-10-27}
}
@article{nouhadziri_2019,
	title        = {'Evaluating Coherence in Dialogue Systems using Entailment'},
	author       = {Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1904.03371},
	urldate      = {2022-10-27}
}
@article{macavaney_2019,
	title        = {{CEDR}: Contextualized Embeddings for Document Ranking},
	author       = {{MacAvaney}, Sean and Yates, Andrew and Cohan, Arman and Goharian, Nazli},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1904.07094},
	url          = {http://arxiv.org/abs/1904.07094},
	urldate      = {2022-10-27}
}
@article{peters_2018a,
	title        = {Deep contextualized word representations},
	author       = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1802.05365},
	url          = {http://arxiv.org/abs/1802.05365},
	urldate      = {2022-10-27}
}
@article{radford_2018_gpt,
	title        = {Improving language understanding by generative pre-training},
	author       = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year         = 2018,
	journal      = {{URL} https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf},
	urldate      = {2022-10-27}
}
@article{personachat,
	title        = {Personalizing Dialogue Agents: I have a dog, do you have pets too?},
	author       = {Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1801.07243},
	url          = {http://arxiv.org/abs/1801.07243},
	urldate      = {2022-10-27}
}
@article{reddy_2018,
	title        = {{CoQA}: A Conversational Question Answering Challenge},
	author       = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1808.07042},
	url          = {http://arxiv.org/abs/1808.07042},
	urldate      = {2022-10-27}
}
@article{choi_2018,
	title        = {{QuAC} : Question Answering in Context},
	author       = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1808.07036},
	url          = {http://arxiv.org/abs/1808.07036},
	urldate      = {2022-10-27}
}
@article{song_2019,
	title        = {{MA\SS}: Masked Sequence to Sequence Pre-training for Language Generation},
	author       = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1905.02450},
	urldate      = {2022-10-27}
}
@article{dong_2019,
	title        = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
	author       = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1905.03197},
	urldate      = {2022-10-27}
}
@article{dai_2019,
	title        = {Transformer-{XL}: Attentive Language Models Beyond a Fixed-Length Context},
	author       = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G. and Le, Quoc V. and Salakhutdinov, Ruslan},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1901.02860},
	url          = {http://arxiv.org/abs/1901.02860},
	urldate      = {2022-10-27}
}
@article{dehghani_2018,
	title        = {Universal Transformers},
	author       = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1807.03819},
	url          = {http://arxiv.org/abs/1807.03819},
	urldate      = {2022-10-27}
}
@article{radford_2019,
	title        = {Language models are unsupervised multitask learners},
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year         = 2019,
	journal      = {{OpenAI} Blog},
	volume       = 1,
	pages        = 8,
	urldate      = {2022-10-27}
}
@article{wolf_2019,
	title        = {{TransferTransfo}: A Transfer Learning Approach for Neural Network Based Conversational Agents},
	author       = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1901.08149},
	url          = {http://arxiv.org/abs/1901.08149},
	urldate      = {2022-10-27}
}
@inproceedings{dinan_2019,
	title        = {Wizard of Wikipedia: Knowledge-powered Conversational Agents},
	author       = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
	year         = 2019,
	booktitle    = {Proceedings of the International Conference on Learning Representations ({ICLR})},
	urldate      = {2022-10-27}
}
@article{nguyen_2016,
	title        = {{MS} {MARCO}: A human generated machine reading comprehension dataset},
	author       = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
	year         = 2016,
	journal      = {{arXiv} preprint {arXiv}:1611.09268},
	urldate      = {2022-10-27}
}
@inproceedings{burtsev_2018,
	title        = {{DeepPavlov}: Open-Source Library for Dialogue Systems},
	author       = {Burtsev, Mikhail and Seliverstov, Alexander and Airapetyan, Rafael and Arkhipov, Mikhail and Baymurzina, Dilyara and Bushkov, Nickolay and Gureenkova, Olga and Khakhulin, Taras and Kuratov, Yuri and Kuznetsov, Denis and Litinsky, Alexey and Logacheva, Varvara and Lymar, Alexey and Malykh, Valentin and Petrov, Maxim and Polulyakh, Vadim and Pugachev, Leonid and Sorokin, Alexey and Vikhreva, Maria and Zaynutdinov, Marat},
	year         = 2018,
	booktitle    = {Proceedings of {ACL} 2018, System Demonstrations},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {122--127},
	doi          = {10.18653/v1/P18-4021},
	url          = {http://aclweb.org/anthology/P18-4021},
	urldate      = {2022-10-27}
}
@misc{xiao_other_2018,
	title        = {{BERT}-as-service},
	author       = {Xiao, Han},
	year         = 2018,
	urldate      = {2022-10-27},
	type         = {OTHER}
}
@article{gabriel_2020,
	title        = {Further advances in open domain dialog systems in the third alexa prize socialbot grand challenge},
	author       = {Gabriel, Raefer and Liu, Yang and Gottardi, Anna and Eric, Mihail and Khatri, Anju and Chadha, Anjali and Chen, Qinlang and Hedayatnia, Behnam and Rajan, Pankaj and Binici, Ali and Others},
	year         = 2020,
	journal      = {Alexa Prize Proceedings},
	urldate      = {2022-10-27}
}
@article{liang_2020,
	title        = {Gunrock 2.0 : A User Adaptive Social Conversational System},
	author       = {Liang, Kaihui and Chau, Austin and Li, Yu and Lu, Xueyuan and Yu, Dian and Zhou, Mingyang and Jain, Ishan and Davidson, Sam and Arnold, Josh and Nguyet, Minh and Yu, Zhou},
	year         = 2020,
	urldate      = {2022-10-27}
}
@article{paranjape_2020,
	title        = {Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-Initiative Conversations},
	author       = {Paranjape, Ashwin and See, Abigail and Kenealy, Kathleen and Li, Haojun and Hardy, Amelia and Qi, Peng and Sadagopan, Kaushik Ram and Phu, Nguyet Minh and Soylu, Dilara and Manning, Christopher},
	year         = 2020,
	url          = {http://arxiv.org/abs/2008.12348},
	urldate      = {2022-10-27}
}
@article{liu_2019b,
	title        = {Roberta: A robustly optimized bert pretraining approach},
	author       = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1907.11692},
	urldate      = {2022-10-27}
}
@article{raffel_2020,
	title        = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	author       = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	year         = 2020,
	journal      = {Journal of Machine Learning Research},
	volume       = 21,
	number       = 140,
	pages        = {1--67},
	urldate      = {2022-10-27}
}
@article{shoeybi_2019,
	title        = {Megatron-lm: Training multi-billion parameter language models using gpu model parallelism},
	author       = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and {LeGresley}, Patrick and Casper, Jared and Catanzaro, Bryan},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1909.08053},
	urldate      = {2022-10-27}
}
@article{brown_2020,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Others},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2005.14165},
	urldate      = {2022-10-27}
}
@incollection{yang_2019,
	title        = {{XLNet}: Generalized Autoregressive Pretraining for Language Understanding},
	author       = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32},
	publisher    = {Curran Associates, Inc.},
	pages        = {5753--5763},
	url          = {http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf},
	urldate      = {2022-10-27},
	editor       = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch}é-Buc, F. and Fox, E. and Garnett, R}
}
@inproceedings{gopalakrishnan_2019,
	title        = {Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations},
	author       = {Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen and others},
	year         = 2019,
	month        = 9,
	day          = 15,
	booktitle    = {Interspeech 2019},
	publisher    = {ISCA},
	address      = {ISCA},
	pages        = {1891--1895},
	doi          = {10.21437/Interspeech.2019-3079},
	url          = {http://www.isca-speech.org/archive/Interspeech\_2019/abstracts/3079.html},
	urldate      = {2022-10-27}
}
@article{guu_2020,
	title        = {Realm: Retrieval-augmented language model pre-training},
	author       = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2002.08909},
	urldate      = {2022-10-27}
}
@article{verga_2020,
	title        = {Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge},
	author       = {Verga, Pat and Sun, Haitian and Soares, Livio Baldini and Cohen, William W},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2007.00849},
	urldate      = {2022-10-27}
}
@article{lewis_2020a,
	title        = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
	author       = {Patrick Lewis and others},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2005.11401},
	urldate      = {2022-10-27}
}
@article{speer_2016,
	title        = {{ConceptNet} 5.5: An Open Multilingual Graph of General Knowledge},
	author       = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
	year         = 2016,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1612.03975},
	url          = {https://arxiv.org/abs/1612.03975},
	urldate      = {2022-10-27},
	abstract     = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource {ConceptNet} that is particularly well suited to be used with modern {NLP} techniques such as word embeddings. {ConceptNet} is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When {ConceptNet} is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as {WordNet} or {DBPedia}. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving {SAT}-style analogies.}
}
@misc{logacheva_other_2018,
	title        = {A dataset of topic-oriented human-to-chatbot dialogues},
	author       = {Logacheva, Varvara and Burtsev, Mikhail and Malykh, Valentin and Poluliakh, Vadim and Rudnicky, Alexander and Serban, Iulian and Lowe, Ryan and Prabhumoye, Shrimai and Black, Alan W and Bengio, Yoshua},
	year         = 2018,
	urldate      = {2022-10-27},
	type         = {OTHER}
}
@incollection{logacheva_2020,
	title        = {{ConvAI2} dataset of non-goal-oriented human-to-bot dialogues},
	author       = {Logacheva, Varvara and Malykh, Valentin and Litinsky, Aleksey and Burtsev, Mikhail},
	year         = 2020,
	booktitle    = {The {NeurIPS}'18 Competition},
	publisher    = {Springer, Cham},
	pages        = {277--294},
	urldate      = {2022-10-27}
}
@incollection{dinan_2020,
	title        = {The second conversational intelligence challenge (convai2)},
	author       = {Dinan, Emily and Logacheva, Varvara and Malykh, Valentin and Miller, Alexander and Shuster, Kurt and Urbanek, Jack and Kiela, Douwe and Szlam, Arthur and Serban, Iulian and Lowe, Ryan and Others},
	year         = 2020,
	booktitle    = {The {NeurIPS}'18 Competition},
	publisher    = {Springer, Cham},
	pages        = {187--208},
	urldate      = {2022-10-27}
}
@article{yazhouzhang_2019a,
	title        = {{ScenarioSA}: A Large Scale Conversational Database for Interactive Sentiment Analysis},
	author       = {Yazhou Zhang, Lingling Song, Dawei Song, Peng Guo, Junwei Zhang, Peng Zhang},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1907.05562},
	urldate      = {2022-10-27}
}
@article{nouhadziri_2019a,
	title        = {'Evaluating Coherence in Dialogue Systems using Entailment'},
	author       = {Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane},
	year         = 2019,
	journal      = {Proceedings of {NAACL}},
	urldate      = {2022-10-27}
}
@inproceedings{macavaney_2019a,
	title        = {{CEDR}: Contextualized embeddings for document ranking},
	author       = {{MacAvaney}, Sean and Yates, Andrew and Cohan, Arman and Goharian, Nazli},
	year         = 2019,
	booktitle    = {Proceedings of the 42nd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	pages        = {1101--1104},
	urldate      = {2022-10-27}
}
@article{wow,
	title        = {Wizard of Wikipedia: Knowledge-Powered Conversation Agents},
	author       = {Emily Dinan, Stephen Roller and others},
	year         = 2018,
	journal      = {Proceedings of {ICLR}},
	urldate      = {2022-10-27}
}
@article{topicalchat,
	title        = {Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations},
	author       = {Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen and others},
	year         = 2019,
	journal      = {Proceedings of Interspeech},
	urldate      = {2022-10-27}
}
@inproceedings{song_2019a,
	title        = {{MA\SS}: Masked Sequence to Sequence Pre-training for Language Generation},
	author       = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {5926--5936},
	urldate      = {2022-10-27}
}
@article{dong_2019a,
	title        = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
	author       = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
	year         = 2019,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1905.03197},
	url          = {https://arxiv.org/abs/1905.03197},
	urldate      = {2022-10-27},
	abstract     = {This paper presents a new Unified pre-trained Language Model ({UniLM}) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. {UniLM} compares favorably with {BERT} on the {GLUE} benchmark, and the {SQuAD} 2.0 and {CoQA} question answering tasks. Moreover, {UniLM} achieves new state-of-the-art results on five natural language generation datasets, including improving the {CNN}/{DailyMail} abstractive summarization {ROUGE}-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization {ROUGE}-L to 35.75 (0.86 absolute improvement), the {CoQA} generative question answering F1 score to 82.5 (37.1 absolute improvement), the {SQuAD} question generation {BLEU}-4 to 22.12 (3.75 absolute improvement), and the {DSTC7} document-grounded dialog response generation {NIST}-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.}
}
@article{dinan_2018,
	title        = {Wizard of wikipedia: Knowledge-powered conversational agents},
	author       = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
	year         = 2018,
	journal      = {{arXiv} preprint {arXiv}:1811.01241},
	urldate      = {2022-10-27}
}
@misc{johannes_other_2020,
	title        = {Personality Prediction from Text},
	author       = {Johannes, Wieser},
	year         = 2020,
	urldate      = {2022-10-27},
	type         = {OTHER}
}
@misc{liang_other_2020,
	title        = {Gunrock 2.0: A User Adaptive Social Conversational System},
	author       = {Liang, Kaihui and Chau, Austin and Li, Yu and Lu, Xueyuan and Yu, Dian and Zhou, Mingyang and Jain, Ishan and Davidson, Sam and Arnold, Josh and Nguyen, Minh and Yu, Zhou},
	year         = 2020,
	urldate      = {2022-10-27},
	type         = {OTHER}
}
@article{bowden_2019,
	title        = {{SlugBot}: Developing a Computational Model and Framework of a Novel Dialogue Genre},
	author       = {Bowden, Kevin K. and Wu, {JiaQi} and Cui, Wen and Juraska, Juraj and Harrison, Vrindavan and Schwarzmann, Brian and Santer, Nick and Walker, Marilyn A.},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1907.10658},
	url          = {http://arxiv.org/abs/1907.10658},
	urldate      = {2022-10-27}
}
@inproceedings{prasad_2008,
	title        = {The Penn Discourse {TreeBank} 2.0.},
	author       = {Prasad, Rashmi and Dinesh, Nikhil and Lee, Alan and Miltsakaki, Eleni and Robaldo, Livio and Joshi, Aravind and Webber, Bonnie},
	year         = 2008,
	month        = 5,
	booktitle    = {Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC\textquoteright08})},
	publisher    = {European Language Resources Association ({ELRA})},
	address      = {Marrakech, Morocco},
	url          = {http://www.lrec-conf.org/proceedings/lrec2008/pdf/754\_paper.pdf},
	urldate      = {2022-10-27},
	abstract     = {We present the second version of the Penn Discourse Treebank, {PDTB}-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between {PDTB}-1.0 and {PDTB}-2.0. We present representative statistics for several aspects of the annotation in the corpus.}
}
@inproceedings{eggins_1996,
	title        = {Analysing Casual Conversation},
	author       = {Eggins, S. and Slade, D.},
	year         = 1996,
	urldate      = {2022-10-27}
}
@incollection{halliday_nd,
	title        = {Language as code and language as behaviour: a systemic-functional interpretation of the nature and ontogenesis of dialogue},
	author       = {Halliday, M. A. K.},
	publisher    = {Bloomsbury Academic},
	address      = {London},
	series       = {Linguistics: Bloomsbury Academic Collections},
	pages        = {3--36},
	isbn         = {978-1-4742-8573-5},
	url          = {http://www.bloomsburycollections.com/book/semiotics-of-culture-and-language-volume-1-language-as-social-semiotic/ch1-language-as-code-and-language-as-behaviour-a-systemic-functional-interpretation-of-the-nature-and-ontogenesis-of-dialogue/},
	urldate      = {2022-10-27},
	edition      = 1
}
@book{halliday_2004,
	title        = {An introduction to functional grammar / M.A.K. Halliday},
	author       = {Halliday, M. A. K. and Matthiessen, Christian M. I. M.},
	year         = 2004,
	publisher    = {Hodder Arnold London},
	pages        = {x, 689 p. :},
	isbn         = {0340761679 9780340761670},
	urldate      = {2022-10-27},
	edition      = {3rd ed. / rev. by Christian M.I.M. Matthiessen.}
}
@book{hasan_1985,
	title        = {Discourse on Discourse. Workshop Reports from the Macquarie Workshop on Discourse Analysis (Sydney, New South Wales, Australia, February 21-25, 1983). Occasional Papers Number 7.},
	author       = {Hasan, Ruqaiya},
	year         = 1985,
	publisher    = {ERIC},
	urldate      = {2022-10-27}
}
@book{glimm_2012,
	title        = {{KI} 2012: Advances in Artificial Intelligence},
	author       = {Glimm, Birte and Krüger, ‎Antonio},
	year         = 2012,
	publisher    = {Springer Nature},
	address      = {Berlin},
	pages        = 1,
	isbn         = {978-3-642-33347-7},
	urldate      = {2022-10-27},
	editor       = {Glimm, Birte and Krüger, Antonio},
	edition      = 1,
	abstract     = {An approach of improving the small talk capabilities of an existing virtual agent architecture is presented. Findings in virtual agent research revealed the need to pay attention to the sophisticated structures found in (human) casual conversations. In particular, existing dialogue act tag sets lack of tags adequately reflecting the subtle structures found in small talk. The approach presented here structures dialogues on two different levels. The micro level consists of meta information (speech functions) that dialogue acts can be tagged with. The macro level is concerned with ordering individual dialogue acts into sequences. The extended dialogue engine allows for a fine-grained selection of responses, enabling the agent to produce varied small talk sequences.}
}
@article{dropout,
	title        = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	author       = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	year         = 2014,
	journal      = {Journal of Machine Learning Research},
	volume       = 15,
	number       = 56,
	pages        = {1929--1958},
	url          = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@incollection{clemson_2013,
	title        = {Five-Factor Model of Personality},
	author       = {Clemson, Lindy and Turner, J. Rick and Turner, J. Rick and Jacquez, Farrah and Raglin, Whitney and Reed, Gabriela and Reed, Gabriela and Limmer, Jane and Floyd, Serina and Reed, Gabriela and Graber, Elana and Beveridge, Ryan M. and Randall, Ashley K. and Bodenmann, Guy and Turner, J. Rick and Malik, Neena and Jent, Jason and Parker, Alyssa and Wang, Jenny T. and Newman, Sarah J. and Beveridge, Ryan M. and Graber, Elana and Wang, Jenny T. and Carrillo, Adriana and Gomez-Meade, Carley and Carrillo, Adriana and Gomez-Meade, Carley and Harlapur, Manjunath and Shimbo, Daichi and Campbell, Tavis S. and Johnson, Jillian A. and Zernicke, Kristin A. and Flannery, Kelly and Monteros, Karla Espinosa and Friedberg, Fred and Harlapur, Manjunath and Shimbo, Daichi and Gidron, Yori and Turner, J. Rick and Rosenberg, Leah and Rosenberg, Leah and Morizio, Alexandre and Bacon, Simon and Chmielewski, Michael S. and Morgan, Theresa A. and Daigre, Amber and Powell, Lynda H. and Janssen, Imke and Killianova, Tereza and Gidron, Yori and Wawrzyniak, Andrew J. and Wawrzyniak, Andrew J. and Brintz, Carrie and Sebastiano, M. Di Katie and Moriguchi, Yoshiya and Ando, Tetusya and Söderback, Ingrid},
	year         = 2013,
	booktitle    = {Encyclopedia of behavioral medicine},
	publisher    = {Springer New York},
	address      = {New York, {NY}},
	pages        = {803--804},
	doi          = {10.1007/978-1-4419-1005-9\_1226},
	isbn         = {978-1-4419-1004-2},
	url          = {http://link.springer.com/10.1007/978-1-4419-1005-9\_1226},
	urldate      = {2022-10-27},
	editor       = {Gellman, Marc D. and Turner, J. Rick}
}
@misc{ram_other_2018,
	title        = {Conversational {AI}: The Science Behind the Alexa Prize},
	author       = {Ram, Ashwin and Prasad, Rohit and Khatri, Chandra and Venkatesh, Anu and Gabriel, Raefer and Liu, Qing and Nunn, Jeff and Hedayatnia, Behnam and Cheng, Ming and Nagar, Ashish and King, Eric and Bland, Kate and Wartick, Amanda and Pan, Yi and Song, Han and Jayadevan, Sk and Hwang, Gene and Pettigrue, Art},
	year         = 2018,
	urldate      = {2022-10-27},
	type         = {OTHER}
}
@inproceedings{socher_2013,
	title        = {Recursive deep models for semantic compositionality over a sentiment treebank},
	author       = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
	year         = 2013,
	booktitle    = {Proceedings of the 2013 conference on empirical methods in natural language processing},
	pages        = {1631--1642},
	urldate      = {2022-10-27}
}
@inproceedings{danescuniculescumizil_2011,
	title        = {Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.},
	author       = {Danescu-Niculescu-Mizil, Cristian and Lee, Lillian},
	year         = 2011,
	booktitle    = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, {ACL} 2011},
	url          = {https://www.cs.cornell.edu/\~cristian/Cornell\_Movie-Dialogs\_Corpus.html},
	urldate      = {2022-10-27}
}
@inproceedings{yanranli_2017,
	title        = {{DailyDialog}: A Manually Labelled Multi-turn Dialogue Dataset},
	author       = {Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao and Niu, Shuzi},
	year         = 2017,
	booktitle    = {Proceedings of The 8th International Joint Conference on Natural Language Processing ({IJCNLP} 2017)},
	url          = {http://yanran.li/dailydialog.html},
	urldate      = {2022-10-27}
}
@article{sang_2003,
	title        = {Introduction to the {CoNLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
	author       = {Sang, Erik F. Tjong Kim and De Meulder, Fien},
	year         = 2003,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.cs/0306050},
	url          = {https://arxiv.org/abs/cs/0306050},
	urldate      = {2022-10-27},
	abstract     = {We describe the {CoNLL}-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.}
}
@article{huang_2015,
	title        = {Bidirectional {LSTM}-{CRF} Models for Sequence Tagging},
	author       = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
	year         = 2015,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1508.01991},
	url          = {https://arxiv.org/abs/1508.01991},
	urldate      = {2022-10-27},
	abstract     = {In this paper, we propose a variety of Long Short-Term Memory ({LSTM}) based models for sequence tagging. These models include {LSTM} networks, bidirectional {LSTM} ({BI}-{LSTM}) networks, {LSTM} with a Conditional Random Field ({CRF}) layer ({LSTM}-{CRF}) and bidirectional {LSTM} with a {CRF} layer ({BI}-{LSTM}-{CRF}). Our work is the first to apply a bidirectional {LSTM} {CRF} (denoted as {BI}-{LSTM}-{CRF}) model to {NLP} benchmark sequence tagging data sets. We show that the {BI}-{LSTM}-{CRF} model can efficiently use both past and future input features thanks to a bidirectional {LSTM} component. It can also use sentence level tag information thanks to a {CRF} layer. The {BI}-{LSTM}-{CRF} model can produce state of the art (or close to) accuracy on {POS}, chunking and {NER} data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.}
}
@inproceedings{strubell_2017,
	title        = {Fast and Accurate Entity Recognition with Iterated Dilated Convolutions},
	author       = {Strubell, Emma and Verga, Patrick and Belanger, David and {McCallum}, Andrew},
	year         = 2017,
	booktitle    = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {2670--2680},
	doi          = {10.18653/v1/D17-1283},
	url          = {http://aclweb.org/anthology/D17-1283},
	urldate      = {2022-10-27}
}
@inproceedings{xu_2017,
	title        = {A local detection approach for named entity recognition and mention detection},
	author       = {Xu, Mingbin and Jiang, Hui and Watcharawittayakul, Sedtawut},
	year         = 2017,
	booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {1237--1247},
	doi          = {10.18653/v1/P17-1114},
	url          = {http://aclweb.org/anthology/P17-1114},
	urldate      = {2022-10-27}
}
@inproceedings{gangluo_2015,
	title        = {Joint named entity recognition and disambiguation},
	author       = {Gang Luo, Xiaojiang Huang, Chin-Yew Lin, Zaiqing Nie},
	year         = 2015,
	booktitle    = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	pages        = {879--888},
	urldate      = {2022-10-27}
}
@inproceedings{passos_2014,
	title        = {Lexicon infused phrase embeddings for named entity resolution},
	author       = {Passos, Alexandre and Kumar, Vineet and {McCallum}, Andrew},
	year         = 2014,
	booktitle    = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {78--86},
	doi          = {10.3115/v1/W14-1609},
	url          = {http://aclweb.org/anthology/W14-1609},
	urldate      = {2022-10-27}
}
@incollection{wang_2017,
	title        = {Named Entity Recognition with Gated Convolutional Neural Networks},
	author       = {Wang, Chunqi and Chen, Wei and Xu, Bo},
	year         = 2017,
	booktitle    = {Chinese computational linguistics and natural language processing based on naturally annotated big data},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	series       = {Lecture notes in computer science},
	volume       = 10565,
	pages        = {110--121},
	doi          = {10.1007/978-3-319-69005-6\_10},
	isbn         = {978-3-319-69004-9},
	issn         = {0302-9743},
	url          = {http://link.springer.com/10.1007/978-3-319-69005-6\_10},
	urldate      = {2022-10-27},
	editor       = {Sun, Maosong and Wang, Xiaojie and Chang, Baobao and Xiong, Deyi}
}
@incollection{wallace_2009,
	title        = {The anatomy of {ALICE}},
	author       = {Wallace, Richard S},
	year         = 2009,
	booktitle    = {Parsing the Turing Test},
	publisher    = {Springer},
	pages        = {181--210},
	urldate      = {2022-10-27}
}
@inproceedings{yi_2019,
	title        = {Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators},
	author       = {Yi, Sanghyun and Goel, Rahul and Khatri, Chandra and Cervone, Alessandra and Chung, Tagyoung and Hedayatnia, Behnam and Venkatesh, Anu and Gabriel, Raefer and Hakkani-Tur, Dilek},
	year         = 2019,
	booktitle    = {Proceedings of the 12th International Conference on Natural Language Generation},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {65--75},
	doi          = {10.18653/v1/W19-8608},
	url          = {https://www.aclweb.org/anthology/W19-8608},
	urldate      = {2022-10-27}
}
@article{henderson_2019,
	title        = {{ConveRT}: Efficient and Accurate Conversational Representations from Transformers},
	author       = {Matthew Henderson and Iñigo Casanueva and Nikola Mrkšić and Pei-Hao Su and Tsung-Hsien Wen and Ivan Vulić},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1911.03688},
	urldate      = {2022-10-27}
}
@incollection{ke_2017,
	title        = {{LightGBM}: A Highly Efficient Gradient Boosting Decision Tree},
	author       = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems 30},
	publisher    = {Curran Associates, Inc.},
	pages        = {3146--3154},
	url          = {http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf},
	urldate      = {2022-10-27},
	editor       = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.}
}
@article{wolf_2019a,
	title        = {Transfertransfo: A transfer learning approach for neural network based conversational agents},
	author       = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1901.08149},
	urldate      = {2022-10-27}
}
@article{dai_2015,
	title        = {Semi-supervised Sequence Learning},
	author       = {Dai, Andrew M. and Le, Quoc V.},
	year         = 2015,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1511.01432},
	url          = {https://arxiv.org/abs/1511.01432},
	urldate      = {2022-10-27},
	abstract     = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as {IMDB}, {DBpedia} and 20 Newsgroups.}
}
@article{cer_2018,
	title        = {Universal Sentence Encoder},
	author       = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
	year         = 2018,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1803.11175},
	url          = {https://arxiv.org/abs/1803.11175},
	urldate      = {2022-10-27},
	abstract     = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other {NLP} tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests ({WEAT}) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on {TF} Hub.}
}
@article{khatri_2018a,
	title        = {Detecting Offensive Content in Open-domain Conversations using Two Stage Semi-supervision},
	author       = {Khatri, Chandra and Hedayatnia, Behnam and Goel, Rahul and Venkatesh, Anushree and Gabriel, Raefer and Mandal, Arindam},
	year         = 2018,
	journal      = {ArXiv},
	volume       = {abs/1811.12900},
	urldate      = {2022-10-27}
}
@article{khatri_2018b,
	title        = {Advancing the State of the Art in Open Domain Dialog Systems through the Alexa Prize},
	author       = {Khatri, Chandra and Hedayatnia, Behnam and Venkatesh, Anu and Nunn, Jeff and Pan, Yi and Liu, Qihan and Song, Han and Gottardi, Anna and Kwatra, Sanjeev and Pancholi, Sanju and Cheng, Ming and Chen, Qinglang and Stubel, Lauren and Gopalakrishnan, Karthik and Bland, Kate and Gabriel, Raefer and Mandal, Arindam and Hakkani-Tür, Dilek Z. and Hwang, Gene and Michel, Nate and King, Eric and Prasad, Rohit},
	year         = 2018,
	journal      = {ArXiv},
	volume       = {abs/1812.10757},
	urldate      = {2022-10-27}
}
@article{defreitasadiwardana_2020,
	title        = {Towards a Human-like Open-Domain Chatbot},
	author       = {De Freitas Adiwardana, Daniel and Luong, Minh-Thang and So, David R. and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and Le, Quoc V.},
	year         = 2020,
	journal      = {ArXiv},
	volume       = {abs/2001.09977},
	urldate      = {2022-10-27}
}
@article{pichl_2018,
	title        = {Alquist 2.0: Alexa Prize Socialbot Based on Sub-Dialogue Models},
	author       = {Pichl, Jan and Marek, Petr and Konrád, Jakub and Matulk, Martin and Šedivy, Jan},
	year         = 2018,
	journal      = {2nd Proceedings of Alexa Prize (Alexa Prize 2018)},
	urldate      = {2022-10-27}
}
@article{chen_2018,
	title        = {Gunrock: Building a human-like social bot by leveraging large scale real user data},
	author       = {Chen, Chun-Yen and Yu, Dian and Wen, Weiming and Yang, Yi Mang and Zhang, Jiaping and Zhou, Mingyang and Jesse, Kevin and Chau, Austin and Bhowmick, Antara and Iyer, Shreenath and Others},
	year         = 2018,
	journal      = {2nd Proceedings of Alexa Prize (Alexa Prize 2018)},
	urldate      = {2022-10-27}
}
@inproceedings{welleck_2019,
	title        = {Dialogue natural language inference},
	author       = {Welleck, Sean and Weston, Jason and Szlam, Arthur and Cho, Kyunghyun},
	year         = 2019,
	booktitle    = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {3731--3741},
	doi          = {10.18653/v1/P19-1363},
	url          = {https://www.aclweb.org/anthology/P19-1363},
	urldate      = {2022-10-27}
}
@misc{roller_other_2020,
	title        = {Recipes for building an open-domain chatbot},
	author       = {Roller, Stephen and Dinan, Emily and Goyal, Naman and Da, Ju and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Shuster, Kurt and Smith, Eric M. and Boureau, Y-Lan and Weston, Jason},
	year         = 2020,
	urldate      = {2022-10-27},
	type         = {OTHER}
}
@article{hedayatnia_2020,
	title        = {Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems},
	author       = {Hedayatnia, Behnam and Kim, Seokhwan and Liu, Yang and Gopalakrishnan, Karthik and Eric, Mihail and Hakkani-Tur, Dilek},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2005.12529},
	urldate      = {2022-10-27}
}
@inproceedings{wu_2020,
	title        = {Scalable Zero-shot Entity Linking with Dense Entity Retrieval},
	author       = {Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {6397--6407},
	doi          = {10.18653/v1/2020.emnlp-main.519},
	url          = {https://www.aclweb.org/anthology/2020.emnlp-main.519},
	urldate      = {2022-10-27}
}
@inproceedings{ultes_2017,
	title        = {Pydial: A multi-domain statistical dialogue system toolkit},
	author       = {Ultes, Stefan and Barahona, Lina M Rojas and Su, Pei-Hao and Vandyke, David and Kim, Dongho and Casanueva, Inigo and Budzianowski, Pawe and Mrkšic, Nikola and Wen, Tsung-Hsien and Gasic, Milica and Others},
	year         = 2017,
	booktitle    = {Proceedings of {ACL} 2017, System Demonstrations},
	pages        = {73--78},
	urldate      = {2022-10-27}
}
@inproceedings{jang_2019,
	title        = {{PyOpenDial}: a python-based domain-independent toolkit for developing spoken dialogue systems with probabilistic rules},
	author       = {Jang, Youngsoo and Lee, Jongmin and Park, Jaeyoung and Lee, Kyeng-Hun and Lison, Pierre and Kim, Kee-Eung},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ({EMNLP}-{IJCNLP}): system demonstrations},
	pages        = {187--192},
	urldate      = {2022-10-27}
}
@inproceedings{kiefer_2021,
	title        = {Vonda: A framework for ontology-based dialogue management},
	author       = {Kiefer, Bernd and Welker, Anna and Biwer, Christophe},
	year         = 2021,
	booktitle    = {Increasing Naturalness and Flexibility in Spoken Dialogue Interaction: 10th International Workshop on Spoken Dialogue Systems},
	pages        = {93--105},
	urldate      = {2022-10-27}
}
@article{finch_2020,
	title        = {Emora {STDM}: A Versatile Framework for Innovative Dialogue System Development},
	author       = {Finch, James D and Choi, Jinho D},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2006.06143},
	urldate      = {2022-10-27}
}
@article{midas,
	title        = {Midas: A dialog act annotation scheme for open domain human machine spoken conversations},
	author       = {Yu, Dian and Yu, Zhou},
	year         = 2019,
	journal      = {{arXiv} preprint {arXiv}:1908.10023},
	urldate      = {2022-10-27}
}
@article{levenshtein_1966,
	title        = {Binary codes capable of correcting deletions, insertions and reversals.},
	author       = {Levenshtein, Vladimir Iosifovich},
	year         = 1966,
	month        = 2,
	journal      = {Soviet Physics Doklady},
	volume       = 10,
	number       = 8,
	pages        = {707--710},
	urldate      = {2022-10-27}
}
@article{williams_2017,
	title        = {Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning},
	author       = {Williams, Jason D and Asadi, Kavosh and Zweig, Geoffrey},
	year         = 2017,
	journal      = {{arXiv} preprint {arXiv}:1702.03274},
	urldate      = {2022-10-27}
}
@inproceedings{zhou_2018,
	title        = {Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network},
	author       = {Zhou, Xiangyang and Li, Lu and Dong, Daxiang and Liu, Yi and Chen, Ying and Zhao, Wayne Xin and Yu, Dianhai and Wu, Hua},
	year         = 2018,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {1118--1127},
	doi          = {10.18653/v1/P18-1103},
	url          = {http://aclweb.org/anthology/P18-1103},
	urldate      = {2022-10-27}
}
@inproceedings{tay_2017,
	title        = {Learning to Rank Question Answer Pairs with Holographic Dual {LSTM} Architecture},
	author       = {Tay, Yi and Phan, Minh C. and Tuan, Luu Anh and Hui, Siu Cheung},
	year         = 2017,
	month        = 8,
	day          = 7,
	booktitle    = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval - {SIGIR} '17},
	publisher    = {{ACM} Press},
	address      = {New York, New York, {USA}},
	pages        = {695--704},
	doi          = {10.1145/3077136.3080790},
	isbn         = 9781450350228,
	url          = {http://dl.acm.org/citation.cfm?doid=3077136.3080790},
	urldate      = {2022-10-27},
	abstract     = {We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory ({LSTM}) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the benefits of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual {LSTM} ({HD}-{LSTM}), a unified architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the {LSTM} are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive experiments, we show that {HD}-{LSTM} outperforms many other neural architectures on two popular benchmark {QA} datasets. Empirical studies confirm the effectiveness of holographic composition over the neural tensor layer.}
}
@article{dorogush_2018,
	title        = {{CatBoost}: gradient boosting with categorical features support},
	author       = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
	year         = 2018,
	journal      = {{arXiv} preprint {arXiv}:1810.11363},
	urldate      = {2022-10-27}
}
@article{wu_2016,
	title        = {Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots},
	author       = {Wu, Yu and Wu, Wei and Xing, Chen and Zhou, Ming and Li, Zhoujun},
	year         = 2016,
	journal      = {{arXiv} preprint {arXiv}:1612.01627},
	urldate      = {2022-10-27}
}
@article{whang_2020,
	title        = {Do Response Selection Models Really Know What's Next? Utterance Manipulation Strategies for Multi-turn Response Selection},
	author       = {Whang, Taesun and Lee, Dongyub and Oh, Dongsuk and Lee, Chanhee and Han, Kijong and Lee, Dong-hun and Lee, Saebyeok},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2009.04703},
	urldate      = {2022-10-27}
}
@article{ng_2020,
	title        = {Improving Dialogue Breakdown Detection with Semi-Supervised Learning},
	author       = {Ng, Nathan and Ghassemi, Marzyeh and Thangarajan, Narendran and Pan, Jiacheng and Guo, Qi},
	year         = 2020,
	journal      = {{arXiv} preprint {arXiv}:2011.00136},
	urldate      = {2022-10-27}
}
@inproceedings{dubey_2019,
	title        = {Lc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia},
	author       = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
	year         = 2019,
	booktitle    = {International Semantic Web Conference},
	pages        = {69--78},
	urldate      = {2022-10-27}
}
@inproceedings{rajpurkar_2016,
	title        = {Squad: 100,000+ questions for machine comprehension of text},
	author       = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	year         = 2016,
	booktitle    = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {2383--2392},
	doi          = {10.18653/v1/D16-1264},
	url          = {http://aclweb.org/anthology/D16-1264},
	urldate      = {2022-10-27}
}
@article{rajpurkar_2018,
	title        = {Know what you don't know: Unanswerable questions for {SQuAD}},
	author       = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	year         = 2018,
	journal      = {{arXiv} preprint {arXiv}:1806.03822},
	urldate      = {2022-10-27}
}
@inproceedings{group_2017,
	title        = {R-{NET}: Machine Reading Comprehension with Self-matching Networks},
	author       = {Group, Natural Language Computing},
	year         = 2017,
	month        = 5,
	url          = {https://www.microsoft.com/en-us/research/publication/mcr/},
	urldate      = {2022-10-27}
}
@article{rajpurkar_2016a,
	title        = {{SQuAD}: 100, 000+ Questions for Machine Comprehension of Text},
	author       = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	year         = 2016,
	journal      = {CoRR},
	volume       = {abs/1606.05250},
	url          = {http://arxiv.org/abs/1606.05250},
	urldate      = {2022-10-27}
}
@inproceedings{heck_2020,
	title        = {{TripPy}: A Triple Copy Strategy for Value Independent Neural Dialog State Tracking},
	author       = {Heck, Michael and van Niekerk, Carel and Lubis, Nurul and Geishauser, Christian and Lin, Hsien-Chin and Moresi, Marco and Gasic, Milica},
	year         = 2020,
	month        = 7,
	booktitle    = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
	publisher    = {Association for Computational Linguistics},
	address      = {1st virtual meeting},
	pages        = {35--44},
	url          = {https://www.aclweb.org/anthology/2020.sigdial-1.4},
	urldate      = {2022-10-27},
	abstract     = {Task-oriented dialog systems rely on dialog state tracking ({DST}) to monitor the user\textquoterights goal during the course of an interaction. Multi-domain and open-vocabulary settings complicate the task considerably and demand scalable solutions. In this paper we present a new approach to {DST} which makes use of various copy mechanisms to fill slots with values. Our model has no need to maintain a list of candidate values. Instead, all values are extracted from the dialog context on-the-fly. A slot is filled by one of three copy mechanisms: (1) Span prediction may extract values directly from the user input; (2) a value may be copied from a system inform memory that keeps track of the system\textquoterights inform operations (3) a value may be copied over from a different slot that is already contained in the dialog state to resolve coreferences within and across domains. Our approach combines the advantages of span-based slot filling methods with memory methods to avoid the use of value picklists altogether. We argue that our strategy simplifies the {DST} task while at the same time achieving state of the art performance on various popular evaluation sets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55\%.}
}
@inproceedings{bert,
	title        = {{BERT}: Pre-training of deep bidirectional transformers for language understanding},
	author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	volume       = {abs/1905.07213},
	pages        = {4171:4186},
	url          = {https://arxiv.org/abs/1810.04805},
	urldate      = {2022-10-27}
}
@inproceedings{elnouby_2021,
	title        = {{XCiT}: Cross-Covariance Image Transformers},
	author       = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Herve},
	year         = 2021,
	booktitle    = {Advances in Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=kzPtpIpF8o},
	urldate      = {2022-10-27},
	editor       = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman}
}
@incollection{halliday_nda,
	title        = {Language as code and language as behaviour: a systemic-functional interpretation of the nature and ontogenesis of dialogue},
	author       = {Halliday, M. A. K.},
	publisher    = {Bloomsbury Academic},
	address      = {London},
	series       = {Linguistics: Bloomsbury Academic Collections},
	pages        = {3--36},
	isbn         = {978-1-4742-8573-5},
	url          = {http://www.bloomsburycollections.com/book/semiotics-of-culture-and-language-volume-1-language-as-social-semiotic/ch1-language-as-code-and-language-as-behaviour-a-systemic-functional-interpretation-of-the-nature-and-ontogenesis-of-dialogue/},
	urldate      = {2022-10-27},
	edition      = 1
}
@inproceedings{schank_1969,
	title        = {A conceptual dependency parser for natural language},
	author       = {Schank, Roger C. and Tesler, Larry},
	year         = 1969,
	month        = 9,
	day          = 1,
	booktitle    = {Proceedings of the 1969 conference on Computational linguistics -},
	publisher    = {Association for Computational Linguistics},
	address      = {Morristown, {NJ}, {USA}},
	pages        = {1--3},
	doi          = {10.3115/990403.990405},
	url          = {http://portal.acm.org/citation.cfm?doid=990403.990405},
	urldate      = {2022-11-04}
}
@article{woods_1970,
	title        = {Transition network grammars for natural language analysis},
	author       = {Woods, W. A.},
	year         = 1970,
	month        = 10,
	journal      = {Communications of the {ACM}},
	volume       = 13,
	number       = 10,
	pages        = {591--606},
	doi          = {10.1145/355598.362773},
	issn         = {0001-0782},
	url          = {https://dl.acm.org/doi/10.1145/355598.362773},
	urldate      = {2022-11-04},
	abstract     = {The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described.}
}
@incollection{johnson_1988,
	title        = {{SHRDLU}, Procedures, Mini-World},
	author       = {Johnson, Michael L.},
	year         = 1988,
	booktitle    = {Mind, Language, Machine},
	publisher    = {Palgrave Macmillan {UK}},
	address      = {London},
	pages        = {113--122},
	doi          = {10.1007/978-1-349-19404-9\_20},
	isbn         = {978-1-349-19404-9},
	url          = {http://link.springer.com/10.1007/978-1-349-19404-9\_20},
	urldate      = {2022-11-04}
}
@article{leven_1996,
	title        = {The roots of backpropagation: From ordered derivatives to neural networks and political forecasting},
	author       = {Leven, Sam},
	year         = 1996,
	month        = 4,
	journal      = {Neural Networks},
	volume       = 9,
	number       = 3,
	pages        = {543--544},
	doi          = {10.1016/0893-6080(96)90015-5},
	issn         = {08936080},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/0893608096900155},
	urldate      = {2022-11-04}
}
@incollection{weizenbaum_2021,
	title        = {{ELIZA\textemdashA} Computer Program for the Study of Natural Language Communication between Man and Machine (1966)},
	author       = {Weizenbaum, Joseph},
	year         = 2021,
	month        = 2,
	day          = 2,
	booktitle    = {Ideas that created the future: classic papers of computer science},
	publisher    = {The {MIT} Press},
	pages        = {271--278},
	doi          = {10.7551/mitpress/12274.003.0029},
	isbn         = 9780262363174,
	url          = {http://direct.mit.edu/books/book/5003/chapter/2657050/ELIZA-A-Computer-Program-for-the-Study-of-Natural},
	urldate      = {2022-11-04},
	editor       = {Lewis, Harry R.}
}
@article{colby_1972,
	title        = {Experimental validation of a computer simulation of paranoid processes},
	author       = {Colby, Kenneth M. and Hilf, Franklin D. and Weber, Sylvia and Kraemer, Helena},
	year         = 1972,
	month        = 10,
	journal      = {Mathematical Biosciences},
	volume       = 15,
	number       = {1-2},
	pages        = {187--191},
	doi          = {10.1016/0025-5564(72)90073-9},
	issn         = {00255564},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/0025556472900739},
	urldate      = {2022-11-04}
}
@article{wilcox_2013,
	title        = {Making it real: Loebner-winning chatbot design},
	author       = {Wilcox, Bruce and Wilcox, Sue},
	year         = 2013,
	month        = 12,
	day          = 30,
	journal      = {Arbor},
	volume       = 189,
	number       = 764,
	pages        = {a086},
	doi          = {10.3989/arbor.2013.764n6009},
	issn         = {1988-{303X}},
	url          = {http://arbor.revistas.csic.es/index.php/arbor/article/view/1888/2079},
	urldate      = {2022-11-04}
}
@inproceedings{lafferty_2004,
	title        = {Kernel conditional random fields: Representation and clique selection},
	author       = {Lafferty, John and Zhu, Xiaojin and Liu, Yan},
	year         = 2004,
	month        = 7,
	day          = 4,
	booktitle    = {Twenty-first international conference on Machine learning - {ICML} '04},
	publisher    = {{ACM} Press},
	address      = {New York, New York, {USA}},
	pages        = 64,
	doi          = {10.1145/1015330.1015337},
	url          = {http://portal.acm.org/citation.cfm?doid=1015330.1015337},
	urldate      = {2022-11-04}
}
@misc{na_website_nd,
	title        = {Latent dirichlet allocation \textbar The Journal of Machine Learning Research},
	url          = {https://dl.acm.org/doi/10.5555/944919.944937},
	urldate      = {2022-11-04},
	type         = {WEBSITE}
}
@misc{na_website_nda,
	title        = {Distant supervision for relation extraction without labeled data - {ACL} Anthology},
	url          = {https://aclanthology.org/P09-1113/},
	urldate      = {2022-11-04},
	type         = {WEBSITE}
}
@misc{na_website_ndb,
	title        = {Improved backing-off for M-gram language modeling \textbar {IEEE} Conference Publication \textbar {IEEE} Xplore},
	url          = {https://ieeexplore.ieee.org/document/479394},
	urldate      = {2022-11-04},
	type         = {WEBSITE}
}
@misc{na_website_ndc,
	title        = {A neural probabilistic language model \textbar The Journal of Machine Learning Research},
	url          = {https://dl.acm.org/doi/10.5555/944919.944966},
	urldate      = {2022-11-04},
	type         = {WEBSITE}
}
@inproceedings{mikolov_2010,
	title        = {Recurrent neural network based language model},
	author       = {Mikolov, Tomáš and Karafiát, Martin and Burget, Lukáš and Černocký, Jan and Khudanpur, Sanjeev},
	year         = 2010,
	month        = 9,
	day          = 26,
	booktitle    = {Interspeech 2010},
	publisher    = {ISCA},
	address      = {ISCA},
	pages        = {1045--1048},
	doi          = {10.21437/Interspeech.2010-343},
	url          = {https://www.isca-speech.org/archive/interspeech\_2010/mikolov10\_interspeech.html},
	urldate      = {2022-11-04}
}
@misc{na_website_ndd,
	title        = {Generating Sequences With Recurrent Neural Networks - {NASA}/{ADS}},
	url          = {https://ui.adsabs.harvard.edu/abs/{2013arXiv1308}.{0850G}/abstract},
	urldate      = {2022-11-04},
	type         = {WEBSITE}
}
@phdthesis{suskever_2013,
	title        = {Training Recurrent Neural Networks},
	author       = {Suskever, Ilya},
	year         = 2013,
	url          = {https://www.cs.utoronto.ca/\~ilya/pubs/ilya\_sutskever\_phd\_thesis.pdf},
	urldate      = {2022-11-04},
	type         = {{THESIS}.{DOCTORAL}}
}
@misc{na_website_nde,
	title        = {Sequence to sequence learning with neural networks \textbar Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
	url          = {https://dl.acm.org/doi/10.5555/2969033.2969173},
	urldate      = {2022-11-04},
	type         = {WEBSITE}
}
@article{mller_2019,
	title        = {When Does Label Smoothing Help?},
	author       = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
	year         = 2019,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1906.02629},
	url          = {https://arxiv.org/abs/1906.02629},
	urldate      = {2022-11-04},
	abstract     = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.}
}
@incollection{halliday_ndb,
	title        = {Language as code and language as behaviour: a systemic-functional interpretation of the nature and ontogenesis of dialogue},
	author       = {Halliday, M. A. K.},
	publisher    = {Bloomsbury Academic},
	address      = {London},
	series       = {Linguistics: Bloomsbury Academic Collections},
	pages        = {3--36},
	isbn         = {978-1-4742-8573-5},
	url          = {http://www.bloomsburycollections.com/book/semiotics-of-culture-and-language-volume-1-language-as-social-semiotic/ch1-language-as-code-and-language-as-behaviour-a-systemic-functional-interpretation-of-the-nature-and-ontogenesis-of-dialogue/},
	urldate      = {2022-11-04},
	edition      = 1
}
@inproceedings{sukhbaatar_2015,
	title        = {End-To-End Memory Networks},
	author       = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	year         = 2015,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 28,
	url          = {https://proceedings.neurips.cc/paper/2015/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf},
	urldate      = {2022-11-04},
	editor       = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.}
}
@article{bahdanau_2014,
	title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
	author       = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year         = 2014,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1409.0473},
	url          = {https://arxiv.org/abs/1409.0473},
	urldate      = {2022-11-04},
	abstract     = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.}
}
@misc{pstu,
	title        = {PSTU dataset: classification of university-related topics},
	author       = {Aleksandr Perevalov},
	year         = 2018,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/Perevalov/pstu\_assistant/blob/master/data/data.txt}},
	commit       = {bdeafb09e7374432ae72969aa2c361c215c5ec48}
}
@inproceedings{go_emotions,
	title        = {{G}o{E}motions: A Dataset of Fine-Grained Emotions},
	author       = {Demszky, Dorottya  and Movshovitz-Attias, Dana  and Ko, Jeongwoo  and Cowen, Alan  and Nemade, Gaurav  and Ravi, Sujith},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {4040--4054},
	doi          = {10.18653/v1/2020.acl-main.372},
	url          = {https://aclanthology.org/2020.acl-main.372},
	abstract     = {Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.}
}
@article{dynasent,
	title        = {DynaSent: {A} Dynamic Benchmark for Sentiment Analysis},
	author       = {Christopher Potts and Zhengxuan Wu and Atticus Geiger and Douwe Kiela},
	year         = 2020,
	journal      = {Proceedings of ACL},
	url          = {https://arxiv.org/abs/2012.15349},
	eprinttype   = {arXiv},
	eprint       = {2012.15349},
	timestamp    = {Fri, 08 Jan 2021 17:23:09 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2012-15349.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sst,
	title        = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
	author       = {Socher, Richard  and Perelygin, Alex  and Wu, Jean  and Chuang, Jason  and Manning, Christopher D.  and Ng, Andrew  and Potts, Christopher},
	year         = 2013,
	month        = oct,
	booktitle    = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	publisher    = {Association for Computational Linguistics},
	address      = {Seattle, Washington, USA},
	pages        = {1631--1642},
	url          = {https://aclanthology.org/D13-1170}
}
@inproceedings{kumar_2016,
	title        = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
	author       = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
	year         = 2016,
	booktitle    = {Proceedings of The 33rd International Conference on Machine Learning},
	publisher    = {PMLR},
	address      = {New York, New York, {USA}},
	series       = {Proceedings of Machine Learning Research},
	volume       = 48,
	pages        = {1378--1387},
	url          = {https://proceedings.mlr.press/v48/kumar16.html},
	urldate      = {2022-11-04},
	editor       = {Balcan, Maria Florina and Weinberger, Kilian Q.},
	abstract     = {Most tasks in natural language processing can be cast into question answering ({QA}) problems over language input. We introduce the dynamic memory network ({DMN}), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The {DMN} can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook\textquoterights {bAbI} dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging ({WSJ}-{PTB}). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.}
}
@article{bengio_2003,
	title        = {A Neural Probabilistic Language Model},
	author       = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Janvin, Christian},
	year         = 2003,
	month        = 3,
	journal      = {J. Mach. Learn. Res.},
	publisher    = {{JMLR}.org},
	volume       = 3,
	number       = {null},
	pages        = {1137–1155},
	issn         = {1532-4435},
	urldate      = {2022-11-04},
	abstract     = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.}
}
@inproceedings{kneser_1995,
	title        = {Improved backing-off for M-gram language modeling},
	author       = {Kneser, R. and Ney, H.},
	year         = 1995,
	booktitle    = {1995 International Conference on Acoustics, Speech, and Signal Processing},
	publisher    = {IEEE},
	pages        = {181--184},
	doi          = {10.1109/{ICA\SSP}.1995.479394},
	isbn         = {0-7803-2431-5},
	url          = {http://ieeexplore.ieee.org/document/479394/},
	urldate      = {2022-11-04}
}
@article{blei_2003,
	title        = {Latent Dirichlet Allocation},
	author       = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year         = 2003,
	month        = 3,
	journal      = {J. Mach. Learn. Res.},
	publisher    = {{JMLR}.org},
	volume       = 3,
	number       = {null},
	pages        = {993–1022},
	issn         = {1532-4435},
	urldate      = {2022-11-04},
	abstract     = {We describe latent Dirichlet allocation ({LDA}), a generative probabilistic model for collections of discrete data such as text corpora. {LDA} is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an {EM} algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic {LSI} model.}
}
@phdthesis{werbos_1974,
	title        = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
	author       = {Werbos, Paul},
	year         = 1974,
	urldate      = {2022-11-04},
	school       = {Harvard University},
	type         = {{THESIS}.{DOCTORAL}}
}
@misc{na_website_ndf,
	title        = {Jabberwacky - Wikipedia},
	url          = {https://en.wikipedia.org/wiki/Jabberwacky},
	urldate      = {2022-11-04},
	type         = {WEBSITE}
}
@article{abushawar_2015,
	title        = {{ALICE} chatbot: trials and outputs},
	author       = {{AbuShawar}, Bayan and Atwell, Eric},
	year         = 2015,
	month        = 12,
	day          = 27,
	journal      = {Computación y Sistemas},
	volume       = 19,
	number       = 4,
	doi          = {10.13053/cys-19-4-2326},
	issn         = {2007-9737},
	url          = {http://cys.cic.ipn.mx/ojs/index.php/{CyS}/article/view/2326},
	urldate      = {2022-11-04}
}
@misc{yahoo,
	title        = {Questions on Yahoo Answers labeled as either informational or conversational, version 1.0, L31 {YAHOO} dataset},
	author       = {Yahoo},
	url          = {https://webscope.sandbox.yahoo.com/catalog.php?datatype=l\&did=82},
	urldate      = {2022-11-13},
	type         = {WEBSITE}
}
@misc{na_website_ndg,
	title        = {{DREAM} Team / Alexa Prize 3},
	url          = {https://deeppavlov.ai/challenges/dream\_alexa\_3},
	urldate      = {2022-11-08},
	type         = {WEBSITE}
}
@misc{na_website_ndh,
	title        = {Alexa Prize {SocialBot} Grand Challenge 3 - Amazon Science},
	url          = {https://www.amazon.science/alexa-prize/socialbot-grand-challenge/2019},
	urldate      = {2022-11-08},
	type         = {WEBSITE}
}
@misc{na_website_ndi,
	title        = {Alexa Prize {SocialBot} Grand Challenge 4 - Amazon Science},
	url          = {https://www.amazon.science/alexa-prize/socialbot-grand-challenge/2020},
	urldate      = {2022-11-08},
	type         = {WEBSITE}
}
@misc{na_website_ndj,
	title        = {{DREAM} Team / Alexa Prize 4},
	url          = {https://deeppavlov.ai/challenges/dream\_alexa\_4},
	urldate      = {2022-11-08},
	type         = {WEBSITE}
}
@misc{na_website_ndk,
	title        = {Overview \textbar Docker Documentation},
	url          = {https://docs.docker.com/compose/},
	urldate      = {2022-11-09},
	type         = {WEBSITE}
}
@misc{na_website_ndl,
	title        = {Pre-trained embeddings \textemdash {DeepPavlov} 1.0.0 documentation},
	url          = {http://docs.deeppavlov.ai/en/master/features/pretrained\_vectors.html\#bert},
	urldate      = {2022-11-09},
	type         = {WEBSITE}
}
@misc{toxic_kaggle,
	title        = {Jigsaw Unintended Bias in Toxicity Classification \textbar Kaggle},
	url          = {https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/discussion},
	urldate      = {2022-11-09},
	type         = {WEBSITE}
}
@misc{na_website_ndn,
	title        = {{DeepPavlov}/sentiment\_sst\_conv\_bert.json at 0.9.0 · deeppavlov/{DeepPavlov} · {GitHub}},
	url          = {https://github.com/deepmipt/{DeepPavlov}/blob/0.9.0/deeppavlov/configs/classifiers/sentiment\_sst\_conv\_bert.json},
	urldate      = {2022-11-09},
	type         = {WEBSITE}
}
@misc{dp_conv_bert,
	title        = {Conversational english BERT from DeepPavlov},
	url          = {https://huggingface.co/DeepPavlov/bert-base-cased-conversational},
	urldate      = {2022-11-09},
	type         = {WEBSITE}
}
@misc{na_website_ndo_emo,
	title        = {Emotion classification dataset (retrieved from Kaggle, supplemented by the neutral example from {ScenarioSA} dataset)},
	url          = {http://files.deeppavlov.ai/datasets/{EmotionDataset}.rar},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@misc{na_website_ndp_emo,
	title        = {Eray Yildiz \textbar Novice \textbar Kaggle},
	url          = {https://www.kaggle.com/eray1yildiz},
	urldate      = {2022-11-09},
	type         = {WEBSITE}
}
@misc{na_website_ndq,
	title        = {{SocialBot} Grand Challenge Rules - Amazon Science},
	url          = {https://www.amazon.science/alexa-prize/socialbot-grand-challenge/rules},
	urldate      = {2022-11-09},
	type         = {WEBSITE}
}
@phdthesis{dilya_thesis,
	title        = {Нейросетевые модели и диалоговая система для ведения разговора на общие темы},
	author       = {Баймурзина, Диляра},
	year         = 2021,
	month        = 10,
	day          = 27,
	url          = {https://mipt.ru/upload/medialibrary/e31/dissertation\_baymurzina.pdf},
	urldate      = {2022-11-09},
	school       = {МФТИ},
	type         = {{THESIS}.{DOCTORAL}}
}
@misc{na_website_ndr,
	title        = {Github, John Hopkins University - data about COVID cases and deaths},
	url          = {https://github.com/CSSEGISandData/COVID-19},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@misc{na_website_nds,
	title        = {Evi(software) - Wikipedia},
	url          = {https://en.wikipedia.org/wiki/Evi\_(software)},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@misc{na_website_ndt,
	title        = {Goodreads \textbar Meet your next favorite book},
	url          = {https://www.goodreads.com/},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@misc{na_website_ndu,
	title        = {Reddit - Dive into anything},
	url          = {https://www.reddit.com/},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@misc{kubernetes,
	title        = {Kubernetes - Wikipedia},
	url          = {https://ru.wikipedia.org/wiki/Kubernetes},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@misc{na_website_ndw,
	title        = {Wikidata},
	url          = {https://www.wikidata.org/wiki/Wikidata:Main\_Page},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@misc{wikihow,
	title        = {{wikiHow}: How-to instructions you can trust.},
	url          = {https://www.wikihow.com/Main-Page},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@inproceedings{na_2019,
	title        = {Разработка диалоговой системы с интеграцией профиля личности},
	author       = {Болотин, Даниил and Карпов, Дмитрий and Рашков, Григорий and Шкурак, Иван},
	year         = 2019,
	publisher    = {MIPT},
	volume       = {5th International Conference on Engineering and Telecommunication {EnT}-{MIPT} 2018},
	url          = {http://2019.en-t.info/old/articles/ent2018-thesis.pdf},
	urldate      = {2022-11-10}
}
@patent{na_2021a,
	title        = {Свидетельство о депонировании зарегистрированного в базе данных {IREG} \#2018449 "Texter ocr-cv-nlp-microservise"},
	author       = {Дуплякин, Владислав and Дмитрий, Карпов and Ондар, Адыгжы and Ушаков, Алексей},
	year         = 2021,
	number       = 2018449,
	urldate      = {2022-11-10},
	nationality  = {IREG}
}
@misc{dff,
	title        = {Dialog Flow Framework},
	url          = {https://deeppavlov.ai/dff},
	urldate      = {2022-11-10},
	type         = {WEBSITE}
}
@inproceedings{roller_2021,
	title        = {Recipes for Building an Open-Domain Chatbot},
	author       = {Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Smith, Eric Michael and Boureau, Y-Lan and Weston, Jason},
	year         = 2021,
	booktitle    = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher    = {Association for Computational Linguistics},
	address      = {Stroudsburg, {PA}, {USA}},
	pages        = {300--325},
	doi          = {10.18653/v1/2021.eacl-main.24},
	url          = {https://aclanthology.org/2021.eacl-main.24},
	urldate      = {2022-11-10}
}
@article{tfidf,
title = {Understanding Inverse Document Frequency: On Theoretical Arguments for IDF},
author = {Robertson, Stephen},
year = {2004},
month = {10},
journal = {Journal of Documentation - J DOC},
volume = {60},
pages = {503-520},
doi = {10.1108/00220410410560582}
}
@article{hedayatnia_2020a,
	title        = {Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems},
	author       = {Hedayatnia, Behnam and Kim, Seokhwan and Liu, Yang and Gopalakrishnan, Karthik and Eric, Mihail and Hakkani-Tür, Dilek},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2005.12529},
	url          = {https://arxiv.org/abs/2005.12529},
	urldate      = {2022-11-10}
}
@misc{na_website_ndz,
	title        = {Questions on Yahoo Answers labeled as either informational or conversational, version 1.0, L31 {YAHOO} dataset},
	url          = {https://webscope.sandbox.yahoo.com/catalog.php?datatype=l\&did=82},
	urldate      = {2022-11-13},
	type         = {WEBSITE}
}
@misc{na_website_ndaa,
	title        = {Hugging Face - Wikipedia},
	url          = {https://en.wikipedia.org/wiki/Hugging\_Face},
	urldate      = {2022-11-20},
	type         = {WEBSITE}
}
@article{lin_2017,
	title        = {A Structured Self-attentive Sentence Embedding},
	author       = {Lin, Zhouhan and Feng, Minwei and Ccero Nogueira dos Santos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1703.03130},
	url          = {http://arxiv.org/abs/1703.03130},
	urldate      = {2022-11-20}
}
@article{lang_sim,
	title        = {Stochastic approach to worldwide language classification: the signals and the noise towards long-range exploration},
	author       = {Beaufils, Vincent and Tomin, Johannes},
	year         = 2020,
	doi          = {10.31235/osf.io/5swba},
	note         = {Implementation we used: \url{http://www.elinguistics.net/Compare\_Languages.aspx}},
	eprinttype   = {SocArXiv},
	eprint       = {5swba}
}
@article{ba_2016,
	title        = {Layer Normalization},
	author       = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	year         = 2016,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1607.06450},
	url          = {https://arxiv.org/abs/1607.06450},
	urldate      = {2022-11-20},
	abstract     = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.}
}
@article{gehring_2017,
	title        = {Convolutional Sequence to Sequence Learning},
	author       = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
	year         = 2017,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1705.03122},
	url          = {https://arxiv.org/abs/1705.03122},
	urldate      = {2022-11-20},
	abstract     = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep {LSTM} setup of Wu and others (2016) on both {WMT}'14 English-German and {WMT}'14 English-French translation at an order of magnitude faster speed, both on {GPU} and {CPU}.}
}
@article{liu_2018,
	title        = {Stochastic Answer Networks for Natural Language Inference},
	author       = {Liu, Xiaodong and Duh, Kevin and Gao, Jianfeng},
	year         = 2018,
	journal      = {arXiv},
	doi          = {10.48550/arxiv.1804.07888},
	url          = {https://arxiv.org/abs/1804.07888},
	urldate      = {2022-11-24},
	abstract     = {We propose a stochastic answer network ({SAN}) to explore multi-step inference strategies in Natural Language Inference. Rather than directly predicting the results given the inputs, the model maintains a state and iteratively refines its predictions. Our experiments show that {SAN} achieves the state-of-the-art results on three benchmarks: Stanford Natural Language Inference ({SNLI}) dataset, {MultiGenre} Natural Language Inference ({MultiNLI}) dataset and Quora Question Pairs dataset.}
}
@article{cho_2014,
	title        = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
	author       = {Cho, {KyungHyun} and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	year         = 2014,
	journal      = {CoRR},
	volume       = {abs/1409.1259},
	url          = {http://arxiv.org/abs/1409.1259},
	urldate      = {2022-11-24}
}
@book{ferreirs_2010,
	title        = {Proceedings of {COMPSTAT}'2010: 19th International Conference on Computational {StatisticsParis} France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
	author       = {Ferreirós, José},
	year         = 2010,
	publisher    = {Physica},
	address      = {Heidelberg},
	pages        = 651,
	isbn         = {978-3-7908-2604-3},
	urldate      = {2022-11-24},
	editor       = {Lechevallier, Yves and Saporta, Gilbert},
	edition      = 2010,
	abstract     = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.}
}
@book{bousquet_2004,
	title        = {Advanced Lectures on Machine Learning: {ML} Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised ... (Lecture Notes in Computer Science (3176))},
	author       = {Bousquet, Olivier. and Luxburg, Ulrike von. and Rätsch, Gunnar.},
	year         = 2004,
	publisher    = {Springer},
	address      = {Berlin},
	pages        = 256,
	isbn         = {978-3-540-23122-6},
	urldate      = {2022-11-24},
	edition      = 2004
}
@phdthesis{na_2022,
	title        = {Методы переноса знаний для нейросетевых моделей обработки естественного языка},
	author       = {Коновалов, Василий},
	year         = 2022,
	url          = {https://mipt.ru/upload/medialibrary/33e/dissertatsiya-konovalov-vasiliy-pavlovich.pdf},
	urldate      = {2022-11-24},
	school       = {МФТИ},
	type         = {{THESIS}.{DOCTORAL}}
}
@article{erikftjongkimsang_2003,
	title        = {Introduction to the {CoNLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
	author       = {Erik F. Tjong and Kim Sang and De Meulder, Fien},
	year         = 2003,
	journal      = {CoRR},
	volume       = {cs.{CL}/0306050},
	url          = {http://arxiv.org/abs/cs/0306050},
	urldate      = {2022-11-24}
}
