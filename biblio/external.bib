@article{hochreiter_1997,
title = {Long short-term memory.},
author = {Hochreiter, S and Schmidhuber, J},
pages = {1735-1780},
url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
year = {1997},
urldate = {2018-08-14},
journal = {Neural Computation},
volume = {9},
number = {8},
doi = {10.1162/neco.1997.9.8.1735},
pmid = {9377276},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}
@article{turing_1950,
title = {Computing Machinery and Intelligence},
author = {Turing, A. M.},
pages = {433},
url = {http://mind.oxfordjournals.org/cgi/doi/10.1093/mind/{LIX}.236.433},
year = {1950},
urldate = {2021-05-06},
journal = {Mind; a quarterly review of psychology and philosophy},
volume = {LIX},
number = {236},
issn = {0026-4423},
doi = {10.1093/mind/{LIX}.236.433}
}
@article{elman_1990,
title = {Finding Structure in Time},
author = {Elman, Jeffrey L.},
pages = {179-211},
url = {http://doi.wiley.com/10.1207/s15516709cog1402\_1},
year = {1990},
month = {mar},
urldate = {2022-11-04},
journal = {Cognitive science},
volume = {14},
number = {2},
issn = {03640213},
doi = {10.1207/s15516709cog1402\_1}
}
@article{graves_2016,
title = {Hybrid computing using a neural network with dynamic external memory.},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
pages = {471-476},
url = {http://dx.doi.org/10.1038/nature20101},
year = {2016},
month = {oct},
day = {27},
urldate = {2017-05-08},
journal = {Nature},
volume = {538},
number = {7626},
doi = {10.1038/nature20101},
pmid = {27732574},
abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer ({DNC}), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a {DNC} can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a {DNC} can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that {DNCs} have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read-write memory.}
}
@inproceedings{chen_2016,
title = {{XGBoost}: A Scalable Tree Boosting System},
author = {Chen, Tianqi and Guestrin, Carlos},
pages = {785-794},
publisher = {{ACM} Press},
url = {http://dl.acm.org/citation.cfm?doid=2939672.2939785},
year = {2016},
month = {aug},
day = {13},
urldate = {2022-10-27},
isbn = {9781450342322},
doi = {10.1145/2939672.2939785},
address = {New York, New York, {USA}},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called {XGBoost}, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, {XGBoost} scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining - {KDD} '16}
}
@article{weizenbaum_1966,
title = {{ELIZA}---a computer program for the study of natural language communication between man and machine},
author = {Weizenbaum, Joseph},
pages = {36-45},
url = {http://portal.acm.org/citation.cfm?doid=365153.365168},
year = {1966},
month = {jan},
day = {1},
urldate = {2022-10-27},
journal = {Communications of the {ACM}},
volume = {9},
number = {1},
issn = {00010782},
doi = {10.1145/365153.365168}
}
@inproceedings{pennington_2014,
title = {Glove: global vectors for word representation},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
pages = {1532-1543},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/D14-1162},
year = {2014},
urldate = {2022-10-27},
doi = {10.3115/v1/D14-1162},
address = {Stroudsburg, {PA}, {USA}},
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})}
}
@inproceedings{collobert_2008,
title = {A unified architecture for natural language processing: Deep neural networks with multitask learning},
author = {Collobert, Ronan and Weston, Jason},
pages = {160-167},
publisher = {{ACM} Press},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390177},
year = {2008},
month = {jul},
day = {5},
urldate = {2022-11-04},
isbn = {9781605582054},
doi = {10.1145/1390156.1390177},
address = {New York, New York, {USA}},
booktitle = {Proceedings of the 25th international conference on Machine learning - {ICML} '08}
}
@inproceedings{he_2016,
title = {Deep residual learning for image recognition},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
pages = {770-778},
publisher = {IEEE},
url = {http://ieeexplore.ieee.org/document/7780459/},
year = {2016},
month = {jun},
day = {27},
urldate = {2020-08-06},
isbn = {978-1-4673-8851-1},
doi = {10.1109/{CVPR}.2016.90},
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers\textemdash8× deeper than {VGG} nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions1, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})}
}
@incollection{caruana_1993,
booktitle = {Machine learning proceedings 1993},
title = {Multitask Learning: A Knowledge-Based Source of Inductive Bias},
author = {Caruana, Richard A.},
pages = {41-48},
publisher = {Elsevier},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558603073500125},
year = {1993},
urldate = {2022-11-04},
isbn = {9781558603073},
doi = {10.1016/B978-1-55860-307-3.50012-5}
}
@inproceedings{graves_2013,
title = {Hybrid speech recognition with Deep Bidirectional {LSTM}},
author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
pages = {273-278},
publisher = {IEEE},
url = {http://ieeexplore.ieee.org/document/6707742/},
year = {2013},
month = {dec},
day = {8},
urldate = {2022-11-04},
isbn = {978-1-4799-2756-2},
doi = {10.1109/{ASRU}.2013.6707742},
abstract = {Deep Bidirectional {LSTM} ({DBLSTM}) recurrent neural networks have recently been shown to give state-of-the-art performance on the {TIMIT} speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of {DBLSTM} as an acoustic model in a standard neural network-{HMM} hybrid system. We find that a {DBLSTM}-{HMM} hybrid gives equally good results on {TIMIT} as the previous work. It also outperforms both {GMM} and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with {DBLSTM} appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
booktitle = {2013 {IEEE} Workshop on Automatic Speech Recognition and Understanding}
}
@inproceedings{papineni_2001,
title = {{BLEU}: A method for automatic evaluation of machine translation},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
pages = {311},
publisher = {Association for Computational Linguistics},
url = {http://portal.acm.org/citation.cfm?doid=1073083.1073135},
year = {2001},
urldate = {2022-11-04},
doi = {10.3115/1073083.1073135},
address = {Morristown, {NJ}, {USA}},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - {ACL} '02}
}
@inproceedings{peters_2018,
title = {Deep contextualized word representations},
author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
pages = {2227-2237},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/N18-1202},
year = {2018},
urldate = {2022-10-27},
doi = {10.18653/v1/N18-1202},
address = {Stroudsburg, {PA}, {USA}},
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}
}
@book{na_2018,
title = {The {NIPS} '17 competition: building intelligent systems},
editor = {Escalera, Sergio and Weimer, Markus},
series = {The springer series on challenges in machine learning},
publisher = {Springer International Publishing},
url = {http://link.springer.com/10.1007/978-3-319-94042-7},
year = {2018},
urldate = {2022-10-27},
isbn = {978-3-319-94041-0},
issn = {2520-{131X}},
doi = {10.1007/978-3-319-94042-7},
address = {Cham}
}
@inproceedings{miller_2016,
title = {Key-Value Memory Networks for Directly Reading Documents},
author = {Miller, Alexander and Fisch, Adam and Dodge, Jesse and Karimi, Amir-Hossein and Bordes, Antoine and Weston, Jason},
pages = {1400-1409},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/D16-1147},
year = {2016},
urldate = {2022-11-04},
doi = {10.18653/v1/D16-1147},
address = {Stroudsburg, {PA}, {USA}},
abstract = {Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering ({QA}) has been directed towards using Knowledge Bases ({KBs}) instead, which has proven effective. Unfortunately {KBs} often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using {KBs}, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, {WikiMovies}, a {QA} dataset that contains raw text alongside a preprocessed {KB}, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing {WikiQA} benchmark.},
booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}
}
@article{vrandei_2014,
title = {Wikidata: A Free Collaborative Knowledgebase},
author = {Vrandečić, Denny and Krötzsch, Markus},
pages = {78-85},
url = {http://dl.acm.org/citation.cfm?doid=2661061.2629489},
year = {2014},
month = {sep},
day = {23},
urldate = {2022-10-27},
journal = {Communications of the {ACM}},
volume = {57},
number = {10},
issn = {00010782},
doi = {10.1145/2629489},
abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.}
}
@inproceedings{kalchbrenner_2014,
title = {A convolutional neural network for modelling sentences},
author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
pages = {655-665},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/P14-1062},
year = {2014},
urldate = {2022-11-04},
doi = {10.3115/v1/P14-1062},
address = {Stroudsburg, {PA}, {USA}},
abstract = {The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ({DCNN}) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the {DCNN} in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
}
@article{hochreiter_1998,
title = {The vanishing gradient problem during learning recurrent neural nets and problem solutions},
author = {Hochreiter, Sepp},
pages = {107-116},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
year = {1998},
month = {apr},
urldate = {2022-11-20},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
volume = {06},
number = {02},
issn = {0218-4885},
doi = {10.1142/S0218488598000094}
}
@inproceedings{zhu_2015,
title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
pages = {19-27},
publisher = {IEEE},
url = {http://ieeexplore.ieee.org/document/7410368/},
year = {2015},
month = {dec},
day = {7},
urldate = {2022-11-20},
isbn = {978-1-4673-8391-2},
doi = {10.1109/{ICCV}.2015.11},
abstract = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware {CNN} to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
booktitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})}
}
@inproceedings{lample_2016,
title = {Neural architectures for named entity recognition},
author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
pages = {260-270},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/N16-1030},
year = {2016},
urldate = {2022-10-27},
doi = {10.18653/v1/N16-1030},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}
}
@inproceedings{zhang_2018,
title = {Personalizing Dialogue Agents: I have a dog, do you have pets too?},
author = {Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
pages = {2204-2213},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/P18-1205},
year = {2018},
urldate = {2022-10-27},
doi = {10.18653/v1/P18-1205},
address = {Stroudsburg, {PA}, {USA}},
abstract = {Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
}
@inproceedings{khatri_2018,
title = {Contextual topic modeling for dialog systems},
author = {Khatri, Chandra and Goel, Rahul and Hedayatnia, Behnam and Metanillou, Angeliki and Venkatesh, Anushree and Gabriel, Raefer and Mandal, Arindam},
pages = {892-899},
publisher = {IEEE},
url = {https://ieeexplore.ieee.org/document/8639552/},
year = {2018},
month = {dec},
day = {18},
urldate = {2022-10-27},
isbn = {978-1-5386-4334-1},
doi = {10.1109/{SLT}.2018.8639552},
abstract = {Accurate prediction of conversation topics can be a valuable signal for creating coherent and engaging dialog systems. In this work, we focus on context-aware topic classification methods for identifying topics in free-form human-chatbot dialogs. We extend previous work on neural topic classification and unsupervised topic keyword detection by incorporating conversational context and dialog act features. On annotated data, we show that incorporating context and dialog acts leads to relative gains in topic classification accuracy by 35\% and on unsupervised keyword detection recall by 11\% for conversational interactions where topics frequently span multiple utterances. We show that topical metrics such as topical depth is highly correlated with dialog evaluation metrics such as coherence and engagement implying that conversational topic models can predict user satisfaction. Our work for detecting conversation topics and keywords can be used to guide chatbots towards coherent dialog.},
booktitle = {2018 {IEEE} Spoken Language Technology Workshop ({SLT})}
}
@inproceedings{wang_2018,
title = {{GLUE}: A multi-task benchmark and analysis platform for natural language understanding},
author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
pages = {353-355},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/W18-5446},
year = {2018},
urldate = {2022-10-27},
doi = {10.18653/v1/W18-5446},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 2018 {EMNLP} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}}
}
@article{larsson_2000,
title = {Information state and dialogue management in the {TRINDI} dialogue move engine toolkit},
author = {Larsson, {STAFFAN} and Traum, {DAVID} R.},
pages = {323-340},
url = {http://www.journals.cambridge.org/{abstract\_S1351324900002539}},
year = {2000},
month = {sep},
urldate = {2022-10-27},
journal = {Natural language engineering},
volume = {6},
number = {3\&4},
issn = {13513249},
doi = {10.1017/S1351324900002539}
}
@inproceedings{mintz_2009,
title = {Distant supervision for relation extraction without labeled data},
author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
pages = {1003},
publisher = {Association for Computational Linguistics},
url = {http://portal.acm.org/citation.cfm?doid=1690219.1690287},
year = {2009},
month = {aug},
day = {2},
urldate = {2022-11-04},
isbn = {9781932432466},
doi = {10.3115/1690219.1690287},
address = {Morristown, {NJ}, {USA}},
booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP}: Volume 2 - {ACL}-{IJCNLP} '09}
}
@inproceedings{liu_2019,
title = {Multi-Task Deep Neural Networks for Natural Language Understanding},
author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
pages = {4487-4496},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/P19-1441},
year = {2019},
urldate = {2022-10-27},
doi = {10.18653/v1/P19-1441},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}
}
@inproceedings{logeswaran_2019,
title = {Zero-Shot Entity Linking by Reading Entity Descriptions},
author = {Logeswaran, Lajanugen and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina and Devlin, Jacob and Lee, Honglak},
pages = {3449-3460},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/P19-1335},
year = {2019},
urldate = {2022-10-27},
doi = {10.18653/v1/P19-1335},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}
}
@inproceedings{rashkin_2019,
title = {Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset},
author = {Rashkin, Hannah and Smith, Eric Michael and Li, Margaret and Boureau, Y-Lan},
pages = {5370-5381},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/P19-1534},
year = {2019},
urldate = {2022-10-27},
doi = {10.18653/v1/P19-1534},
address = {Stroudsburg, {PA}, {USA}},
abstract = {One challenge for dialogue agents is recognizing feelings in the conversation partner and replying accordingly, a key communicative skill. While it is straightforward for humans to recognize and acknowledge others' feelings in a conversation, this is a significant challenge for {AI} systems due to the paucity of suitable publicly-available datasets for training and evaluation. This work proposes a new benchmark for empathetic dialogue generation and {EmpatheticDialogues}, a novel dataset of 25k conversations grounded in emotional situations. Our experiments indicate that dialogue models that use our dataset are perceived to be more empathetic by human evaluators, compared to models merely trained on large-scale Internet conversation data. We also present empirical comparisons of dialogue model adaptations for empathetic responding, leveraging existing models or datasets without requiring lengthy re-training of the full model.},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}
}
@inproceedings{reimers_2019,
title = {Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
author = {Reimers, Nils and Gurevych, Iryna},
pages = {3973-3983},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/D19-1410},
year = {2019},
urldate = {2022-10-27},
doi = {10.18653/v1/D19-1410},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})}
}
@article{zhou_2020,
title = {The design and implementation of xiaoice, an empathetic social chatbot},
author = {Zhou, Li and Gao, Jianfeng and Li, Di and Shum, Heung-Yeung},
pages = {53-93},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/coli\_a\_00368},
year = {2020},
month = {mar},
urldate = {2022-10-27},
journal = {Computational Linguistics},
volume = {46},
number = {1},
issn = {0891-2017},
doi = {10.1162/coli\_a\_00368},
abstract = {This article describes the development of Microsoft {XiaoIce}, the most popular social chatbot in the world. {XiaoIce} is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human–machine social chat as decision-making over Markov Decision Processes, and optimize {XiaoIce} for long-term user engagement, measured in expected Conversation-turns Per Session ({CPS}). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how {XiaoIce} dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, {XiaoIce} has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that {XiaoIce} has achieved an average {CPS} of 23, which is significantly higher than that of other chatbots and even human conversations.}
}
@article{roy_2021,
title = {Efficient Content-Based Sparse Attention with Routing Transformers},
author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
pages = {53-68},
url = {https://direct.mit.edu/tacl/article/97776},
year = {2021},
month = {feb},
urldate = {2022-10-27},
journal = {Transactions of the Association for Computational Linguistics},
volume = {9},
issn = {2307-{387X}},
doi = {10.1162/tacl\_a\_00353},
abstract = {Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O( n 1.5 d) from O( n 2 d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on {ImageNet}-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released {PG}-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow. 1}
}
@book{chomsky_1957,
title = {Syntactic Structures},
author = {Chomsky, Noam},
publisher = {De Gruyter},
url = {https://www.degruyter.com/document/doi/10.1515/9783112316009/html},
year = {1957},
month = {dec},
day = {31},
urldate = {2022-11-04},
isbn = {9783112316009},
doi = {10.1515/9783112316009}
}
@article{devlin_2018,
title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
url = {https://arxiv.org/abs/1810.04805},
year = {2018},
urldate = {2022-03-05},
journal = {arXiv},
doi = {10.48550/arxiv.1810.04805},
abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@article{zhang_2019,
title = {{DialoGPT}: Large-Scale Generative Pre-training for Conversational Response Generation},
author = {Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill},
url = {https://arxiv.org/abs/1911.00536},
year = {2019},
urldate = {2022-10-27},
journal = {arXiv},
doi = {10.48550/arxiv.1911.00536},
abstract = {We present a large, tunable neural conversational response generation model, {DialoGPT} (dialogue generative pre-trained transformer). Trained on {147M} conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, {DialoGPT} extends the Hugging Face {PyTorch} transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage {DialoGPT} generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.}
}
@article{vaswani_2017,
title = {Attention Is All You Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
url = {https://arxiv.org/abs/1706.03762},
year = {2017},
urldate = {2022-11-15},
journal = {arXiv},
doi = {10.48550/arxiv.1706.03762},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.}
}
@article{kingma_2014,
title = {Adam: A Method for Stochastic Optimization},
author = {Kingma, Diederik P. and Ba, Jimmy},
url = {https://arxiv.org/abs/1412.6980},
year = {2014},
urldate = {2022-05-09},
journal = {arXiv},
doi = {10.48550/arxiv.1412.6980},
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.}
}
@inproceedings{lewis_2020,
title = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
pages = {7871-7880},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.acl-main.703},
year = {2020},
urldate = {2022-10-27},
doi = {10.18653/v1/2020.acl-main.703},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 58th annual meeting of the association for computational linguistics}
}
@inproceedings{bosselut_2019,
title = {{COMET}: commonsense transformers for automatic knowledge graph construction},
author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
pages = {4762-4779},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/P19-1470},
year = {2019},
urldate = {2022-10-27},
doi = {10.18653/v1/P19-1470},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}
}
@article{mikolov_2013,
title = {Efficient Estimation of Word Representations in Vector Space},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
url = {https://arxiv.org/abs/1301.3781},
year = {2013},
urldate = {2022-11-04},
journal = {arXiv},
doi = {10.48550/arxiv.1301.3781},
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.}
}
@article{sutskever_2014,
title = {Sequence to Sequence Learning with Neural Networks},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
url = {https://arxiv.org/abs/1409.3215},
year = {2014},
urldate = {2022-08-30},
journal = {arXiv},
doi = {10.48550/arxiv.1409.3215},
abstract = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difficult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a fixed dimensionality, and then another deep {LSTM} to decode the target sequence from the vector. Our main result is that on an English to French translation task from the {WMT}'14 dataset, the translations produced by the {LSTM} achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}'s {BLEU} score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not have difficulty on long sentences. For comparison, a phrase-based {SMT} system achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM} to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its {BLEU} score increases to 36.5, which is close to the previous best result on this task. The {LSTM} also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the {LSTM}'s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.}
}
@article{shavrina_2020,
title = {{RussianSuperGLUE}: A Russian Language Understanding Evaluation Benchmark},
author = {Shavrina, Tatiana and Fenogenova, Alena and Anton, Emelyanov and Denis, Shevelev and Ekaterina, Artemova and Malykh, Valentin and Vladislav, Mikhailov and Maria, Tikhonova and Chertok, Andrey and Evlampiev, Andrey},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2010.15925}
}
@inproceedings{stickland_2019,
title = {{BERT} and {PALs}: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning},
author = {Stickland, Asa Cooper and Murray, Iain},
pages = {5986:5995},
url = {https://arxiv.org/abs/1902.02671},
year = {2019},
urldate = {2022-10-27},
volume = {97},
booktitle = {Proceedings of the 36th International Conference on Machine Learning}
}
@article{arachie_2021,
title = {Constrained Labeling for Weakly Supervised Learning},
author = {Arachie, Chidubem and Huang, Bert},
year = {2021},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2009.07360}
}
@article{pilault_2020,
title = {Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in {NLP} Using Fewer Parameters \& Less Data},
author = {Pilault, Jonathan and Elhattami, Amine and Pal, Christopher},
year = {2020},
urldate = {2022-10-27}
}
@article{worsham_2020,
title = {Multi-task learning for natural language processing in the 2020s: where are we going?},
author = {Worsham, Joseph and Kalita, Jugal},
url = {https://arxiv.org/abs/2007.16008},
year = {2020},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2007.16008}
}
@article{chen_2021,
title = {Multi-Task Learning in Natural Language Processing: An Overview},
author = {Chen, Shijie and Zhang, Yu and Yang, Qiang},
url = {https://arxiv.org/abs/2109.09138},
year = {2021},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2109.09138}
}
@article{caruana_1997,
title = {Multitask learning},
author = {Caruana, Rich},
pages = {41-75},
publisher = {Springer},
year = {1997},
urldate = {2022-10-27},
journal = {Machine learning},
volume = {28},
number = {1}
}
@article{liu_2019a,
title = {Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding},
author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
url = {http://arxiv.org/abs/1904.09482},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1904.09482}
}
@article{wang_2021,
title = {Entailment as Few-Shot Learner},
author = {Wang, Sinong and Fang, Han and Khabsa, Madian and Mao, Hanzi and Ma, Hao},
url = {https://arxiv.org/abs/2104.14690},
year = {2021},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2104.14690}
}
@inproceedings{maziarka_2021,
title = {Multitask Learning Using {BERT} with Task-Embedded Attention},
author = {Maziarka, Lukasz and Danel, Tomasz},
pages = {1-6},
publisher = {IEEE},
url = {https://ieeexplore.ieee.org/document/9533990/},
year = {2021},
month = {jul},
day = {18},
urldate = {2022-10-27},
isbn = {978-1-6654-3900-8},
doi = {10.1109/{IJCNN52387}.2021.9533990},
booktitle = {2021 International Joint Conference on Neural Networks ({IJCNN})}
}
@inproceedings{wang_2018a,
title = {Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning},
author = {Wang, Weichao and Feng, Shi and Gao, Wei and Wang, Daling and Zhang, Yifei},
pages = {338-348},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/D18-1031},
year = {2018},
urldate = {2022-10-27},
doi = {10.18653/v1/D18-1031},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}
}
@inproceedings{zhao_2020,
title = {{SpanMlt}: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction},
author = {Zhao, He and Huang, Longtao and Zhang, Rong and Lu, Quan and xue, hui},
pages = {3239-3248},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.acl-main.296},
year = {2020},
urldate = {2022-10-27},
doi = {10.18653/v1/2020.acl-main.296},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}
}
@inproceedings{magooda_2021,
title = {Exploring Multitask Learning for Low-Resource Abstractive Summarization},
author = {Magooda, Ahmed and Litman, Diane and Elaraby, Mohamed},
pages = {1652-1661},
publisher = {Association for Computational Linguistics},
url = {https://aclanthology.org/2021.findings-emnlp.142},
year = {2021},
urldate = {2022-10-27},
doi = {10.18653/v1/2021.findings-emnlp.142},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2021}
}
@inproceedings{huang_2021,
title = {{GhostBERT}: Generate More Features with Cheap Operations for {BERT}},
author = {Huang, Zhiqi and Hou, Lu and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
pages = {6512-6523},
publisher = {Association for Computational Linguistics},
url = {https://aclanthology.org/2021.acl-long.509},
year = {2021},
urldate = {2022-10-27},
doi = {10.18653/v1/2021.acl-long.509},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}
}
@article{wang_2019,
title = {{SuperGLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
url = {http://arxiv.org/abs/1905.00537},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1905.00537}
}
@inproceedings{aroyehun_2018,
title = {Aggression Detection in Social Media: Using Deep Neural Networks,Data Augmentation, and Pseudo Labeling.Proceedings of the First Workshop on Trolling},
author = {Aroyehun, Segun Taofeek and Gelbukh, Alexander},
pages = {90:97},
url = {https://www.aclweb.org/anthology/W18-4411.pdf},
year = {2018},
urldate = {2022-10-27},
booktitle = {Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying}
}
@inproceedings{pentina_2017,
title = {Multi-Task Learning with Labeled and Unlabeled Tasks},
author = {Pentina, Anastasia and Lampert, Christoph H.},
pages = {2807:2816},
url = {http://proceedings.mlr.press/v70/pentina17a.html},
year = {2017},
urldate = {2022-10-27},
volume = {70},
booktitle = {Proceedings of the 34th International Conference on Machine Learning}
}
@inproceedings{gottumukkala_2020,
title = {Dynamic Sampling Strategies for Multi-Task Reading Comprehension},
author = {Gottumukkala, Ananth and Dua, Dheeru and Singh, Sameer and Gardner, Matt},
pages = {920-924},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.acl-main.86},
year = {2020},
urldate = {2022-10-27},
doi = {10.18653/v1/2020.acl-main.86},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}
}
@article{kuratov_2019,
title = {Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language},
author = {Kuratov, Yuri and Arkhipov, Mikhail Y.},
url = {http://arxiv.org/abs/1905.07213},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1905.07213}
}
@inproceedings{mller_2020,
title = {When Does Label Smoothing Help?},
author = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
url = {https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf},
year = {2020},
urldate = {2022-10-27},
booktitle = {Proceedings of the 33th {NeurIPS}}
}
@article{choromanski_2020,
title = {Rethinking Attention with Performers},
author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlós, Tamás and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy J. and Weller, Adrian},
url = {https://arxiv.org/abs/2009.14794},
year = {2020},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2009.14794}
}
@article{kitaev_2020,
title = {Reformer: The Efficient Transformer},
author = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
url = {https://arxiv.org/abs/2001.04451},
year = {2020},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2001.04451}
}
@article{wang_2020,
title = {Linformer: Self-Attention with Linear Complexity},
author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
url = {https://arxiv.org/abs/2006.04768},
year = {2020},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2006.04768}
}
@article{jaszczur_2021,
title = {Sparse is Enough in Scaling Transformers},
author = {Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, Lukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
url = {https://arxiv.org/abs/2111.12763},
year = {2021},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2111.12763}
}
@article{pilault_2020a,
title = {Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in {NLP} Using Fewer Parameters \& Less Data},
author = {Pilault, Jonathan and Elhattami, Amine and Pal, Christopher J.},
url = {https://arxiv.org/abs/2009.09139},
year = {2020},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2009.09139}
}
@article{lu_2019,
title = {12-in-1: Multi-Task Vision and Language Representation Learning},
author = {Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
url = {http://arxiv.org/abs/1912.02315},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1912.02315}
}
@inproceedings{sundararaman_2021,
title = {Learning task sampling policy for multitask learning},
author = {Sundararaman, Dhanasekar and Tsai, Henry and Lee, Kuang-Huei and Turc, Iulia and Carin, Lawrence},
pages = {4410-4415},
publisher = {Association for Computational Linguistics},
url = {https://aclanthology.org/2021.findings-emnlp.375},
year = {2021},
urldate = {2022-10-27},
doi = {10.18653/v1/2021.findings-emnlp.375},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2021}
}
@article{ma_2021,
title = {{GradTS}: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks},
author = {Ma, Weicheng and Lou, Renze and Zhang, Kai and Wang, Lili and Vosoughi, Soroush},
url = {https://arxiv.org/abs/2109.05748},
year = {2021},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/2109.05748}
}
@book{chafe_1994,
title = {Discourse, consciousness, and time: The flow and displacement of conscious experience in speaking and writing},
author = {Chafe, Wallace},
publisher = {University of Chicago Press},
year = {1994},
urldate = {2022-10-27}
}
@article{nogueira_2019,
title = {Passage Re-ranking with {BERT}},
author = {Nogueira, Rodrigo and Cho, Kyunghyun},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1901.04085}
}
@article{yazhouzhang_2019,
title = {'{ScenarioSA}: A Large Scale Conversational Database for Interactive Sentiment Analysis'},
author = {Yazhou Zhang, Lingling Song, Dawei Song, Peng Guo, Junwei Zhang, Peng Zhang},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1907.05562}
}
@article{nouhadziri_2019,
title = {'Evaluating Coherence in Dialogue Systems using Entailment'},
author = {Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1904.03371}
}
@article{macavaney_2019,
title = {{CEDR}: Contextualized Embeddings for Document Ranking},
author = {{MacAvaney}, Sean and Yates, Andrew and Cohan, Arman and Goharian, Nazli},
url = {http://arxiv.org/abs/1904.07094},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1904.07094}
}
@article{peters_2018a,
title = {Deep contextualized word representations},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
url = {http://arxiv.org/abs/1802.05365},
year = {2018},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1802.05365}
}
@article{radford_2018,
title = {Improving language understanding by generative pre-training},
author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
year = {2018},
urldate = {2022-10-27},
journal = {{URL} https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf}
}
@article{zhang_2018a,
title = {Personalizing Dialogue Agents: I have a dog, do you have pets too?},
author = {Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
url = {http://arxiv.org/abs/1801.07243},
year = {2018},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1801.07243}
}
@article{reddy_2018,
title = {{CoQA}: A Conversational Question Answering Challenge},
author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
url = {http://arxiv.org/abs/1808.07042},
year = {2018},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1808.07042}
}
@article{choi_2018,
title = {{QuAC} : Question Answering in Context},
author = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
url = {http://arxiv.org/abs/1808.07036},
year = {2018},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1808.07036}
}
@article{song_2019,
title = {{MA\SS}: Masked Sequence to Sequence Pre-training for Language Generation},
author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1905.02450}
}
@article{dong_2019,
title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1905.03197}
}
@article{dai_2019,
title = {Transformer-{XL}: Attentive Language Models Beyond a Fixed-Length Context},
author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G. and Le, Quoc V. and Salakhutdinov, Ruslan},
url = {http://arxiv.org/abs/1901.02860},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1901.02860}
}
@article{dehghani_2018,
title = {Universal Transformers},
author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
url = {http://arxiv.org/abs/1807.03819},
year = {2018},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1807.03819}
}
@article{radford_2019,
title = {Language models are unsupervised multitask learners},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
pages = {8},
year = {2019},
urldate = {2022-10-27},
journal = {{OpenAI} Blog},
volume = {1}
}
@article{wolf_2019,
title = {{TransferTransfo}: A Transfer Learning Approach for Neural Network Based Conversational Agents},
author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
url = {http://arxiv.org/abs/1901.08149},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1901.08149}
}
@inproceedings{dinan_2019,
title = {Wizard of Wikipedia: Knowledge-powered Conversational Agents},
author = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
year = {2019},
urldate = {2022-10-27},
booktitle = {Proceedings of the International Conference on Learning Representations ({ICLR})}
}
@article{nguyen_2016,
title = {{MS} {MARCO}: A human generated machine reading comprehension dataset},
author = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
year = {2016},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1611.09268}
}
@inproceedings{burtsev_2018,
title = {{DeepPavlov}: Open-Source Library for Dialogue Systems},
author = {Burtsev, Mikhail and Seliverstov, Alexander and Airapetyan, Rafael and Arkhipov, Mikhail and Baymurzina, Dilyara and Bushkov, Nickolay and Gureenkova, Olga and Khakhulin, Taras and Kuratov, Yuri and Kuznetsov, Denis and Litinsky, Alexey and Logacheva, Varvara and Lymar, Alexey and Malykh, Valentin and Petrov, Maxim and Polulyakh, Vadim and Pugachev, Leonid and Sorokin, Alexey and Vikhreva, Maria and Zaynutdinov, Marat},
pages = {122-127},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/P18-4021},
year = {2018},
urldate = {2022-10-27},
doi = {10.18653/v1/P18-4021},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of {ACL} 2018, System Demonstrations}
}
@misc{xiao_other_2018,
title = {{BERT}-as-service},
author = {Xiao, Han},
year = {2018},
urldate = {2022-10-27},
type = {OTHER}
}
@article{gabriel_2020,
title = {Further advances in open domain dialog systems in the third alexa prize socialbot grand challenge},
author = {Gabriel, Raefer and Liu, Yang and Gottardi, Anna and Eric, Mihail and Khatri, Anju and Chadha, Anjali and Chen, Qinlang and Hedayatnia, Behnam and Rajan, Pankaj and Binici, Ali and Others,},
year = {2020},
urldate = {2022-10-27},
journal = {Alexa Prize Proceedings}
}
@article{liang_2020,
title = {Gunrock 2.0 : A User Adaptive Social Conversational System},
author = {Liang, Kaihui and Chau, Austin and Li, Yu and Lu, Xueyuan and Yu, Dian and Zhou, Mingyang and Jain, Ishan and Davidson, Sam and Arnold, Josh and Nguyet, Minh and Yu, Zhou},
year = {2020},
urldate = {2022-10-27}
}
@article{paranjape_2020,
title = {Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-Initiative Conversations},
author = {Paranjape, Ashwin and See, Abigail and Kenealy, Kathleen and Li, Haojun and Hardy, Amelia and Qi, Peng and Sadagopan, Kaushik Ram and Phu, Nguyet Minh and Soylu, Dilara and Manning, Christopher D.},
url = {http://arxiv.org/abs/2008.12348},
year = {2020},
urldate = {2022-10-27}
}
@article{liu_2019b,
title = {Roberta: A robustly optimized bert pretraining approach},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1907.11692}
}
@article{raffel_2020,
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
pages = {1-67},
year = {2020},
urldate = {2022-10-27},
journal = {Journal of Machine Learning Research},
volume = {21},
number = {140}
}
@article{shoeybi_2019,
title = {Megatron-lm: Training multi-billion parameter language models using gpu model parallelism},
author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and {LeGresley}, Patrick and Casper, Jared and Catanzaro, Bryan},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1909.08053}
}
@article{brown_2020,
title = {Language models are few-shot learners},
author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Others,},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2005.14165}
}
@incollection{yang_2019,
booktitle = {Advances in Neural Information Processing Systems 32},
title = {{XLNet}: Generalized Autoregressive Pretraining for Language Understanding},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch}é-Buc, F. and Fox, E. and Garnett, R.},
pages = {5753-5763},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf},
year = {2019},
urldate = {2022-10-27}
}
@inproceedings{gopalakrishnan_2019,
title = {Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations},
author = {Gopalakrishnan, Karthik and Hedayatnia, Behnam and Chen, Qinlang and Gottardi, Anna and Kwatra, Sanjeev and Venkatesh, Anu and Gabriel, Raefer and Hakkani-Tür, Dilek},
pages = {1891-1895},
publisher = {ISCA},
url = {http://www.isca-speech.org/archive/Interspeech\_2019/abstracts/3079.html},
year = {2019},
month = {sep},
day = {15},
urldate = {2022-10-27},
doi = {10.21437/Interspeech.2019-3079},
address = {ISCA},
booktitle = {Interspeech 2019}
}
@article{guu_2020,
title = {Realm: Retrieval-augmented language model pre-training},
author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2002.08909}
}
@article{verga_2020,
title = {Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge},
author = {Verga, Pat and Sun, Haitian and Soares, Livio Baldini and Cohen, William W},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2007.00849}
}
@article{lewis_2020a,
title = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandara and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Others,},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2005.11401}
}
@article{speer_2016,
title = {{ConceptNet} 5.5: An Open Multilingual Graph of General Knowledge},
author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
url = {https://arxiv.org/abs/1612.03975},
year = {2016},
urldate = {2022-10-27},
journal = {arXiv},
doi = {10.48550/arxiv.1612.03975},
abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource {ConceptNet} that is particularly well suited to be used with modern {NLP} techniques such as word embeddings. {ConceptNet} is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When {ConceptNet} is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as {WordNet} or {DBPedia}. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving {SAT}-style analogies.}
}
@misc{logacheva_other_2018,
title = {A dataset of topic-oriented human-to-chatbot dialogues},
author = {Logacheva, Varvara and Burtsev, Mikhail and Malykh, Valentin and Poluliakh, Vadim and Rudnicky, Alexander and Serban, Iulian and Lowe, Ryan and Prabhumoye, Shrimai and Black, Alan W and Bengio, Yoshua},
year = {2018},
urldate = {2022-10-27},
type = {OTHER}
}
@incollection{logacheva_2020,
booktitle = {The {NeurIPS}'18 Competition},
title = {{ConvAI2} dataset of non-goal-oriented human-to-bot dialogues},
author = {Logacheva, Varvara and Malykh, Valentin and Litinsky, Aleksey and Burtsev, Mikhail},
pages = {277-294},
publisher = {Springer, Cham},
year = {2020},
urldate = {2022-10-27}
}
@incollection{dinan_2020,
booktitle = {The {NeurIPS}'18 Competition},
title = {The second conversational intelligence challenge (convai2)},
author = {Dinan, Emily and Logacheva, Varvara and Malykh, Valentin and Miller, Alexander and Shuster, Kurt and Urbanek, Jack and Kiela, Douwe and Szlam, Arthur and Serban, Iulian and Lowe, Ryan and Others,},
pages = {187-208},
publisher = {Springer, Cham},
year = {2020},
urldate = {2022-10-27}
}
@article{yazhouzhang_2019a,
title = {{ScenarioSA}: A Large Scale Conversational Database for Interactive Sentiment Analysis},
author = {Yazhou Zhang, Lingling Song, Dawei Song, Peng Guo, Junwei Zhang, Peng Zhang},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1907.05562}
}
@article{nouhadziri_2019a,
title = {'Evaluating Coherence in Dialogue Systems using Entailment'},
author = {Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane},
year = {2019},
urldate = {2022-10-27},
journal = {Proceedings of {NAACL}}
}
@inproceedings{macavaney_2019a,
title = {{CEDR}: Contextualized embeddings for document ranking},
author = {{MacAvaney}, Sean and Yates, Andrew and Cohan, Arman and Goharian, Nazli},
pages = {1101-1104},
year = {2019},
urldate = {2022-10-27},
booktitle = {Proceedings of the 42nd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval}
}
@article{emilydinan_2018,
title = {Wizard of Wikipedia: Knowledge-Powered Conversation Agents},
author = {Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston},
year = {2018},
urldate = {2022-10-27},
journal = {Proceedings of {ICLR}}
}
@article{karthikgopalakrishnan_2019,
title = {Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations},
author = {Karthik Gopalakrishnan, Behnam Hedayatnia,Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur},
year = {2019},
urldate = {2022-10-27},
journal = {Proceedings of Interspeech}
}
@inproceedings{song_2019a,
title = {{MA\SS}: Masked Sequence to Sequence Pre-training for Language Generation},
author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
pages = {5926-5936},
year = {2019},
urldate = {2022-10-27},
booktitle = {International Conference on Machine Learning}
}
@article{dong_2019a,
title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
url = {https://arxiv.org/abs/1905.03197},
year = {2019},
urldate = {2022-10-27},
journal = {arXiv},
doi = {10.48550/arxiv.1905.03197},
abstract = {This paper presents a new Unified pre-trained Language Model ({UniLM}) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. {UniLM} compares favorably with {BERT} on the {GLUE} benchmark, and the {SQuAD} 2.0 and {CoQA} question answering tasks. Moreover, {UniLM} achieves new state-of-the-art results on five natural language generation datasets, including improving the {CNN}/{DailyMail} abstractive summarization {ROUGE}-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization {ROUGE}-L to 35.75 (0.86 absolute improvement), the {CoQA} generative question answering F1 score to 82.5 (37.1 absolute improvement), the {SQuAD} question generation {BLEU}-4 to 22.12 (3.75 absolute improvement), and the {DSTC7} document-grounded dialog response generation {NIST}-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.}
}
@article{dinan_2018,
title = {Wizard of wikipedia: Knowledge-powered conversational agents},
author = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
year = {2018},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1811.01241}
}
@misc{johannes_other_2020,
title = {Personality Prediction from Text},
author = {Johannes, Wieser},
year = {2020},
urldate = {2022-10-27},
type = {OTHER}
}
@misc{liang_other_2020,
title = {Gunrock 2.0: A User Adaptive Social Conversational System},
author = {Liang, Kaihui and Chau, Austin and Li, Yu and Lu, Xueyuan and Yu, Dian and Zhou, Mingyang and Jain, Ishan and Davidson, Sam and Arnold, Josh and Nguyen, Minh and Yu, Zhou},
year = {2020},
urldate = {2022-10-27},
type = {OTHER}
}
@article{bowden_2019,
title = {{SlugBot}: Developing a Computational Model and Framework of a Novel Dialogue Genre},
author = {Bowden, Kevin K. and Wu, {JiaQi} and Cui, Wen and Juraska, Juraj and Harrison, Vrindavan and Schwarzmann, Brian and Santer, Nick and Walker, Marilyn A.},
url = {http://arxiv.org/abs/1907.10658},
year = {2019},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1907.10658}
}
@inproceedings{prasad_2008,
title = {The Penn Discourse {TreeBank} 2.0.},
author = {Prasad, Rashmi and Dinesh, Nikhil and Lee, Alan and Miltsakaki, Eleni and Robaldo, Livio and Joshi, Aravind and Webber, Bonnie},
publisher = {European Language Resources Association ({ELRA})},
url = {http://www.lrec-conf.org/proceedings/lrec2008/pdf/754\_paper.pdf},
year = {2008},
month = {may},
urldate = {2022-10-27},
address = {Marrakech, Morocco},
abstract = {We present the second version of the Penn Discourse Treebank, {PDTB}-2.0, describing its lexically-grounded annotations of discourse relations and their two abstract object arguments over the 1 million word Wall Street Journal corpus. We describe all aspects of the annotation, including (a) the argument structure of discourse relations, (b) the sense annotation of the relations, and (c) the attribution of discourse relations and each of their arguments. We list the differences between {PDTB}-1.0 and {PDTB}-2.0. We present representative statistics for several aspects of the annotation in the corpus.},
booktitle = {Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC\textquoteright08})}
}
@inproceedings{eggins_1996,
title = {Analysing Casual Conversation},
author = {Eggins, S. and Slade, D.},
year = {1996},
urldate = {2022-10-27}
}
@incollection{halliday_nd,
title = {Language as code and language as behaviour: a systemic-functional interpretation of the nature and ontogenesis of dialogue},
author = {Halliday, M. A. K.},
series = {Linguistics: Bloomsbury Academic Collections},
pages = {3-36},
publisher = {Bloomsbury Academic},
url = {http://www.bloomsburycollections.com/book/semiotics-of-culture-and-language-volume-1-language-as-social-semiotic/ch1-language-as-code-and-language-as-behaviour-a-systemic-functional-interpretation-of-the-nature-and-ontogenesis-of-dialogue/},
urldate = {2022-10-27},
edition = {1},
isbn = {978-1-4742-8573-5},
address = {London}
}
@book{halliday_2004,
title = {An introduction to functional grammar / M.A.K. Halliday},
author = {Halliday, M. A. K. and Matthiessen, Christian M. I. M.},
pages = {x, 689 p. :},
publisher = {Hodder Arnold London},
year = {2004},
urldate = {2022-10-27},
edition = {3rd ed. / rev. by Christian M.I.M. Matthiessen.},
isbn = {0340761679 9780340761670}
}
@book{hasan_1985,
title = {Discourse on Discourse. Workshop Reports from the Macquarie Workshop on Discourse Analysis (Sydney, New South Wales, Australia, February 21-25, 1983). Occasional Papers Number 7.},
author = {Hasan, Ruqaiya},
publisher = {ERIC},
year = {1985},
urldate = {2022-10-27}
}
@book{glimm_2012,
title = {{KI} 2012: Advances in Artificial Intelligence},
author = {Glimm, Birte and Krüger, ‎Antonio},
editor = {Glimm, Birte and Krüger, Antonio},
pages = {1},
publisher = {Springer Nature},
year = {2012},
urldate = {2022-10-27},
edition = {1},
isbn = {978-3-642-33347-7},
address = {Berlin},
abstract = {An approach of improving the small talk capabilities of an existing virtual agent architecture is presented. Findings in virtual agent research revealed the need to pay attention to the sophisticated structures found in (human) casual conversations. In particular, existing dialogue act tag sets lack of tags adequately reflecting the subtle structures found in small talk. The approach presented here structures dialogues on two different levels. The micro level consists of meta information (speech functions) that dialogue acts can be tagged with. The macro level is concerned with ordering individual dialogue acts into sequences. The extended dialogue engine allows for a fine-grained selection of responses, enabling the agent to produce varied small talk sequences.}
}
@incollection{clemson_2013,
booktitle = {Encyclopedia of behavioral medicine},
title = {Five-Factor Model of Personality},
author = {Clemson, Lindy and Turner, J. Rick and Turner, J. Rick and Jacquez, Farrah and Raglin, Whitney and Reed, Gabriela and Reed, Gabriela and Limmer, Jane and Floyd, Serina and Reed, Gabriela and Graber, Elana and Beveridge, Ryan M. and Randall, Ashley K. and Bodenmann, Guy and Turner, J. Rick and Malik, Neena and Jent, Jason and Parker, Alyssa and Wang, Jenny T. and Newman, Sarah J. and Beveridge, Ryan M. and Graber, Elana and Wang, Jenny T. and Carrillo, Adriana and Gomez-Meade, Carley and Carrillo, Adriana and Gomez-Meade, Carley and Harlapur, Manjunath and Shimbo, Daichi and Campbell, Tavis S. and Johnson, Jillian A. and Zernicke, Kristin A. and Flannery, Kelly and Monteros, Karla Espinosa and Friedberg, Fred and Harlapur, Manjunath and Shimbo, Daichi and Gidron, Yori and Turner, J. Rick and Rosenberg, Leah and Rosenberg, Leah and Morizio, Alexandre and Bacon, Simon and Chmielewski, Michael S. and Morgan, Theresa A. and Daigre, Amber and Powell, Lynda H. and Janssen, Imke and Killianova, Tereza and Gidron, Yori and Wawrzyniak, Andrew J. and Wawrzyniak, Andrew J. and Brintz, Carrie and Sebastiano, M. Di Katie and Moriguchi, Yoshiya and Ando, Tetusya and Söderback, Ingrid},
editor = {Gellman, Marc D. and Turner, J. Rick},
pages = {803-804},
publisher = {Springer New York},
url = {http://link.springer.com/10.1007/978-1-4419-1005-9\_1226},
year = {2013},
urldate = {2022-10-27},
isbn = {978-1-4419-1004-2},
doi = {10.1007/978-1-4419-1005-9\_1226},
address = {New York, {NY}}
}
@misc{ram_other_2018,
title = {Conversational {AI}: The Science Behind the Alexa Prize},
author = {Ram, Ashwin and Prasad, Rohit and Khatri, Chandra and Venkatesh, Anu and Gabriel, Raefer and Liu, Qing and Nunn, Jeff and Hedayatnia, Behnam and Cheng, Ming and Nagar, Ashish and King, Eric and Bland, Kate and Wartick, Amanda and Pan, Yi and Song, Han and Jayadevan, Sk and Hwang, Gene and Pettigrue, Art},
year = {2018},
urldate = {2022-10-27},
type = {OTHER}
}
@inproceedings{socher_2013,
title = {Recursive deep models for semantic compositionality over a sentiment treebank},
author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
pages = {1631-1642},
year = {2013},
urldate = {2022-10-27},
booktitle = {Proceedings of the 2013 conference on empirical methods in natural language processing}
}
@inproceedings{danescuniculescumizil_2011,
title = {Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.},
author = {Danescu-Niculescu-Mizil, Cristian and Lee, Lillian},
url = {https://www.cs.cornell.edu/\~cristian/{Cornell\_Movie}-{Dialogs\_Corpus}.html},
year = {2011},
urldate = {2022-10-27},
booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, {ACL} 2011}
}
@inproceedings{yanranli_2017,
title = {{DailyDialog}: A Manually Labelled Multi-turn Dialogue Dataset},
author = {Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Niu, Shuzi},
url = {http://yanran.li/dailydialog.html},
year = {2017},
urldate = {2022-10-27},
booktitle = {Proceedings of The 8th International Joint Conference on Natural Language Processing ({IJCNLP} 2017)}
}
@article{sang_2003,
title = {Introduction to the {CoNLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
author = {Sang, Erik F. Tjong Kim and De Meulder, Fien},
url = {https://arxiv.org/abs/cs/0306050},
year = {2003},
urldate = {2022-10-27},
journal = {arXiv},
doi = {10.48550/arxiv.cs/0306050},
abstract = {We describe the {CoNLL}-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.}
}
@article{huang_2015,
title = {Bidirectional {LSTM}-{CRF} Models for Sequence Tagging},
author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
url = {https://arxiv.org/abs/1508.01991},
year = {2015},
urldate = {2022-10-27},
journal = {arXiv},
doi = {10.48550/arxiv.1508.01991},
abstract = {In this paper, we propose a variety of Long Short-Term Memory ({LSTM}) based models for sequence tagging. These models include {LSTM} networks, bidirectional {LSTM} ({BI}-{LSTM}) networks, {LSTM} with a Conditional Random Field ({CRF}) layer ({LSTM}-{CRF}) and bidirectional {LSTM} with a {CRF} layer ({BI}-{LSTM}-{CRF}). Our work is the first to apply a bidirectional {LSTM} {CRF} (denoted as {BI}-{LSTM}-{CRF}) model to {NLP} benchmark sequence tagging data sets. We show that the {BI}-{LSTM}-{CRF} model can efficiently use both past and future input features thanks to a bidirectional {LSTM} component. It can also use sentence level tag information thanks to a {CRF} layer. The {BI}-{LSTM}-{CRF} model can produce state of the art (or close to) accuracy on {POS}, chunking and {NER} data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.}
}
@inproceedings{strubell_2017,
title = {Fast and Accurate Entity Recognition with Iterated Dilated Convolutions},
author = {Strubell, Emma and Verga, Patrick and Belanger, David and {McCallum}, Andrew},
pages = {2670-2680},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/D17-1283},
year = {2017},
urldate = {2022-10-27},
doi = {10.18653/v1/D17-1283},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}
}
@inproceedings{xu_2017,
title = {A local detection approach for named entity recognition and mention detection},
author = {Xu, Mingbin and Jiang, Hui and Watcharawittayakul, Sedtawut},
pages = {1237-1247},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/P17-1114},
year = {2017},
urldate = {2022-10-27},
doi = {10.18653/v1/P17-1114},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
}
@inproceedings{gangluo_2015,
title = {Joint named entity recognition and disambiguation},
author = {Gang Luo, Xiaojiang Huang, Chin-Yew Lin, Zaiqing Nie},
pages = {879-888},
year = {2015},
urldate = {2022-10-27},
booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}
}
@inproceedings{passos_2014,
title = {Lexicon infused phrase embeddings for named entity resolution},
author = {Passos, Alexandre and Kumar, Vineet and {McCallum}, Andrew},
pages = {78-86},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/W14-1609},
year = {2014},
urldate = {2022-10-27},
doi = {10.3115/v1/W14-1609},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning}
}
@incollection{wang_2017,
booktitle = {Chinese computational linguistics and natural language processing based on naturally annotated big data},
title = {Named Entity Recognition with Gated Convolutional Neural Networks},
author = {Wang, Chunqi and Chen, Wei and Xu, Bo},
editor = {Sun, Maosong and Wang, Xiaojie and Chang, Baobao and Xiong, Deyi},
series = {Lecture notes in computer science},
pages = {110-121},
publisher = {Springer International Publishing},
url = {http://link.springer.com/10.1007/978-3-319-69005-6\_10},
year = {2017},
urldate = {2022-10-27},
volume = {10565},
isbn = {978-3-319-69004-9},
issn = {0302-9743},
doi = {10.1007/978-3-319-69005-6\_10},
address = {Cham}
}
@incollection{wallace_2009,
booktitle = {Parsing the Turing Test},
title = {The anatomy of {ALICE}},
author = {Wallace, Richard S},
pages = {181-210},
publisher = {Springer},
year = {2009},
urldate = {2022-10-27}
}
@inproceedings{yi_2019,
title = {Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators},
author = {Yi, Sanghyun and Goel, Rahul and Khatri, Chandra and Cervone, Alessandra and Chung, Tagyoung and Hedayatnia, Behnam and Venkatesh, Anu and Gabriel, Raefer and Hakkani-Tur, Dilek},
pages = {65-75},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/W19-8608},
year = {2019},
urldate = {2022-10-27},
doi = {10.18653/v1/W19-8608},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 12th International Conference on Natural Language Generation}
}
@article{henderson_2019,
title = {{ConveRT}: Efficient and Accurate Conversational Representations from Transformers},
author = {Henderson, Matthew and Casanueva, Iñigo and Mrkšic, Nikola and Su, Pei-Hao and Vulic, Ivan and Others,},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1911.03688}
}
@incollection{ke_2017,
booktitle = {Advances in Neural Information Processing Systems 30},
title = {{LightGBM}: A Highly Efficient Gradient Boosting Decision Tree},
author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
pages = {3146-3154},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf},
year = {2017},
urldate = {2022-10-27}
}
@article{wolf_2019a,
title = {Transfertransfo: A transfer learning approach for neural network based conversational agents},
author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1901.08149}
}
@article{dai_2015,
title = {Semi-supervised Sequence Learning},
author = {Dai, Andrew M. and Le, Quoc V.},
url = {https://arxiv.org/abs/1511.01432},
year = {2015},
urldate = {2022-10-27},
journal = {arXiv},
doi = {10.48550/arxiv.1511.01432},
abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as {IMDB}, {DBpedia} and 20 Newsgroups.}
}
@article{cer_2018,
title = {Universal Sentence Encoder},
author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
url = {https://arxiv.org/abs/1803.11175},
year = {2018},
urldate = {2022-10-27},
journal = {arXiv},
doi = {10.48550/arxiv.1803.11175},
abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other {NLP} tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests ({WEAT}) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on {TF} Hub.}
}
@article{khatri_2018a,
title = {Detecting Offensive Content in Open-domain Conversations using Two Stage Semi-supervision},
author = {Khatri, Chandra and Hedayatnia, Behnam and Goel, Rahul and Venkatesh, Anushree and Gabriel, Raefer and Mandal, Arindam},
year = {2018},
urldate = {2022-10-27},
journal = {ArXiv},
volume = {abs/1811.12900}
}
@article{khatri_2018b,
title = {Advancing the State of the Art in Open Domain Dialog Systems through the Alexa Prize},
author = {Khatri, Chandra and Hedayatnia, Behnam and Venkatesh, Anu and Nunn, Jeff and Pan, Yi and Liu, Qihan and Song, Han and Gottardi, Anna and Kwatra, Sanjeev and Pancholi, Sanju and Cheng, Ming and Chen, Qinglang and Stubel, Lauren and Gopalakrishnan, Karthik and Bland, Kate and Gabriel, Raefer and Mandal, Arindam and Hakkani-Tür, Dilek Z. and Hwang, Gene and Michel, Nate and King, Eric and Prasad, Rohit},
year = {2018},
urldate = {2022-10-27},
journal = {ArXiv},
volume = {abs/1812.10757}
}
@article{defreitasadiwardana_2020,
title = {Towards a Human-like Open-Domain Chatbot},
author = {De Freitas Adiwardana, Daniel and Luong, Minh-Thang and So, David R. and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and Le, Quoc V.},
year = {2020},
urldate = {2022-10-27},
journal = {ArXiv},
volume = {abs/2001.09977}
}
@article{pichl_2018,
title = {Alquist 2.0: Alexa Prize Socialbot Based on Sub-Dialogue Models},
author = {Pichl, Jan and Marek, Petr and Konrád, Jakub and Matulk, Martin and Šedivy, Jan},
year = {2018},
urldate = {2022-10-27},
journal = {2nd Proceedings of Alexa Prize (Alexa Prize 2018)}
}
@article{chen_2018,
title = {Gunrock: Building a human-like social bot by leveraging large scale real user data},
author = {Chen, Chun-Yen and Yu, Dian and Wen, Weiming and Yang, Yi Mang and Zhang, Jiaping and Zhou, Mingyang and Jesse, Kevin and Chau, Austin and Bhowmick, Antara and Iyer, Shreenath and Others,},
year = {2018},
urldate = {2022-10-27},
journal = {2nd Proceedings of Alexa Prize (Alexa Prize 2018)}
}
@inproceedings{welleck_2019,
title = {Dialogue natural language inference},
author = {Welleck, Sean and Weston, Jason and Szlam, Arthur and Cho, Kyunghyun},
pages = {3731-3741},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/P19-1363},
year = {2019},
urldate = {2022-10-27},
doi = {10.18653/v1/P19-1363},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}
}
@misc{roller_other_2020,
title = {Recipes for building an open-domain chatbot},
author = {Roller, Stephen and Dinan, Emily and Goyal, Naman and Da Ju, and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Shuster, Kurt and Smith, Eric M. and Boureau, Y-Lan and Weston, Jason},
year = {2020},
urldate = {2022-10-27},
type = {OTHER}
}
@article{hedayatnia_2020,
title = {Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems},
author = {Hedayatnia, Behnam and Kim, Seokhwan and Liu, Yang and Gopalakrishnan, Karthik and Eric, Mihail and Hakkani-Tur, Dilek},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2005.12529}
}
@article{kuratov_2019a,
title = {{DREAM} technical report for the Alexa Prize 2019},
author = {Kuratov, Yuri and Yusupov, Idris and Baymurzina, Dilyara and Kuznetsov, Denis and Cherniavskii, Daniil and Dmitrievskiy, Alexander and Ermakova, Elena and Ignatov, Fedor and Karpov, Dmitry and Kornev, Daniel and Others,},
year = {2019},
urldate = {2022-10-27},
journal = {3rd Proceedings of Alexa Prize}
}
@inproceedings{karpov_2021,
title = {Data pseudo-labeling while adapting {BERT} for multitask approaches},
author = {Karpov, Dmitry and Burtsev, Michail},
url = {http://www.dialog-21.ru/media/5364/karpovdplusburtsevm079.pdf},
year = {2021},
urldate = {2022-10-27},
booktitle = {Proceedings of the International Conference {\textquotedblleftDialogue} 2021\textquotedblright}
}
@inproceedings{wu_2020,
title = {Scalable Zero-shot Entity Linking with Dense Entity Retrieval},
author = {Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
pages = {6397-6407},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.emnlp-main.519},
year = {2020},
urldate = {2022-10-27},
doi = {10.18653/v1/2020.emnlp-main.519},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})}
}
@inproceedings{ultes_2017,
title = {Pydial: A multi-domain statistical dialogue system toolkit},
author = {Ultes, Stefan and Barahona, Lina M Rojas and Su, Pei-Hao and Vandyke, David and Kim, Dongho and Casanueva, Inigo and Budzianowski, Pawe and Mrkšic, Nikola and Wen, Tsung-Hsien and Gasic, Milica and Others,},
pages = {73-78},
year = {2017},
urldate = {2022-10-27},
booktitle = {Proceedings of {ACL} 2017, System Demonstrations}
}
@inproceedings{jang_2019,
title = {{PyOpenDial}: a python-based domain-independent toolkit for developing spoken dialogue systems with probabilistic rules},
author = {Jang, Youngsoo and Lee, Jongmin and Park, Jaeyoung and Lee, Kyeng-Hun and Lison, Pierre and Kim, Kee-Eung},
pages = {187-192},
year = {2019},
urldate = {2022-10-27},
booktitle = {Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing ({EMNLP}-{IJCNLP}): system demonstrations}
}
@inproceedings{kiefer_2021,
title = {Vonda: A framework for ontology-based dialogue management},
author = {Kiefer, Bernd and Welker, Anna and Biwer, Christophe},
pages = {93-105},
year = {2021},
urldate = {2022-10-27},
booktitle = {Increasing Naturalness and Flexibility in Spoken Dialogue Interaction: 10th International Workshop on Spoken Dialogue Systems}
}
@article{finch_2020,
title = {Emora {STDM}: A Versatile Framework for Innovative Dialogue System Development},
author = {Finch, James D and Choi, Jinho D},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2006.06143}
}
@article{yu_2019,
title = {Midas: A dialog act annotation scheme for open domain human machine spoken conversations},
author = {Yu, Dian and Yu, Zhou},
year = {2019},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1908.10023}
}
@article{levenshtein_1966,
title = {Binary codes capable of correcting deletions, insertions and reversals.},
author = {Levenshtein, Vladimir Iosifovich},
pages = {707-710},
year = {1966},
month = {feb},
urldate = {2022-10-27},
journal = {Soviet Physics Doklady},
volume = {10},
number = {8}
}
@article{williams_2017,
title = {Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning},
author = {Williams, Jason D and Asadi, Kavosh and Zweig, Geoffrey},
year = {2017},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1702.03274}
}
@inproceedings{zhou_2018,
title = {Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network},
author = {Zhou, Xiangyang and Li, Lu and Dong, Daxiang and Liu, Yi and Chen, Ying and Zhao, Wayne Xin and Yu, Dianhai and Wu, Hua},
pages = {1118-1127},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/P18-1103},
year = {2018},
urldate = {2022-10-27},
doi = {10.18653/v1/P18-1103},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
}
@inproceedings{tay_2017,
title = {Learning to Rank Question Answer Pairs with Holographic Dual {LSTM} Architecture},
author = {Tay, Yi and Phan, Minh C. and Tuan, Luu Anh and Hui, Siu Cheung},
pages = {695-704},
publisher = {{ACM} Press},
url = {http://dl.acm.org/citation.cfm?doid=3077136.3080790},
year = {2017},
month = {aug},
day = {7},
urldate = {2022-10-27},
isbn = {9781450350228},
doi = {10.1145/3077136.3080790},
address = {New York, New York, {USA}},
abstract = {We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory ({LSTM}) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the benefits of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual {LSTM} ({HD}-{LSTM}), a unified architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the {LSTM} are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive experiments, we show that {HD}-{LSTM} outperforms many other neural architectures on two popular benchmark {QA} datasets. Empirical studies confirm the effectiveness of holographic composition over the neural tensor layer.},
booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval - {SIGIR} '17}
}
@article{dorogush_2018,
title = {{CatBoost}: gradient boosting with categorical features support},
author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
year = {2018},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1810.11363}
}
@article{wu_2016,
title = {Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots},
author = {Wu, Yu and Wu, Wei and Xing, Chen and Zhou, Ming and Li, Zhoujun},
year = {2016},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1612.01627}
}
@article{whang_2020,
title = {Do Response Selection Models Really Know What's Next? Utterance Manipulation Strategies for Multi-turn Response Selection},
author = {Whang, Taesun and Lee, Dongyub and Oh, Dongsuk and Lee, Chanhee and Han, Kijong and Lee, Dong-hun and Lee, Saebyeok},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2009.04703}
}
@article{ng_2020,
title = {Improving Dialogue Breakdown Detection with Semi-Supervised Learning},
author = {Ng, Nathan and Ghassemi, Marzyeh and Thangarajan, Narendran and Pan, Jiacheng and Guo, Qi},
year = {2020},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:2011.00136}
}
@inproceedings{dubey_2019,
title = {Lc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia},
author = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
pages = {69-78},
year = {2019},
urldate = {2022-10-27},
booktitle = {International Semantic Web Conference}
}
@inproceedings{rajpurkar_2016,
title = {Squad: 100,000+ questions for machine comprehension of text},
author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
pages = {2383-2392},
publisher = {Association for Computational Linguistics},
url = {http://aclweb.org/anthology/D16-1264},
year = {2016},
urldate = {2022-10-27},
doi = {10.18653/v1/D16-1264},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}
}
@article{rajpurkar_2018,
title = {Know what you don't know: Unanswerable questions for {SQuAD}},
author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
year = {2018},
urldate = {2022-10-27},
journal = {{arXiv} preprint {arXiv}:1806.03822}
}
@inproceedings{group_2017,
title = {R-{NET}: Machine Reading Comprehension with Self-matching Networks},
author = {Group, Natural Language Computing},
url = {https://www.microsoft.com/en-us/research/publication/mcr/},
year = {2017},
month = {may},
urldate = {2022-10-27}
}
@article{rajpurkar_2016a,
title = {{SQuAD}: 100, 000+ Questions for Machine Comprehension of Text},
author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
url = {http://arxiv.org/abs/1606.05250},
year = {2016},
urldate = {2022-10-27},
journal = {CoRR},
volume = {abs/1606.05250}
}
@inproceedings{heck_2020,
title = {{TripPy}: A Triple Copy Strategy for Value Independent Neural Dialog State Tracking},
author = {Heck, Michael and van Niekerk, Carel and Lubis, Nurul and Geishauser, Christian and Lin, Hsien-Chin and Moresi, Marco and Gasic, Milica},
pages = {35-44},
publisher = {Association for Computational Linguistics},
url = {https://www.aclweb.org/anthology/2020.sigdial-1.4},
year = {2020},
month = {jul},
urldate = {2022-10-27},
address = {1st virtual meeting},
abstract = {Task-oriented dialog systems rely on dialog state tracking ({DST}) to monitor the user\textquoterights goal during the course of an interaction. Multi-domain and open-vocabulary settings complicate the task considerably and demand scalable solutions. In this paper we present a new approach to {DST} which makes use of various copy mechanisms to fill slots with values. Our model has no need to maintain a list of candidate values. Instead, all values are extracted from the dialog context on-the-fly. A slot is filled by one of three copy mechanisms: (1) Span prediction may extract values directly from the user input; (2) a value may be copied from a system inform memory that keeps track of the system\textquoterights inform operations (3) a value may be copied over from a different slot that is already contained in the dialog state to resolve coreferences within and across domains. Our approach combines the advantages of span-based slot filling methods with memory methods to avoid the use of value picklists altogether. We argue that our strategy simplifies the {DST} task while at the same time achieving state of the art performance on various popular evaluation sets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55\%.},
booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue}
}
@inproceedings{devlin_2019,
title = {{BERT}: Pre-training of deep bidirectional transformers for language understanding},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
pages = {4171:4186},
publisher = {Association for Computational Linguistics},
url = {https://arxiv.org/abs/1810.04805},
year = {2019},
urldate = {2022-10-27},
volume = {abs/1905.07213},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}
}
@inproceedings{elnouby_2021,
title = {{XCiT}: Cross-Covariance Image Transformers},
author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Herve},
editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
url = {https://openreview.net/forum?id={kzPtpIpF8o}},
year = {2021},
urldate = {2022-10-27},
booktitle = {Advances in Neural Information Processing Systems}
}
@incollection{halliday_nda,
title = {Language as code and language as behaviour: a systemic-functional interpretation of the nature and ontogenesis of dialogue},
author = {Halliday, M. A. K.},
series = {Linguistics: Bloomsbury Academic Collections},
pages = {3-36},
publisher = {Bloomsbury Academic},
url = {http://www.bloomsburycollections.com/book/semiotics-of-culture-and-language-volume-1-language-as-social-semiotic/ch1-language-as-code-and-language-as-behaviour-a-systemic-functional-interpretation-of-the-nature-and-ontogenesis-of-dialogue/},
urldate = {2022-10-27},
edition = {1},
isbn = {978-1-4742-8573-5},
address = {London}
}
@inproceedings{schank_1969,
title = {A conceptual dependency parser for natural language},
author = {Schank, Roger C. and Tesler, Larry},
pages = {1-3},
publisher = {Association for Computational Linguistics},
url = {http://portal.acm.org/citation.cfm?doid=990403.990405},
year = {1969},
month = {sep},
day = {1},
urldate = {2022-11-04},
doi = {10.3115/990403.990405},
address = {Morristown, {NJ}, {USA}},
booktitle = {Proceedings of the 1969 conference on Computational linguistics -}
}
@article{woods_1970,
title = {Transition network grammars for natural language analysis},
author = {Woods, W. A.},
pages = {591-606},
url = {https://dl.acm.org/doi/10.1145/355598.362773},
year = {1970},
month = {oct},
urldate = {2022-11-04},
journal = {Communications of the {ACM}},
volume = {13},
number = {10},
issn = {0001-0782},
doi = {10.1145/355598.362773},
abstract = {The use of augmented transition network grammars for the analysis of natural language sentences is described. Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing. The advantages of this model for natural language analysis are discussed in detail and illustrated by examples. An implementation of an experimental parsing system for transition network grammars is briefly described.}
}
@incollection{johnson_1988,
booktitle = {Mind, Language, Machine},
title = {{SHRDLU}, Procedures, Mini-World},
author = {Johnson, Michael L.},
pages = {113-122},
publisher = {Palgrave Macmillan {UK}},
url = {http://link.springer.com/10.1007/978-1-349-19404-9\_20},
year = {1988},
urldate = {2022-11-04},
isbn = {978-1-349-19404-9},
doi = {10.1007/978-1-349-19404-9\_20},
address = {London}
}
@article{leven_1996,
title = {The roots of backpropagation: From ordered derivatives to neural networks and political forecasting},
author = {Leven, Sam},
pages = {543-544},
url = {https://linkinghub.elsevier.com/retrieve/pii/0893608096900155},
year = {1996},
month = {apr},
urldate = {2022-11-04},
journal = {Neural Networks},
volume = {9},
number = {3},
issn = {08936080},
doi = {10.1016/0893-6080(96)90015-5}
}
@incollection{weizenbaum_2021,
booktitle = {Ideas that created the future: classic papers of computer science},
title = {{ELIZA\textemdashA} Computer Program for the Study of Natural Language Communication between Man and Machine (1966)},
author = {Weizenbaum, Joseph},
editor = {Lewis, Harry R.},
pages = {271-278},
publisher = {The {MIT} Press},
url = {http://direct.mit.edu/books/book/5003/chapter/2657050/{ELIZA}-A-Computer-Program-for-the-Study-of-Natural},
year = {2021},
month = {feb},
day = {2},
urldate = {2022-11-04},
isbn = {9780262363174},
doi = {10.7551/mitpress/12274.003.0029}
}
@article{colby_1972,
title = {Experimental validation of a computer simulation of paranoid processes},
author = {Colby, Kenneth M. and Hilf, Franklin D. and Weber, Sylvia and Kraemer, Helena},
pages = {187-191},
url = {https://linkinghub.elsevier.com/retrieve/pii/0025556472900739},
year = {1972},
month = {oct},
urldate = {2022-11-04},
journal = {Mathematical Biosciences},
volume = {15},
number = {1-2},
issn = {00255564},
doi = {10.1016/0025-5564(72)90073-9}
}
@article{wilcox_2013,
title = {Making it real: Loebner-winning chatbot design},
author = {Wilcox, Bruce and Wilcox, Sue},
pages = {a086},
url = {http://arbor.revistas.csic.es/index.php/arbor/article/view/1888/2079},
year = {2013},
month = {dec},
day = {30},
urldate = {2022-11-04},
journal = {Arbor},
volume = {189},
number = {764},
issn = {1988-{303X}},
doi = {10.3989/arbor.2013.764n6009}
}
@inproceedings{lafferty_2004,
title = {Kernel conditional random fields: Representation and clique selection},
author = {Lafferty, John and Zhu, Xiaojin and Liu, Yan},
pages = {64},
publisher = {{ACM} Press},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015337},
year = {2004},
month = {jul},
day = {4},
urldate = {2022-11-04},
isbn = {1581138285},
doi = {10.1145/1015330.1015337},
address = {New York, New York, {USA}},
booktitle = {Twenty-first international conference on Machine learning - {ICML} '04}
}
@misc{na_website_nd,
title = {Latent dirichlet allocation \textbar The Journal of Machine Learning Research},
url = {https://dl.acm.org/doi/10.5555/944919.944937},
urldate = {2022-11-04},
type = {WEBSITE}
}
@misc{na_website_nda,
title = {Distant supervision for relation extraction without labeled data - {ACL} Anthology},
url = {https://aclanthology.org/P09-1113/},
urldate = {2022-11-04},
type = {WEBSITE}
}
@misc{na_website_ndb,
title = {Improved backing-off for M-gram language modeling \textbar {IEEE} Conference Publication \textbar {IEEE} Xplore},
url = {https://ieeexplore.ieee.org/document/479394},
urldate = {2022-11-04},
type = {WEBSITE}
}
@misc{na_website_ndc,
title = {A neural probabilistic language model \textbar The Journal of Machine Learning Research},
url = {https://dl.acm.org/doi/10.5555/944919.944966},
urldate = {2022-11-04},
type = {WEBSITE}
}
@inproceedings{mikolov_2010,
title = {Recurrent neural network based language model},
author = {Mikolov, Tomáš and Karafiát, Martin and Burget, Lukáš and Černocký, Jan and Khudanpur, Sanjeev},
pages = {1045-1048},
publisher = {ISCA},
url = {https://www.isca-speech.org/archive/interspeech\_2010/mikolov10\_interspeech.html},
year = {2010},
month = {sep},
day = {26},
urldate = {2022-11-04},
doi = {10.21437/Interspeech.2010-343},
address = {ISCA},
booktitle = {Interspeech 2010}
}
@misc{na_website_ndd,
title = {Generating Sequences With Recurrent Neural Networks - {NASA}/{ADS}},
url = {https://ui.adsabs.harvard.edu/abs/{2013arXiv1308}.{0850G}/abstract},
urldate = {2022-11-04},
type = {WEBSITE}
}
@phdthesis{suskever_2013,
title = {Training Recurrent Neural Networks},
author = {Suskever, Ilya},
url = {https://www.cs.utoronto.ca/\~ilya/pubs/ilya\_sutskever\_phd\_thesis.pdf},
year = {2013},
urldate = {2022-11-04},
type = {{THESIS}.{DOCTORAL}}
}
@misc{na_website_nde,
title = {Sequence to sequence learning with neural networks \textbar Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
url = {https://dl.acm.org/doi/10.5555/2969033.2969173},
urldate = {2022-11-04},
type = {WEBSITE}
}
@inproceedings{devlin_2019a,
title = {{BERT}: Pre-training of deep bidirectional transformers for language understanding},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
pages = {4171:4186},
url = {https://arxiv.org/abs/1810.04805},
year = {2019},
urldate = {2022-11-04},
volume = {abs/1905.07213},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}
}
@article{mller_2019,
title = {When Does Label Smoothing Help?},
author = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
url = {https://arxiv.org/abs/1906.02629},
year = {2019},
urldate = {2022-11-04},
journal = {arXiv},
doi = {10.48550/arxiv.1906.02629},
abstract = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.}
}
@incollection{halliday_ndb,
title = {Language as code and language as behaviour: a systemic-functional interpretation of the nature and ontogenesis of dialogue},
author = {Halliday, M. A. K.},
series = {Linguistics: Bloomsbury Academic Collections},
pages = {3-36},
publisher = {Bloomsbury Academic},
url = {http://www.bloomsburycollections.com/book/semiotics-of-culture-and-language-volume-1-language-as-social-semiotic/ch1-language-as-code-and-language-as-behaviour-a-systemic-functional-interpretation-of-the-nature-and-ontogenesis-of-dialogue/},
urldate = {2022-11-04},
edition = {1},
isbn = {978-1-4742-8573-5},
address = {London}
}
@inproceedings{sukhbaatar_2015,
title = {End-To-End Memory Networks},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
publisher = {Curran Associates, Inc.},
url = {https://proceedings.neurips.cc/paper/2015/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf},
year = {2015},
urldate = {2022-11-04},
volume = {28},
booktitle = {Advances in Neural Information Processing Systems}
}
@article{bahdanau_2014,
title = {Neural Machine Translation by Jointly Learning to Align and Translate},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
url = {https://arxiv.org/abs/1409.0473},
year = {2014},
urldate = {2022-11-04},
journal = {arXiv},
doi = {10.48550/arxiv.1409.0473},
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.}
}
@inproceedings{kumar_2016,
title = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
series = {Proceedings of Machine Learning Research},
pages = {1378-1387},
publisher = {PMLR},
url = {https://proceedings.mlr.press/v48/kumar16.html},
year = {2016},
urldate = {2022-11-04},
volume = {48},
address = {New York, New York, {USA}},
abstract = {Most tasks in natural language processing can be cast into question answering ({QA}) problems over language input. We introduce the dynamic memory network ({DMN}), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The {DMN} can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook\textquoterights {bAbI} dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging ({WSJ}-{PTB}). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning}
}
@article{bengio_2003,
title = {A Neural Probabilistic Language Model},
author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Janvin, Christian},
pages = {1137–1155},
publisher = {{JMLR}.org},
year = {2003},
month = {mar},
urldate = {2022-11-04},
journal = {J. Mach. Learn. Res.},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.}
}
@inproceedings{kneser_1995,
title = {Improved backing-off for M-gram language modeling},
author = {Kneser, R. and Ney, H.},
pages = {181-184},
publisher = {IEEE},
url = {http://ieeexplore.ieee.org/document/479394/},
year = {1995},
urldate = {2022-11-04},
isbn = {0-7803-2431-5},
doi = {10.1109/{ICA\SSP}.1995.479394},
booktitle = {1995 International Conference on Acoustics, Speech, and Signal Processing}
}
@article{blei_2003,
title = {Latent Dirichlet Allocation},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
pages = {993–1022},
publisher = {{JMLR}.org},
year = {2003},
month = {mar},
urldate = {2022-11-04},
journal = {J. Mach. Learn. Res.},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We describe latent Dirichlet allocation ({LDA}), a generative probabilistic model for collections of discrete data such as text corpora. {LDA} is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an {EM} algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic {LSI} model.}
}
@phdthesis{werbos_1974,
title = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
author = {Werbos, Paul},
school = {Harvard University},
year = {1974},
urldate = {2022-11-04},
type = {{THESIS}.{DOCTORAL}}
}
@misc{na_website_ndf,
title = {Jabberwacky - Wikipedia},
url = {https://en.wikipedia.org/wiki/Jabberwacky},
urldate = {2022-11-04},
type = {WEBSITE}
}
@article{abushawar_2015,
title = {{ALICE} chatbot: trials and outputs},
author = {{AbuShawar}, Bayan and Atwell, Eric},
url = {http://cys.cic.ipn.mx/ojs/index.php/{CyS}/article/view/2326},
year = {2015},
month = {dec},
day = {27},
urldate = {2022-11-04},
journal = {Computación y Sistemas},
volume = {19},
number = {4},
issn = {2007-9737},
doi = {10.13053/cys-19-4-2326}
}
@misc{na_website_ndg,
title = {{DREAM} Team / Alexa Prize 3},
url = {https://deeppavlov.ai/challenges/dream\_alexa\_3},
urldate = {2022-11-08},
type = {WEBSITE}
}
@misc{na_website_ndh,
title = {Alexa Prize {SocialBot} Grand Challenge 3 - Amazon Science},
url = {https://www.amazon.science/alexa-prize/socialbot-grand-challenge/2019},
urldate = {2022-11-08},
type = {WEBSITE}
}
@misc{na_website_ndi,
title = {Alexa Prize {SocialBot} Grand Challenge 4 - Amazon Science},
url = {https://www.amazon.science/alexa-prize/socialbot-grand-challenge/2020},
urldate = {2022-11-08},
type = {WEBSITE}
}
@misc{na_website_ndj,
title = {{DREAM} Team / Alexa Prize 4},
url = {https://deeppavlov.ai/challenges/dream\_alexa\_4},
urldate = {2022-11-08},
type = {WEBSITE}
}
@article{baymurzina_2021,
title = {{DREAM} technical report for the Alexa Prize 4},
author = {Baymurzina, Dilyara and Kuznetsov, Denis and Evseev, Dmitry and Karpov, Dmitry and Sagirova, Alsu and Peganov, Anton and Ignatov, Fedor and Ermakova, Elena and Cherniavskii, Daniil and Kumeyko, Sergey and Serikov, Oleg and Kuratov, Yury and Ostyakova, Lidiya and Kornev, Daniel and Burtsev, Mikhail},
url = {https://www.amazon.science/alexa-prize/proceedings/dream-technical-report-for-the-alexa-prize-4},
year = {2021},
urldate = {2022-11-08},
journal = {Alexa Prize {SocialBot} Grand Challenge 4 Proceedings}
}
@article{kuratov_2021,
title = {Socialbot {DREAM} in alexa prize challenge 2019},
author = {Kuratov, Y. M. and Yusupov, I. F. and Baymurzina, D. R. and Kuznetsov, D. P. and Cherniavskii, D. V. and Dmitrievskiy, A. and Ermakova, E. S. and Ignatov, F. S. and Karpov, D. A. and Kornev, D. A. and Le, T. A. and Pugin, P. Y. and Burtsev, M. S.},
pages = {62-89},
url = {https://elibrary.ru/doi\_resolution.asp?doi=10\%{2E53815\}%{2F20726759\}%{5F2021\}%{5F13\}%{5F3\}%{5F62}},
year = {2021},
urldate = {2022-11-08},
journal = {Proceedings of Moscow Institute of Physics and Technology},
volume = {13},
number = {3},
issn = {2072-6759},
doi = {10.53815/20726759\_2021\_13\_3\_62}
}
@misc{na_website_ndk,
title = {Overview \textbar Docker Documentation},
url = {https://docs.docker.com/compose/},
urldate = {2022-11-09},
type = {WEBSITE}
}
@misc{na_website_ndl,
title = {Pre-trained embeddings \textemdash {DeepPavlov} 1.0.0 documentation},
url = {http://docs.deeppavlov.ai/en/master/features/pretrained\_vectors.html\#bert},
urldate = {2022-11-09},
type = {WEBSITE}
}
@misc{na_website_ndm,
title = {Jigsaw Unintended Bias in Toxicity Classification \textbar Kaggle},
url = {https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/discussion},
urldate = {2022-11-09},
type = {WEBSITE}
}
@misc{na_website_ndn,
title = {{DeepPavlov}/sentiment\_sst\_conv\_bert.json at 0.9.0 · deeppavlov/{DeepPavlov} · {GitHub}},
url = {https://github.com/deepmipt/{DeepPavlov}/blob/0.9.0/deeppavlov/configs/classifiers/sentiment\_sst\_conv\_bert.json},
urldate = {2022-11-09},
type = {WEBSITE}
}
@misc{dp_conv_bert,
title = {Conversational english BERT from DeepPAVLOV},
url = {https://huggingface.co/DeepPavlov/bert-base-cased-conversational},
urldate = {2022-11-09},
type = {WEBSITE}
}
@misc{na_website_ndo,
title = {Emotion classification dataset (retrieved from Kaggle, supplemented by the neutral example from {ScenarioSA} dataset)},
url = {http://files.deeppavlov.ai/datasets/{EmotionDataset}.rar},
urldate = {2022-11-10},
type = {WEBSITE}
}
@misc{na_website_ndp,
title = {Eray Yildiz \textbar Novice \textbar Kaggle},
url = {https://www.kaggle.com/eray1yildiz},
urldate = {2022-11-09},
type = {WEBSITE}
}
@misc{na_website_ndq,
title = {{SocialBot} Grand Challenge Rules - Amazon Science},
url = {https://www.amazon.science/alexa-prize/socialbot-grand-challenge/rules},
urldate = {2022-11-09},
type = {WEBSITE}
}
@phdthesis{na_2021,
title = {Нейросетевые модели и диалоговая система для ведения разговора на общие темы},
author = {Баймурзина, Диляра},
school = {МФТИ},
url = {https://mipt.ru/upload/medialibrary/e31/dissertation\_baymurzina.pdf},
year = {2021},
month = {oct},
day = {27},
urldate = {2022-11-09},
type = {{THESIS}.{DOCTORAL}}
}
@misc{na_website_ndr,
title = {{GitHub} - {C\SSEGISandData}/{COVID}-19: Novel Coronavirus ({COVID}-19) Cases, provided by {JHU} {C\SSE}},
url = {https://github.com/{C\SSEGISandData}/{COVID}-19},
urldate = {2022-11-10},
type = {WEBSITE}
}
@misc{na_website_nds,
title = {Evi(software) - Wikipedia},
url = {https://en.wikipedia.org/wiki/Evi\_(software)},
urldate = {2022-11-10},
type = {WEBSITE}
}
@misc{na_website_ndt,
title = {Goodreads \textbar Meet your next favorite book},
url = {https://www.goodreads.com/},
urldate = {2022-11-10},
type = {WEBSITE}
}
@misc{na_website_ndu,
title = {Reddit - Dive into anything},
url = {https://www.reddit.com/},
urldate = {2022-11-10},
type = {WEBSITE}
}
@misc{na_website_ndv,
title = {Kubernetes - Wikipedia},
url = {https://ru.wikipedia.org/wiki/Kubernetes},
urldate = {2022-11-10},
type = {WEBSITE}
}
@misc{na_website_ndw,
title = {Wikidata},
url = {https://www.wikidata.org/wiki/Wikidata:{Main\_Page}},
urldate = {2022-11-10},
type = {WEBSITE}
}
@misc{na_website_ndx,
title = {{wikiHow}: How-to instructions you can trust.},
url = {https://www.wikihow.com/Main-Page},
urldate = {2022-11-10},
type = {WEBSITE}
}
@inproceedings{na_2019,
title = {Разработка диалоговой системы с интеграцией профиля личности},
author = {Болотин, Даниил and Карпов, Дмитрий and Рашков, Григорий and Шкурак, Иван},
publisher = {MIPT},
url = {http://2019.en-t.info/old/articles/ent2018-thesis.pdf},
year = {2019},
urldate = {2022-11-10},
volume = {5th International Conference on Engineering and Telecommunication {EnT}-{MIPT} 2018}
}
@patent{na_2021a,
title = {Свидетельство о депонировании зарегистрированного в базе данных {IREG} \#2018449 "Texter ocr-cv-nlp-microservise"},
author = {Дуплякин, Владислав and Дмитрий, Карпов and Ондар, Адыгжы and Ушаков, Алексей},
year = {2021},
urldate = {2022-11-10},
nationality = {IREG},
number = {2018449}
}
@misc{na_website_ndy,
title = {Dialog Flow Framework},
url = {https://deeppavlov.ai/dff},
urldate = {2022-11-10},
type = {WEBSITE}
}
@inproceedings{roller_2021,
title = {Recipes for Building an Open-Domain Chatbot},
author = {Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Smith, Eric Michael and Boureau, Y-Lan and Weston, Jason},
pages = {300-325},
publisher = {Association for Computational Linguistics},
url = {https://aclanthology.org/2021.eacl-main.24},
year = {2021},
urldate = {2022-11-10},
doi = {10.18653/v1/2021.eacl-main.24},
address = {Stroudsburg, {PA}, {USA}},
booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}
}
@article{hedayatnia_2020a,
title = {Policy-Driven Neural Response Generation for Knowledge-Grounded Dialogue Systems},
author = {Hedayatnia, Behnam and Kim, Seokhwan and Liu, Yang and Gopalakrishnan, Karthik and Eric, Mihail and Hakkani-Tür, Dilek},
url = {https://arxiv.org/abs/2005.12529},
year = {2020},
urldate = {2022-11-10},
journal = {CoRR},
volume = {abs/2005.12529}
}
@misc{na_website_ndz,
title = {Questions on Yahoo Answers labeled as either informational or conversational, version 1.0, L31 {YAHOO} dataset},
url = {https://webscope.sandbox.yahoo.com/catalog.php?datatype=l\&did=82},
urldate = {2022-11-13},
type = {WEBSITE}
}
@misc{na_website_ndaa,
title = {Hugging Face - Wikipedia},
url = {https://en.wikipedia.org/wiki/{Hugging\_Face}},
urldate = {2022-11-20},
type = {WEBSITE}
}
@article{lin_2017,
title = {A Structured Self-attentive Sentence Embedding},
author = {Lin, Zhouhan and Feng, Minwei and Ccero Nogueira dos Santos, and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
url = {http://arxiv.org/abs/1703.03130},
year = {2017},
urldate = {2022-11-20},
journal = {CoRR},
volume = {abs/1703.03130}
}
@article{ba_2016,
title = {Layer Normalization},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
url = {https://arxiv.org/abs/1607.06450},
year = {2016},
urldate = {2022-11-20},
journal = {arXiv},
doi = {10.48550/arxiv.1607.06450},
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.}
}
@article{gehring_2017,
title = {Convolutional Sequence to Sequence Learning},
author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
url = {https://arxiv.org/abs/1705.03122},
year = {2017},
urldate = {2022-11-20},
journal = {arXiv},
doi = {10.48550/arxiv.1705.03122},
abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep {LSTM} setup of Wu et al. (2016) on both {WMT}'14 English-German and {WMT}'14 English-French translation at an order of magnitude faster speed, both on {GPU} and {CPU}.}
}
@article{liu_2018,
title = {Stochastic Answer Networks for Natural Language Inference},
author = {Liu, Xiaodong and Duh, Kevin and Gao, Jianfeng},
url = {https://arxiv.org/abs/1804.07888},
year = {2018},
urldate = {2022-11-24},
journal = {arXiv},
doi = {10.48550/arxiv.1804.07888},
abstract = {We propose a stochastic answer network ({SAN}) to explore multi-step inference strategies in Natural Language Inference. Rather than directly predicting the results given the inputs, the model maintains a state and iteratively refines its predictions. Our experiments show that {SAN} achieves the state-of-the-art results on three benchmarks: Stanford Natural Language Inference ({SNLI}) dataset, {MultiGenre} Natural Language Inference ({MultiNLI}) dataset and Quora Question Pairs dataset.}
}
@article{cho_2014,
title = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
author = {Cho, {KyungHyun} and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
url = {http://arxiv.org/abs/1409.1259},
year = {2014},
urldate = {2022-11-24},
journal = {CoRR},
volume = {abs/1409.1259}
}
@book{ferreirs_2010,
title = {Proceedings of {COMPSTAT}'2010: 19th International Conference on Computational {StatisticsParis} France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
author = {Ferreirós, José},
editor = {Lechevallier, Yves and Saporta, Gilbert},
pages = {651},
publisher = {Physica},
year = {2010},
urldate = {2022-11-24},
edition = {2010},
isbn = {978-3-7908-2604-3},
address = {Heidelberg},
abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.}
}
@book{bousquet_2004,
title = {Advanced Lectures on Machine Learning: {ML} Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised ... (Lecture Notes in Computer Science (3176))},
author = {Bousquet, Olivier. and Luxburg, Ulrike von. and Rätsch, Gunnar.},
pages = {256},
publisher = {Springer},
year = {2004},
urldate = {2022-11-24},
edition = {2004},
isbn = {978-3-540-23122-6},
address = {Berlin}
}
@phdthesis{na_2022,
title = {Методы переноса знаний для нейросетевых моделей обработки естественного языка},
author = {Коновалов, Василий},
school = {МФТИ},
url = {https://mipt.ru/upload/medialibrary/33e/dissertatsiya-konovalov-vasiliy-pavlovich.pdf},
year = {2022},
urldate = {2022-11-24},
type = {{THESIS}.{DOCTORAL}}
}
@article{erikftjongkimsang_2003,
title = {Introduction to the {CoNLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
author = {Erik F. Tjong Kim Sang, and De Meulder, Fien},
url = {http://arxiv.org/abs/cs/0306050},
year = {2003},
urldate = {2022-11-24},
journal = {CoRR},
volume = {cs.{CL}/0306050}
}