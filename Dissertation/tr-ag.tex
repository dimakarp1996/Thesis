\chapter{Трансформер-агностичные модели}\label{ch:tr-ag}
% TODO - ЧТОБЫ ВЕЗДЕ НАЗЫВАЛИСЬ АГНОСТИЧНЫМИ, А НЕ ИНВАРИАНТНЫМИ!!!!
Многозадачные нейросетевые модели, описанные в предыдущем разделе \ref{ch:pseudolabel}, позволяют добиться существенной экономии вычислительных ресурсов. Тем не менее, к числу их неустранимых недостатков относится негибкость. Они поддерживают только один тип голов (multilabel), они требуют наличия меток для каждой задачи у каждого примера. 

В связи с этим была поставлена цель поиска и исследования более эффективных архитектур. В частности, было необходимо предложить многозадачную нейросетевую архитектуру, лишенную описанных выше недостатков архитектуры из \ref{ch:pseudolabel}. Помимо этого, необходимо было исследовать перенос знаний в предложенной нейросетевой архитектуре: как перенос знаний между задачами, так и перенос знаний между языками при использовании многозадачных моделей. При этом задачи для исследования было необходимо выбрать исходя из потребностей реальных диалоговых платформ, например, платформы DREAM\ref{ch:dream}. 

Архитектура, исследование которой проводилось, описана в следующем разделе. Попытки улучшения данной архитектуры описаны отдельно. 

\subsection{Архитектура трансформер-агностичной многозадачной модели}\label{ch:tr-ag:architecture} 
Предложенная многозадачная трансформер-агностичная модель основана на классе \texttt{AutoModel} из HuggingFace, который разрешает использовать разнве модели, основанные на архитектуре Трансформер\footnote{Список поддерживаемых моделей: \url{https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\#automodel}}. Для наших экспериментов, мы использовали модели, основанные на архитектуре типа BERT, но абсолютно тот же подход может применяться и к другим нейросетевым моделям на базе архитектуры Трансформер. 

Этот подход заключается в следующем:
\begin{itemize}

    \item[*] Как и в оригинальной статье~\cite{bert}, возвращаются финальные скрытые состояния для всех токенов и выход пулингового слоя BERT. 

    \item[*] К выходу пулера применяется дропаут, по умолчанию равный 0.2\footnote{Как варьирование dropout, так и варьирование attention dropout не привело к улучшению характеристик модели.}. Для задач выбора из нескольких вариантов ответа или классификации каждого токена в предложении, выход преобразуется аналогично соответствующим задачам. 

    \item[*]После этого этапа, мы применяем задаче-специфичный линейный слой с размерностью выхода \texttt{n}. Для всех задач, кроме регрессии или выбора из нескольких вариантов ответа\footnote{Для задачи выбора из нескольких вариантов ответа, каждой метке изначально относится несколько примеров. Т.е с таким числом классов после преобразования формы число реальных и предсказанных меток соответствует друг другу.}, \texttt{n} равняется числу классов для задачи. Во всех других случаях \texttt{n} равняется 1.
 
    \item[*]В конце применяется функция потерь. Если для каждого примера из задачи ожидается только одна метка, применяется категорическая кросс-энтропия, в остальных случаях - бинарная кросс-энтропия. 

\end{itemize}

Обращу отдельное внимание на то, что в каждом тренировочном батче для данной нейросетевой многозадачной модели должны содержаться примеры только из одной задачи. В противном случае эффективный размер батча для каждой из задач оказывался меньше задаваемого изначально, что приводило к ухудшению метрик модели. 

Такая многозадачная модель почти не требует дополнительных параметров, и, как следствие, дополнительных вычислительных ресурсов(кроме линейных слоев, которые требуют на порядки меньше ресурсов, чем тело архитектуры Трансформер). Так, для моделей типа distilBERT, предложенная многозадачная модель требует всего лишь примерно на  0.1\% больше видеопамяти, чем любая из соответствующих ей однозадачных моделей. В зависимости от числа задач, числа классов и "тела" многозадачной модели, количество дополнительных параметров варьируется вокруг этого числа. 

При этом данная модель является трансформер-агностичной, что позволяет быстро подставлять в нее разные типы базовых моделей. Это выгодно отличает данную модель от иерархических многозадачных моделей, таких, как \cite{PAL:19} и \cite{TaskEmbedded2021}. 

Отдельно выделяю то, что данная модель успешно интегрирована в библиотеку DeepPavlov\cite{Burtsev2018DeepPavlovAO}.

\subsection{Какие эксперименты не сработали}\label{ch:tr-ag:failed_attempts} 
Автор диссертационной работы пробовал большое количество различных вариантов улучшения архитектуры многозадачной трансформер-агностичной модели по сравнению с описанными в разделе \ref{ch:tr-ag:architecture}. 
\begin{itemize} 
 \item[*] Первой серией идей была модификация выхода CLS-выхода модели BERT, который классифицируется задаче-специфичными головами. Для данной задачи проверялось использование задаче-специфичных слоев, описанных в \cite{GhostBERT2021, TaskEmbedded2021, el-nouby2021xcit}. Кроме данных слоев, проверялось также использование LSTM-представления CLS токена или конкатенация этого представления с самим CLS токеном. При проверке на валидационном наборе данных GLUE, прироста средней точности по сравнению с описаннрй выше архитектурой данные эксперименты не дали. 
\item[*] Второй серией идей являлось использование задаче-специфичных тренируемых токенов. А именно, идеи заключались в том, чтобыприсваивать каждой из задач какой-нибудь неиспользуемый токен из словаря BERT (каждой задаче - свой токен). И добавлять этот токен сразу же после CLS к каждой из задач. После чего проводить классификацию не просто по CLS токену, а либо по конкатенации CLS и задаче-специфичного токена, либо же просто по задаче-специфичному токену. На наборе данных GLUE эти идеи не привели к улучшению метрик.

\ю Для случая, когда к каждому примеру добавляется не один задаче-специфичный токен, а сразу все задаче-специфичные токены по очереди (то есть для трехзадачной классификации, условно, каждый пример имеет вид \textit{[CLS] [TOKEN FOR TASK1] [TOKEN FOR TASK2] [TOKEN FOR TASK3] [TOKENIZED_EXAMPLE]}, улучшения при проведении классификации как в предыдущем пункте также не последовало. 

Вероятно, провал этих экспериментов связан с тем, что если CLS-токен был хорошо предобучен на оригинальном BERT для проведения классификации, то задаче-специфичные токены предобучены не были, а какая-то дополнительная полезная информация, зависящая от задачи, через них так и не передавалась(ну или наоборот, она слишком хорошо передавалась через [CLS]. 

\item][*] Хотя исследование разных методов сэмплирования задач при многозадачном обучении не является фокусом данной диссертационной работы, автор также проводил эксперименты и с разными методами сэмплирования. Так, вариант перехода для каждой задачи в режим сходимости (с делением функции потерь для этой задачи на 4),если на ней нет заметного улучшения, и выхода из этого режима, если на этой задаче появляется заметное ухудшение, не сработал. Эксперимент, в котором после некого числа эпох (достаточно большого, чтобы измерить дисперсию модели), модель обучалась только на некой доле самых сложных примеров с самой высокой дисперсией, тоже не сработал\footnote{Справедливости ради, существуют и эффективные способы улучшения сэмплирования, дававшие заметный прирост метрик - например, \cite{GradTS}. Данный подход вполне можно объединить с предлагаемым в моей диссертации.}. 

В числе неудачных экспериментов также можно упомянуть эксперименты по дистилляции более крупных моделей BERT в более мелкие, которые проводились путем добавления в функцию потерь нового члена, "приближающего" веса более крупной модели к соответствующим весам более мелкой (по метрике mean squared error). Эта идея не сработала. Автор предполагает, что это было связано с тем, что данный способ является слишком грубым для того, чтобы приблизиться к адекватной аппроксимации сложных нейросетевых функций, которые используются в модели BERT. В связи ч этим, в дальнейшем идеи с приближением весов не проверялись. 
\subsection{Преимущество трансформер-агностичной многозадачной модели над многозадачной моделью с одним линейным слоем} 
\label{ch:tr-ag:advantages}

Многозадачная трансформер-агностичная модель является логичным усовершенствованием многозадачной модели с одним линейным слоем. По сравнению с данной моделью, многозадачная трансформер-агностичная модель имеет больше гибкости, так как не требует меток для каждой из задач у каждого примера. При этом подобная модель лучше адаптируется к каждой конкретной задаче, так как при обучении на примерах из каждой задачи у этой модели обновляются веса только тех нейронов в финальных линейных, которые отвечают конкретно за эту задачу. В то же время, у модели с одним линейным слоем пример из каждой задачи при обучении чуточку "сбивает" веса \textbf{всех} задач. 

Данный эффект можно сгладить, используя псевдоразметку каждого примера для каждой задачи уже обученными однозадачными моделями, но это требует больших затрат времени и вычислительных мощностей, а также вносит в обучающую выборку для каждой задачи искажения, связанные с необходимостью видеть большое количество не свойственных этой задаче примеров, размеченных каким-то неидеальным образом. Так, для задачи классификации тональности доля положительных обучающих примеров после псевдоразметки, описанной в главе \ref{mtldream:tr-ag}, уменьшилась с 42.4\% до 12.7\% \ref{appendix:sentiment}. При этом все плюсы от псевдоразметки там, где она оправдана, можно получить и при применении многозадачной трансформер-агностичной модели - которая к тому же предоставляет и возможность выбора, на каких задачах делать псевдоразметку, а на каких не делать.

Другое преимущество многозадачной трансформер-агностичной модели, проявляющееся даже на параллельно размеченных данных, заключается в большей гибкости голов. Если модель с одним линейным слоем по факту поддерживает только один тип головы, а имеено multilabel, то многозадачная трансформер-агностичная модель поддерживает и singlelabel головы для каждой задачи. Реальное применение показало, что на тех задачах, где выход модели можно представить в singlelabel виде, его лучше представлять в singlelabel виде. Это, по всей видимости, связано с тем, что задача "выбрать один самый вероятный класс" проще для линейного слоя, чем задача "выбрать сколько-то классов, чья вероятность выше какой-то границы". И применяя трансформер-агностичную модель, мы можем сознательно выбирать решение именно этой задачи.

Еще одно преимущество многозадачной трансформер-агностичной модели, которое в рамках этой работы еще не было раскрыто до конца, заключается в том, что она поддерживает больший спектр задач. В частности, она (в реализации на момент написания диссертационной работы) поддерживает такие задачи, как распознавание именованных сущностей или выбор из нескольких вариантов ответа, которые невозможно реализовать в рамках модели с одним линейным слоем. 


\section{Datasets}


We explored the performance of the multi-task mode from the subset of datasets necessary for the dialog systems~\cite{wochat,lrec}. We trained models on datasets for five problems, i.e emotion classification, toxicity classification, sentiment classification, intent classification, and topic classification. Due to the scope of our work, we prepared a Russian-language dataset and an English-language dataset for every task. We note that for the Russian-language dataset and the English-language dataset, the indexes of the same classes used by the models were also the same. The sizes of all datasets by class and split can be found in Appendix. 

\subsection{Emotion classification }

As an English language dataset, we used dataset \texttt{go\_emotions}~\cite{emotions} for the emotion classification. We used Ekman-grouped emotions, grouping them into seven types, i.e \textit{anger}, \textit{fear}, \textit{disgust}, \textit{joy}, \textit{surprise}, \textit{sadness}, and \textit{neutral}. After such grouping, we selected only single-label examples. There were 39.5k such training examples. The train/test/validation split of this dataset was approximately 80/10/10.


For the emotion classification, we used the \texttt{go\_emotions}~\cite{emotions} dataset. This dataset consists of short comments from Reddit, such as \textit{LOL. Super cute!} or \textit{Yikes. I admire your patience}. We have grouped all emotion classes from this dataset into seven Ekman types, i.e \textit{anger}, \textit{fear}, \textit{disgust}, \textit{joy}, \textit{surprise}, \textit{sadness}, and \textit{neutral}. After such grouping, we selected only single-label examples. We leave exploring multi-task learning with the multilabel data for future research. - ВСТАВИТЬ ИЗ ТОГО РАЗДЕЛА ЧТО НЕ ВЫШЛО

As a Russian language dataset, we used for this task dataset \texttt{CEDR}~\cite{ru_emotions}\footnote{We retrieved this dataset from the URL \url{https://huggingface.co/datasets/cedr}}. This dataset has 5 classes - \textit{anger}, \textit{fear}, \textit{joy}, \textit{surprise}, and \textit{sadness} - but the samples from this dataset can belong to more than 1 class or (unlike \texttt{go\_emotions}) belong to no class. 

From this dataset, we selected only examples that belong to 1 class or that have no class, labeling no-class examples as \textit{neutral}. The class nomenclature of this dataset was almost the same as for the English dataset, except for the \textit{disgust} class. Nonetheless, as \textit{disgust} examples comprised less than 1.5\% of the English training samples, it didn't impact knowledge transfer much.

The work~\cite{ru_emotions} provided only the train-test split of the \texttt{CEDR} dataset, which is 80/20. We singled out 12.5\% of the training examples from CEDR as the validation set. 

\subsection{Sentiment classification}

As an English language dataset, we used dataset \texttt{DynaSent}(r1)~\cite{sentiment} for the sentiment classification. We used only examples from the first found of the collection, to match the Russian data by difficulty. This single-label dataset with 80.5k training samples has three classes - positive, negative, and neutral. One can contend that classes from the \texttt{go\_emotions} dataset can be grouped by sentiment, as proposed by the work~\cite{emotions}, but also by emotions. However, our experiments show that augmentation of the sentiment dataset with such samples decreases the metrics rather than increasing them. That shows that these datasets are sufficiently different to be used separately, as sentiment-grouping of the emotion dataset is too rough. This dataset has 3.6k validation samples and the same number of test samples. 

As a Russian language dataset, we used the dataset \texttt{RuReviews}~\cite{ru_sentiment} of the product reviews from the large Russian e-commerce website, from the "Women’s Clothes and Accessories" category. Even though this dataset is domain-specific, we chose it because it is open source and it has a relatively large size. As the train/validation/test split of this dataset was not provided, we used the same split that exists for the \texttt{DynaSent}(r1) dataset. 
For the sentiment classification, we utilized \texttt{DynaSent}(r1)~\cite{sentiment} dataset. It contains naturally occurring sentences. i.e. \textit{Need a cheap spatula?} There are three classes in this balanced multi-class dataset -- \textit{positive},  \textit{negative}, and \textit{neural}. % One can contend that classes from the \texttt{go\_emotions} dataset can be grouped by sentiment, as proposed in~\cite{emotions}, but also by emotions. However, our experiments show that augmentation of the sentiment dataset with such samples decreases the metrics rather than increasing them. That shows that these datasets are sufficiently different for separate use, as sentiment-grouping of the emotion dataset is too rough. 


\subsection{Toxicity classification}

For the English language, we used the \texttt{Wiki Talk} dataset~\cite{toxic} for the toxicity classification. This Wikipedia comment dataset had two classes: toxic and not toxic. This dataset has ~127k training samples, ~31k validation samples, and ~62k testing samples.\footnote{We used the cleaned version of this dataset provided by HuggingFace at \url{https://huggingface.co/datasets/OxAISH-AL-LLM/wiki\_toxic}}. Nevertheless, we also performed some additional data cleaning, such as removing quotes. 

For the Russian languages, we used the two-class dataset \texttt{RuToxic}~\cite{ru_toxic} of toxic comments from the largest Russian anonymous imageboard Dvach\footnote{Link to the imageboard: \url{http://2ch.hk}}. This dataset originally had ~162k training samples. As the authors didn't provide the original split in their repository\footnote{Russian toxicity dataset was retrieved from \url{https://github.com/s-nlp/rudetoxifier}}, we split this dataset in the same proportions as the split proportions of the Wiki Talk dataset.
We used the \texttt{Wiki Talk}~\cite{toxic} dataset for the toxicity classification. This Wikipedia comment dataset had two classes: \textit{toxic} and \textit{not toxic}. Unsurprisingly, the dataset contains vulgar slang. However, about 90\% of examples from this dataset are not toxic, i.e \textit{"Hi! so umm i guess yer incharge here hehehe. so wassup?"} We used the preprocessed version of this dataset provided by HuggingFace\footnote{\url{https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic}}.


\subsection{Intent classification and topic classification}

We used dataset \texttt{MASSIVE}~\cite{massive} for the intent classification for the Russian and English languages. All examples in this dataset were labeled simultaneously for 51 languages, including the English and Russian languages. This dataset has 11514 train samples, 2033 validation samples, and 2974 test samples. Every sample belongs to one of 60 intent classes. 

We used the same dataset in the same way for topic classification as well, as this dataset is labeled by intent and by topic. Every sample from this dataset belongs to one of the 18 topic classes.

We also used the multi-class \texttt{SLURP}~\cite{slurp} dataset for the intent classification and topic classification. The \texttt{SLURP} dataset contains the spoken utterances, which aim for the voice assistant, e.g. \textit{play rock playlist} or \textit{show me news from c. n. n.}.
% Every sample is single-label and belongs to one intent and one topic.

To demonstrate the MTL model performance on other domains and datasets, we have also benchmarked our model on the GLUE tasks~\cite{GLUE:19}.
%For every task and every task split, the total number of labels for every class is included in Appendix. 
\section {Настройки экспериментов}

Для всех 
For all the experiments described in this article, the optimizer was AdamW~\cite{adam} with betas (0.9, 0.99), and the learning rate was 2e-5.%Ask Vasily for an article that proves that we should not tune the learning rate
We used average accuracy for all tasks as an early stop metric. The training had a validation patience of 3, and the learning rate was halved if the early stopping metric did not improve for 2 epochs. 

The training was usually completed in fewer than 10-15 epochs and never exceeded 25 epochs.% , even though the maximal number of epochs was equal to 100. 
We have set the batch size to 160 to speed up the computations. For all the multi-task experiments, we used plain sampling (the probability of sampling an example from the dataset was proportional to the dataset size). 
% Additionally, we explored the impact of using different sampling methods for multi-task training. We report only the results from the experiments when sampling was plain (the probability of sampling an example from the dataset was proportional to the dataset size). Annealed sampling~\cite{PAL:19} and uniform sampling did not make any improvement over plain sampling in our preliminary experiments.
Мы приводим результаты для каждого запуска в Аппендиксе. TODO = ВСТАВИТЬ ИХ ТУДА


For all the experiments described in this article, the optimizer was AdamW~\cite{adam} with betas (0.9, 0.99), and the learning rate was 2e-5. 
We used average accuracies for all tasks as an early stop metric. The training had validation patience 3, and the learning rate was dropped by 2 times if the early stopping metric did not improve for 2 epochs. 

The training was usually completed in fewer than 10-15 epochs and never exceeded 25 epochs, even though the maximal number of epochs was set to 100.

We set the batch size to 160. We had also tried batch size 32, and the metrics for batch size 160 were just insignificantly better. However, the article ~\cite{tuning_neural_networks} claims that this difference can be eliminated by better finetuning. Anyway, we finally chose batch size 160 because the computations with batch size 160 were performed several times faster.

In the preliminary multi-task experiments, apart from the plain sampling (a sampling mode where the example sampling probability is proportional to the task size), we also tried annealed sampling~\cite{PAL:19} and uniform sampling (the same sampling probability for all tasks). We performed such experiments for Russian and English distilbert-like models, for Russian and English tasks. The results for these sampling modes did not bring out a noticeable improvement, thus we used only plain sampling.

We averaged all the experiment results by three runs. %\footnote{The results for every run can be found at \url{https://github.com/dimakarp1996/Dialogue2023/AllRuns.pdf}. We include also the source code at \url{https://github.com/dimakarp1996/Dialogue2023/source_code.zip}}.


\subsection{Single-task vs Multi-task: full data}

We performed experiments on three different BERT-based backbones: \textit{distilbert-base-cased}~\cite{alina}, \textit{bert-base-cased}, and \textit{bert-large-cased}~\cite{bert}. While \textit{distilbert-base-cased} takes 40\% less memory than \textit{bert-base-cased}, \textit{bert-large-cased} takes 3.1 times as much memory as \textit{bert-base-cased}. These three backbones cover a large variety of possible cases of using neural classifiers for dialog models. %Is it ok or maybe we need to use the short names somewhere?

We also compared every such experiment with analogous single-task experiments (with all the same hyperparameters). All such experiments are presented in Table~\ref{tab:mtl_dialog}. We have averaged results for three restarts with different random seeds.

We have performed similar experiments on the GLUE benchmark with the same hyperparameters.
% \footnote{We also explored different hyperparameters(batch size 32, 5 training epochs, uncased backbones) and it did not significantly alter the results.} 
Table~\ref{tab:mtl_glue} presents the results of these experiment series. For every experiment, we report accuracy / macro-averaged F1.

\begin{table*}
 \caption{Metrics for five dialog tasks. Acc stands for accuracy, F1 stands for F1 score, mode S stands for single-task, and mode M stands for multi-task(with plain sampling). Averaged by three runs.}
 \label{tab:mtl_dialog}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{Mode} & \multirow{2}{*}{Average} & Emotions & Sentiment & Toxic & Intents & Topics & Batches \\
& & & 39.4k & 80.5k & 127.6k & 11.5k & 11.5k & seen \\ \hline
\textit{\multirow{2}{*}{distilbert-base-cased}} & S & \textbf{82.9/78.4} & \textbf{70.3/63.1} & 74.7/74.3 & 91.5/81.2 & \textbf{87.4/82.7} & \textbf{91.0/90.6} & 11390 \\
 & M  & 82.1/77.2 & 67.7/60.7 & \textbf{75.2/75.0} & 90.6/79.8 & 86.3/80.4 & 90.8/90.1 & 14000 \\ \hline
\textit{\multirow{2}{*}{bert-base-cased}} & S & \textbf{83.9/79.7} & \textbf{71.2/64.2} & 76.1/75.8 & \textbf{93.2/83.5} & \textbf{87.9/84.2} & \textbf{91.3/90.7} & 9470 \\
 & M &  83.0/78.4 & 69.0/63.1 & \textbf{76.5/76.4} & 91.4/80.8 & 87.1/81.2 & 91.2/90.6 & 11760 \\ \hline
\textit{\multirow{2}{*}{bert-large-cased}} & S &  \textbf{84.7/80.5} & \textbf{70.9/64.4} & \textbf{80.5/80.4} & \textbf{92.1/82.2} & \textbf{88.4/84.9} & 91.3/90.7 & 8526 \\
 & M  & 83.6/78.7 & 69.0/61.8 & 79.0/78.9 & 91.3/80.9 & 87.3/80.9 & \textbf{91.3/90.8} & 11200 \\ \hline
 \end{tabular}
 }
 
For the English-language tasks, we made the experiments for the backbones \textit{distilbert-base,-cased}~\cite{distilbert} and \textit{bert-base-cased}~\cite{bert}. %large is not reported here

For the Russian-language tasks, we made experiments for the backbones \textit{DeepPavlov/distilrubert-base-cased-conversational}~\cite{distilrubert} and \textit{DeepPavlov/rubert-base-cased-conversational}~\cite{rubert}. As distilled BERTs takes 40\% less memory than BERTs and are 60\% faster, these experiments cover a variety of different model uses for different computational budgets and quality demands. 

The results of the first stage of experiments are presented in Tables 1-2. For every experiment, we provide accuracy / f1-macro. 

\begin{table*}
\caption{Accuracy / f1 macro on the English data for the transformer-agnostic model. English cased models trained on English data, batch size 160, plain sampling. Mode S stands for singletask, mode M stands for multi-task}
\label{en_results}
%\begin{tabular}{|c|c||c|c|c|c|c|c|} \hline
\begin{tabular}{|c|c||c|c|c|c|c|c||c|} \hline
%Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\39.4 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\79.2 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\127.6k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular}  \\
Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\39.4 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\79.2 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\127.6k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Batches \\seen\end{tabular} \\
\hline \hline
\textit{distilbert-base-cased} & S & 82.9/78.4 & 70.3/63.1 & 74.7/74.3 & 91.5/81.2 & 87.4/82.7 & 91.0/90.6 & 11390 \\ \hline
\textit{distilbert-base-cased} & M & 82.1/77.2 & 67.7/60.7 & 75.2/75.0 & 90.6/79.8 & 86.3/80.4 & 90.8/90.1 & 14000 \\ \hline
\textit{bert-base-cased} & S & 83.9/79.7 & 71.2/64.2 & 76.1/75.8 & 93.2/83.5 & 87.9/84.2 & 91.3/90.7 & 9470 \\ \hline
\textit{bert-base-cased} & M & 83.0/78.4 & 69.0/63.1 & 76.5/76.4 & 91.4/80.8 & 87.1/81.2 & 91.2/90.6 & 11760 \\ \hline
% bert-large & S &  & 84.7/80.5 & 70.9/64.4 & 80.5/80.4 & 92.1/82.2 & 88.4/84.9 & 91.3/90.7 & 8526 \\ \hline
\end{tabular}
\end{table*}



\begin{table*}
\caption{Accuracy / f1 macro on the Russian data for the transformer-agnostic model. Russian cased models trained on Russian data, batch size 160, plain sampling. Mode S stands for singletask, and mode M stands for multi-task.}
\label{ru_results}
%\begin{tabular}{|c|c||c|c|c|c|c|c|} \hline
\begin{tabular}{|c|c||c|c|c|c|c|c||c|} \hline
%Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\6.5 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\82.6 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\93.3k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular}  \\
Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\6.5 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\82.6 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\93.3k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular} &\begin{tabular}[c]{@{}l@{}}Batches\\seen\end{tabular} \\
\hline \hline
\textit{DeepPavlov/distilrubert-base-cased-conversational} & S & 86.9/84.1 & 82.2/76.1 & 77.9/78.2 & 97.1/95.4 & 86.7/81.6 & 90.4/89.5 & 8472 \\ \hline
\textit{DeepPavlov/distilrubert-base-cased-conversational} & M & 86.3/82.6 & 81.0/74.6 & 77.7/77.7 & 96.9/95.0 & 85.2/75.9 & 90.7/89.9 & 8540 \\ \hline
\textit{DeepPavlov/rubert-base-cased-conversational} & S & 86.5/83.4 & 80.9/75.3 & 78.0/78.2 & 97.2/95.6 & 86.2/79.1 & 90.0/89.0 & 7999 \\ \hline
\textit{DeepPavlov/rubert-base-cased-conversational} & M & 86.2/82.6 & 80.5/73.8 & 77.6/77.6 & 96.8/95.0 & 85.3/76.9 & 90.5/89.8 & 8113 \\ \hline
% ru single, distilru single  - might need to restart in the future? Or not?
\end{tabular}
\end{table*}



Overall, the performance of the multi-task transformer-agnostic models closely matches the performance of the analogous singletask models. This effect holds for the Russian language as well as for the English language. 

%It is also worth noting that distilbert-like English models almost match bert-like English models by their average metrics, and for the Russian language the average metrics of distilbert-like models are no worse at all than the metrics of bert-like models. 
In the next experiment, we put the main focus on the distilbert-like models for the purpose of speeding up the computations.


\subsection{Multilingual multi-task backbones, the impact of cross-lingual training}

In the second stage of experiments, we utilized only multilingual backbones. Specifically, we used \textit{distilbert-base-multilingual-cased} and \textit{bert-base-multilingual-cased}. Our main goal was:

 \begin{itemize}
\item Compare the performance of the multi-task and singletask models with the multilingual backbones for the Russian language.
\item Check out how the performance of singletask models and the performance of multi-task models varies if we add to them the English language data, and the data are merged by task (for every task, the model is trained on English+Russian training data and validated on Russian data).
\item Check out whether treating English-language tasks as separate tasks, yields any improvements if we perform the validation on the Russian data.
\end{itemize} 


\begin{table*}
\caption{Accuracy / f1 macro on the Russian data for the transformer-agnostic model. Multilingual cased models, batch size 160, plain sampling. Mode S stands for singletask, mode M stands for multi-task, RU stands for the Russian language, and EN stands for the English language. Merged means that Russian and English data are merged by task. Separate means that Russian and English tasks are treated as separate tasks.}
\label{mult_results}
%\begin{tabular}{|c|c|c||c|c|c|c|c|c|} \hline
\scalebox{0.8}{
\begin{tabular}{|c|c|c||c|c|c|c|c|c||c|} \hline
%Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\6.5 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\82.6 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\93.3k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular}  \\
Model & \begin{tabular}[c]{@{}l@{}}Training\\data\end{tabular} & Mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\end{tabular} &\begin{tabular}[c]{@{}l@{}}Batches\\seen\end{tabular} \\
\hline \hline
\textit{distilbert-base-multilingual-cased} & RU & S & 84.7/81.0 & 77.4/69.1 & 77.7/77.9 & 96.7/94.8 & 83.5/76.6 & 88.1/86.9 & 10058 \\ \hline
\textit{distilbert-base-multilingual-cased} & RU & M & 84.3/80.2 & 78.1/70.5 & 76.8/76.7 & 96.5/94.4 & 81.9/72.3 & 88.2/87.1 & 9821 \\ \hline
\textit{distilbert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & S & 85.2/81.8 & 78.9/70.2 & 77.4/77.3 & 96.8/94.9 & 84.7/79.1 & 88.4/87.4 & 31843 \\ \hline
\textit{distilbert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & M & 84.5/81.1 & 77.9/70.7 & 76.6/76.7 & 96.5/94.5 & 82.9/76.5 & 88.4/87.2 & 17790 \\ \hline
\textit{distilbert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\separate\end{tabular} & M & 84.4/80.6 & 77.6/70.0 & 76.8/77.1 & 96.5/94.5 & 82.4/73.9 & 88.3/87.2 & 23688 \\ \hline
\textit{bert-base-multilingual-cased} & RU & S & 84.7/80.2 & 76.6/64.2 & 77.8/78.2 & 96.9/95.1 & 83.9/76.3 & 88.4/87.0 & 10884 \\ \hline
\textit{bert-base-multilingual-cased} & RU & M & 84.8/81.4 & 78.4/71.4 & 76.3/76.3 & 96.8/94.8 & 83.7/76.6 & 89.0/87.8 & 12810 \\ \hline
\textit{bert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & S & 85.6/82.3 & 78.9/70.1 & 77.6/77.8 & 96.9/94.9 & 85.0/80.4 & 89.4/88.5 & 23752 \\ \hline
\textit{bert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & M & 85.2/82.3 & 79.2/72.7 & 76.4/76.6 & 96.7/94.8 & 84.3/79.3 & 89.4/88.3 & 20755 \\ \hline
\textit{bert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\separate\end{tabular} & M & 85.0/81.6 & 78.3/71.4 & 77.1/77.0 & 96.7/94.7 & 84.0/76.7 & 89.1/88.0 & 22701 \\ \hline
\end{tabular}
}
\end{table*}

As we see, the results of all settings are pretty similar: using Russian+English data puts us on the plateau, while improvements are only moderate. Treating English tasks as separate tasks does not bring out improvements, and brings out even a small deterioration. 

In the same setting, we also explored whether utilizing English-language tasks as separate tasks is more beneficial than merging the English data and Russian data by task. This approach did not prove to be any better or worse.

The exact exploration of the impact of adding English data where we have limited Russian data (like in most cases) requires additional investigation, which was done in the next series of experiments. The real-world situation is that we usually have a huge body of datasets for English data, but not nearly as much for Russian data. This gives additional practical value to that experiments. 

\subsection{When the adding of English data helps?}

In this experiment series, we explore multi-task settings with merged labels. We study how much improves the performance of multilingual distilbert (multi-task or singletask), trained on some share of Russian train data if we add English training data to this share of Russian train data and validate on the English validation data. 

Specifically, we performed experiments for the following data shares: 0\%,3\%, 5\%, 15 \%, 20\%, 25\%, 50\%, and 100\%. For 0\%, we added to the table the model trained on English train data and validated on Russian validation data, and the model which is trained on English train data and validated on English validation data (but tested still on Russian test data). We restarted the experiments with three random seeds. For every series of experiments, we randomly shuffled the datasets and then selected all subsets at once, while the larger subsets contained all examples from the smaller subsets (like, 10\% subset contains all examples from 5\% and also from 3\%)

We present the averaged results in ~\ref{mult_smalldata_results}, in Appendix. We averaged the results by three runs. For training on the 3-5\% of the Russian data without the English data, we averaged the results by five runs due to the high variability of results. We plot the results below, in~\ref{fig:thresholds_acc}. The task-wise results for the Russian data are also plotted in Appendix.

We also note that in the settings where 100\% share of the English data was used, we performed the experiments also with validation on the Russian data instead of the English data. That change did not impact the scores in any meaningful way.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{RuMTL/thresholds_acc}
  \caption{Singletask and multi-task accuracy, while using some share of Russian training data alone or augmenting them with the full English data in "merged by task" mode.}\label{fig:thresholds_acc}
\end{figure}
\begin{figure}[ht]
    \includegraphics[width=\textwidth]{RuMTL/accuracy_by_task_n_samples}
  \caption{Singletask and multi-task accuracy, while using some share of Russian training data alone, depending on the number of samples. Accuracies are provided for every task, average accuracy is also provided for comparison to be more convenient.}\label{fig:accuracy_by_task_n_samples}
\end{figure}

%\begin{figure}[ht]
%    \includegraphics[width=\textwidth]{plot}
%  \caption{Singletask and multi-task f1 macro, while using some share of Russian training data alone or augmenting them with the full English data in "merged by task" mode.}\label{fig:thresholds_f1_macro}
%\end{figure}

 
 
 %HOW CAN I MAKE THIS TABLE SMALLER?
\end{table*}
\begin{table*}
\caption{Accuracy / F1 macro. Batch size 160, plain sampling. Mode M stands for multi-task, mode S stands for single-task, and Share stands for the share of training data used. Backbone distilbert-base-cased. Averaged by five runs.}
\label{tab:mtl_dialog_part}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
 \multirow{2}{*}{Mode} &  \multirow{2}{*}{Share} &  \multirow{2}{*}{Average} & Emotions & Sentiment & Toxic & Intents & Topics & Batches \\
& & & 39.4k & 80.5k & 127.6k & 11.5k & 11.5k & seen \\
\hline 
S & 15\% & 78.5/70.9 & 65.8/50.6 & 69.3/68.8 & 92.2/81.2 & 78.7/68.8 & 86.3/85.1 & 2173 \\ \hline
M & 15\% & 77.4/70.6 & 64.0/55.0 & 68.3/67.7 & 91.6/80.0 & 76.9/64.6 & 86.4/85.4 & 4741 \\ \hline
S & 10\% & 77.1/68.4 & 64.6/45.0 & 68.3/67.8 & 92.2/81.0 & 75.5/64.7 & 84.8/83.3 & 1579 \\ \hline
M & 10\% & 75.9/69.1 & 62.6/53.6 & 66.6/65.8 & 91.5/79.7 & 74.3/63.2 & 84.6/83.3 & 4295 \\ \hline
S & 9\% & 76.7/67.4 & 64.6/43.9 & 68.2/67.7 & 91.8/80.4 & 74.4/62.7 & 84.2/82.4 & 1457 \\ \hline
M & 9\% & 75.4/67.8 & 62.1/52.4 & 66.5/65.7 & 91.4/79.5 & 72.4/58.5 & 84.4/83.0 & 3695 \\ \hline
%S & 8\% & 73.8/64.6 & 63.7/43.5 & 67.6/67.0 & 92.0/80.5 & 62.0/49.7 & 83.9/82.1 & 1381 \\ \hline
%M & 8\% & 74.9/67.2 & 61.7/51.4 & 66.7/66.0 & 91.5/79.5 & 71.1/57.3 & 83.6/81.7 & 3511 \\ \hline
%S & 7.5\% & 73.5/64.0 & 63.8/42.5 & 67.4/66.9 & 91.6/80.1 & 61.3/48.6 & 83.6/81.7 & 1293 \\ \hline
%M & 7.5\% & 74.6/66.9 & 61.0/50.2 & 67.0/66.4 & 91.5/79.5 & 70.3/56.9 & 83.2/81.4 & 2995 \\ \hline
S & 7\% & 73.5/64.0 & 63.3/42.1 & 67.9/67.4 & 91.8/80.1 & 61.4/49.4 & 83.3/81.1 & 1251 \\ \hline
M & 7\% & 74.2/66.4 & 61.1/50.4 & 65.8/65.1 & 91.0/78.9 & 70.0/56.3 & 83.1/81.3 & 2882 \\ \hline
S & 5\% & 69.1/59.0 & 62.5/38.9 & 66.9/66.3 & 91.8/79.9 & 42.7/30.8 & 81.6/78.8 & 901 \\ \hline
M & 5\% & 71.7/62.4 & 60.5/48.6 & 64.4/63.4 & 90.8/78.5 & 62.4/44.4 & 80.2/77.3 & 2381 \\ \hline
S & 3\% & 59.6/49.0 & 60.6/37.7 & 65.2/64.5 & 91.8/79.3 & 26.5/16.9 & 54.0/46.5 & 584 \\ \hline
M & 3\% & 68.8/58.1 & 58.6/42.7 & 62.5/61.3 & 91.0/78.3 & 55.5/37.1 & 76.4/71.0 & 1566 \\ \hline
S & 2\% & 44.9/31.5 & 48.7/21.7 & 39.6/26.5 & 91.8/79.0 & 2.6/0.2 & 41.4/30.1 & 274 \\ \hline
M & 2\% & 64.8/52.1 & 57.6/38.4 & 61.4/60.1 & 90.8/78.0 & 44.2/23.5 & 69.9/60.4 & 923 \\ \hline
\end{tabular}
}
\end{table*}
\begin{table*}
\caption{Metrics of the DeepPavlov's MTL model for the GLUE benchmark. M.Corr stands for Matthew's correlation, P/S corr stands for Pearson/Spearman correlation, Acc stands for accuracy, and m/mm means "matched/mismatched". Mode S stands for single-task, and mode M stands for multi-task. Results show that multi-task models either approach the metrics of analogous single-task models or even exceed them.}
\label{tab:mtl_glue}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{3}{*}{Model} & \multirow{3}{*}{Mode}  & Average & CoLA & SST-2 & MRPC &STS-B &QQP&MNLI & QNLI & RTE & AX & Batches\\
       &        & train size  & 8.6k & 67.3k & 2.5k & 5.7k & 363.8k & 392.7k & 104.7k & 2.5k & as MNLI & seen \\ 
       &        & metric  & M.Corr & Acc & F1/Acc & P/S Corr & F1/Acc & Acc(m/mm) & Acc & Acc & M.Corr &  \\ \hline
Human & - & 87.1 & 66.4 & 97.8 & 86.3/80.8 & 92.7/92.6 & 59.5/80.4 & 92.0/92.8 & 91.2 & 93.6 & - & -\\ \hline
%\textit{\multirow{2}{*}{distilbert-base-cased}} & S & 73.1 & \textbf{42.4} & \textbf{92.1} & 85.6/\textbf{80.3} & 78.8/76.8 & \textbf{69.5/88.5} & \textbf{81.3/80.8} & \textbf{87.5} & 49.8 & 29.9 & 70846 \\ 
\textit{\multirow{2}{*}{distilbert-base-cased}} & S & 73.3 & \textbf{42.4} & \textbf{92.1} & 85.6/\textbf{80.3} & 78.8/76.8 & \textbf{69.5/88.5} & \textbf{81.3/80.8} & \textbf{87.5} & 52.1 & 29.9 & 70861 \\ 
 & M & \textbf{74.5} & 36.0 & 91.0 & \textbf{85.7}/79.9 & \textbf{82.6/81.6} & 68.4/87.4 & 80.4/80.3 & 86.0 & \textbf{69.5} & \textbf{30.1} & 88905 \\  \hline
\textit{\multirow{2}{*}{bert-base-cased}} & S & 77.3 & \textbf{53.7} & \textbf{93.2} & \textbf{87.7/82.8} & 83.8/82.2 & \textbf{70.3/88.9} & \textbf{83.8/83.1} & \textbf{90.6} & 62.1 & 32.1 & 42722\\ 
 & M & \textbf{77.8} & 45.8 & 92.9 & 86.8/82.2 & \textbf{85.3/84.7} & 70.2/88.6 & 83.5/82.6 & 90.1 & \textbf{74.5} & \textbf{32.8} & 112613\\  \hline
\textit{\multirow{2}{*}{bert-large-cased}} & S & \textbf{79.5} & \textbf{59.2} & \textbf{94.9} & 85.0/80.6 & \textbf{85.8/84.5} & 70.5/89.1 & \textbf{86.7/85.6} & 92.2 & 70.1 & \textbf{39.4} & 37290 \\ 
 & M & \textbf{79.5} & 50.8 & 94.1 & \textbf{87.3/82.8} & 83.8/83.9 & \textbf{71.0/89.2} & 85.9/85.0 & \textbf{92.4} & \textbf{78.5} & 38.5 & 53343 \\  \hline
\end{tabular}
}
\end{table*}
\subsection{Single-task vs Multi-task: impact of reducing training data}
We also report the impact of reducing training data. Specifically, we have trained the model with the same hyperparameters as in the previous setting, but we have kept only a small share of training data while leaving test and validation data intact. In every such experiment, any percentage of training data contained all training data from lower percentages.%maybe express in the other way?
In this case, the multi-task metrics exceed single-task ones only on small-scale data (like 2-5\% of the whole dataset), and this advantage evaporates even on 9\% of the dataset. 
We did this exploration only for \textit{distilbert-base-cased} to speed up the calculations. Also, for every training data share in this experiment, we have made five restarts for different random seeds and have averaged the metrics. 

We present the detailed results of this experiment in Table~\ref{tab:mtl_dialog_part} and Figure \ref{fig:mtl_dialog_part}. One can see that generally, the multi-task knowledge transfer works better for tasks with a lower number of samples.

\input{EnMTL/Plot.tex}

\section{Discussion} 

Based on the metrics on the diverse set of dialog-related tasks, the proposed multi-task transformer-agnostic model almost matches the single-label model. 
For all explored BERT-based backbones the drop in average accuracy is between 0.8\%-0.9\% on the dialogue tasks.

For the GLUE tasks, the multi-task models even exceed the single-task ones by average accuracy. This applies to GLUE tasks with not large enough training sets (AX, STS-B, and especially RTE) which benefit from knowledge transfer from large-scale tasks (MNLI, QQP).
%Distilbert-like models almost match the bert-like ones by their overall performance, for single-task and multi-task settings.
The training of the multi-task neural network, however, took more training steps than the training of the corresponding single-task models with the same early stopping criteria. The reason is that the training did not stop until the metrics on relatively small-data tasks stopped improving. 
% Therefore, examples from relatively high-size tasks, metrics for which had already reached their saturation points, were seen more frequently than they would have been seen if the model was single-task and trained for any of these tasks. If we train on the data subsets, this effect is more pronounced, probably because the gap between the saturation points from smaller-data tasks and larger-data tasks widens.
From all the tasks, the dropdown is the lowest on the topic classification tasks, which suggests that this task is more prone to knowledge transfer.

For the small-scale data, we can see that if we train on small shares of training data (2-5\%), multi-task models overcome single-task models, for 2\% and 3\% -- by a huge margin. However, even on 9\%, for all dialog tasks, this advantage eliminates.

The multi-task small-scale advantage in accuracy strongly depends on the data size. For the toxicity classification (127k samples), single-task models excel multi-task ones even at 2\% data. For the sentiment and emotion classification (79.2k and 39.3k), the advantage starts at 3\%, for topic classification (11.5k) at 5\%, and for intent classification (11.5k) at 9\%. 

Therefore, the smaller the auxiliary dataset (on the scale of ~200-2000 samples), the larger the advantage of multi-task models. This advantage shows that the knowledge transfer effect is the most noticeable for small-scale datasets.

Also, the difference between metrics of the topic classification and intent classification makes us suppose that the advantage of multi-task training depends not just on the number of samples, but on the number of samples per class. Testing this hypothesis, however, might require additional investigation. 

We also leave exploring whether these conclusions hold for the different task types, for example, sequence tagging and question answering, as a subject of future research. Testing these 
conclusions on other languages or on other transformer-based models (for example, decoder-base ones)  is also a possible future field of study.

.
Multi-task transformer-agnostic models almost match the singletask models by metrics on the dialog tasks. The accuracy gap between the multi-task and singletask monolingual models is about 0.8-0.9\% for the English language and about 0.3-0.6\% for the Russian language. For the multilingual models, the gap remains within the same limit, except for the multilingual BERT trained on Russian data, where the gap evaporates completely.

% We also report that the training of the multi-task neural network for English data took 21-24\% more training steps than the training of the singletask models with the same criteria of early stopping. The reason is that the model did not stop until the metrics on relatively low-size tasks stop improving, therefore examples from relatively high-size tasks that already were trained) were seen more times than they would have been seen if the model was singletask and trained for any of these tasks. Убрать вообще batches seen, раз не видно паттерна?

We also show that if we have Russian and English data for the same tasks, it does not matter match whether we unite data for every task, or we treat them as separate tasks. The choice of sampling, between plain, uniform, and annealed, also did not matter match in our experiments; however, this might also indicate that annealed sampling requires thorough tuning of hyperparameters.

For the small-scale data, we can see that if we train the multilingual distilbert on small shares of Russian training data (2-5\%), multi-task models overcome singletask models in the average accuracy. As shown in Figure 2, this accuracy advantage strongly depends on the dataset size - the smaller the truncated dataset size, the larger the advantage. For intent and topic datasets this advantage evaporates at 1151 training samples, but for the emotion dataset, surprisingly, this advantage holds true with any dataset partition, possibly due to the effect of knowledge transfer from the sentiment task.

The hypothesis of the knowledge transfer dependency of the dataset size is additionally supported by the fact that for experiments with adding English data, multi-task models showed no clear pattern of advantage over singletask ones.

We also find that while having a limited amount of Russian training data (3-10\% RU share in Table 4 from Appendix)  we can additionally improve the multi-task BERT metrics up to several percent if we add the English data to the training sample. 

And in this case, we can also perform the validation on the English validation data, it does not change metrics in a meaningful way compared to the validation on the Russian validation data set. 

And also, the lower share of training data we have, the larger the accuracy gain for adding the English data to the training sample.


\section{Conclusion}

The proposed transformer-agnostic multi-task models yield results matching or approaching the single-task models on most tasks. If we truncate the tasks to the low-size number (200-2000 samples), multi-task models start exceeding the single-task ones. This effect might depend on the number of samples per class.
\fi
The multi-task transformer-agnostic architecture we propose yields just a minor decrease in accuracy and F1 macro on the dialog tasks compared to the singletask architecture. 

Adding English data to the Russian dataset can improve the metrics by up to several percent for the singletask and multi-task training. Also, starting from some data sizes, multi-task architecture starts overcoming the singletask one.


\section{Выводы (название как в pseudolabel сделать)}


