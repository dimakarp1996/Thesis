\chapter{Трансформер-агностичные модели}\label{ch:tr-ag}
% TODO - ЧТОБЫ ВЕЗДЕ НАЗЫВАЛИСЬ АГНОСТИЧНЫМИ, А НЕ ИНВАРИАНТНЫМИ!!!!

1)Модификация CLS-выхода модели BERT, который классифицируется multitask головами - GHOSTBERT, Task Embedded Attention, XCA, ClassAttention. Проверял на MRPC+RTE и потом на всех классификационных задачах GLUE, прироста на валидации не увидел.
2)Не модифицируя multitask BERT (просто тело и линейные слои), сравниваю его на русском SUPERGLUE с обычным BERT. Имеется отставание от обычного BERT по большинству задач.
3)Эксперименты с дообучением на другом языке. А именно, если модель, учившуюся на русском SUPERGLUE, дообучить ещё и на английском SUPERGLUE(или наоборот), приведёт ли это к улучшению? Не привело. В разных экспериментах я либо объединял данные на русском и английском языке, либо прописывал их как разные задачи, ни то, ни то не сработало.
4)Эксперименты с выбором задач. Учил на подмножествах однотипных задач из SuperGLUE одного типа multitask модель и сравнивал с singletask моделью. Добился следующего положительного результата - на некоторых комбинациях датасетов multitask модель несколько превосходит singletask. Но учитывая маленькие размеры датасетов, устойчивость результатов под вопросом.
5)При выборе задач пробовал варьировать dropout и attention dropout. Не привело к улучшению.
6)Вместо обычного CLS токена пробовал использовать его LSTM представление, пробовал и его конкатенировать с CLS токеном. Не привело к улучшению
7)Пробовал использовать task specific trainable tokens - при препроцессинге каждого примера либо ко фразе добавлять свой токен на каждой задачи, либо добавлять task specific Токены для каждой из задач. И учиться классифицировать не по простому CLS, а либо по конкатенации CLS и task specific, либо просто по task specific. На GLUE и SUPERGLUE идея не привела к улучшению
8)Использование stochastic answer network слоя в архитектуре MT-DNN, с одной SAN и task-specific компонентами на несколько задач GLUE вместо нескольких SAN. Эксперимент не сработал
9)Исследование сэмплирования. На сабсете GLUE, пробовал такую идею, как для каждой задачи переходить в Converged режим(в котором функция потерь делится на 4), если на ней нет улучшения, и выходить из этого режима , если после этого на ней есть ухудшение. Эксперимент не дал улучшения
10)Продолжение исследования сэмплирования. Обучать модель 10 эпох, но при этом на 5 эпохах учить модель на всем датасете, а на следующих 5 фильтровать по дисперсии метрик на предыдущих эпохах и учить только на некой доле самых сложных примеров (с самой высокой дисперсией). Эксперимент тоже не дал улучшения




\subsection{Преимущество по сравнению с многозадачной моделью с одним линейным слоем}\label{ch:tr-ag:advantages}

Многозадачная трансформер-агностичная модель является логичным усовершенствованием многозадачной модели с одним линейным слоем. По сравнению с данной моделью, многозадачная трансформер-агностичная модель имеет больше гибкости, так как не требует меток для каждой из задач у каждого примера. При этом подобная модель лучше адаптируется к каждой конкретной задаче, так как при обучении на примерах из каждой задачи у этой модели обновляются веса только тех нейронов в финальных линейных, которые отвечают конкретно за эту задачу. В то же время, у модели с одним линейным слоем пример из каждой задачи при обучении чуточку "сбивает" веса \textbf{всех} задач. 

Данный эффект можно сгладить, используя псевдоразметку каждого примера для каждой задачи уже обученными однозадачными моделями, но это требует больших затрат времени и вычислительных мощностей, а также вносит в обучающую выборку для каждой задачи искажения, связанные с необходимостью видеть большое количество не свойственных этой задаче примеров, размеченных каким-то неидеальным образом. Так, для задачи классификации тональности доля положительных обучающих примеров после разметки уменьшилась с 42.4\% до 12.7\% \ref{appendix:sentiment}. При этом все плюсы от псевдоразметки там, где она оправдана, можно получить и при применении многозадачной трансформер-агностичной модели - которая к тому же предоставляет и возможность выбора, на каких задачах делать псевдоразметку, а на каких не делать.

Другое преимущество многозадачной трансформер-агностичной модели, проявляющееся даже на параллельно размеченных данных, заключается в большей гибкости голов. Если модель с одним линейным слоем по факту поддерживает только один тип головы, а имеено multilabel, то многозадачная трансформер-агностичная модель поддерживает и singlelabel головы для каждой задачи. Реальное применение показало, что на тех задачах, где выход модели можно представить в singlelabel виде, его лучше представлять в singlelabel виде. Это, по всей видимости, связано с тем, что задача "выбрать один самый вероятный класс" проще для линейного слоя, чем задача "выбрать сколько-то классов, чья вероятность выше какой-то границы". И применяя трансформер-агностичную модель, мы можем сознательно выбирать решение именно этой задачи.

Еще одно преимущество многозадачной трансформер-агностичной модели, которое в рамках этой работы еще не было раскрыто до конца, заключается в том, что она поддерживает больший спектр задач. В частности, она (в реализации на момент написания диссертационной работы) поддерживает такие задачи, как распознавание именованных сущностей или выбор из нескольких вариантов ответа, которые невозможно реализовать в рамках модели с одним линейным слоем. 




\section{Introduction}
Nowadays, the technology of conversational AI systems is on the rise. The interest in virtual assistants prompts the development of conversational NLP models alongside traditional NLP models. The text classification task is one of the most crucial NLP tasks for virtual assistants.

Transformer-based models, such as BERT, are widely used for text classification. The original article~\cite{bert} proposed the use of a separate BERT model for each task in the case of multi-task benchmarks. In such a way, if several classification tasks need to be solved in parallel, several prediction models should be employed, which increases the demand for computational resources. This problem leads to the idea of training one single model that can yield results for these tasks simultaneously. 

Multi-task learning (MTL) as part of transfer learning allows training one single model simultaneously for multiple related tasks so that the knowledge acquired in one task enhances the performance on another task.

Different real-world conditions may put various demands on the quality of neural models and their use of computational resources. Such conditions necessitate the use of transformer-agnostic models. The \texttt{transformers}~\cite{huggingface_transformers} library allows using different transformer-based models including the distilled ones to save computational resources and speed up the inference time~\cite{alina}. 

% Developers can use models from this library in different dialogue systems and platforms for dialogue system development. Among such platforms, one can pay attention to~\cite{baymurzina2021dream} and \cite{rasa}.
\ifinterspeechfinal
Our contribution is the implementation of transformer-agnostic multi-task models in DeepPavlov and researching the multi-task knowledge transfer in these models while training them for multiple dialog-related tasks.
\else
Our contribution is the implementation of transformer-agnostic multi-task models in our open-source library and researching the multi-task knowledge transfer in these models while training them for multiple dialog-related tasks.
\fi
\section{Related work}

Researchers have been studying multi-task learning (MTL) for a long time~\cite{caruana1997multi-task}. Since the rise of neural networks, researchers have proposed a wide range of approaches to MTL, including adversarial networks~\cite{wang-etal-2018-personalized} and using cross-lingual word embeddings~\cite{buryat} . However, these methods did not develop further, as nowadays NLP is based on transformer-based models. Nevertheless, as transformer architectures come out quite often, this review mostly focuses on agnostic architectures, which work with all kinds of transformers, rather than transformer-specific architectures.

In some kinds~\cite{pseudolabeling} of multitask transformer-agnostic architectures, every sample needs to be labeled or pseudo-labeled for all considered tasks. However, this procedure lacks flexibility.

One of the most frequently used transformer-agnostic architectures is ~\cite{MTDNN:19} that obtained the state-of-the-art results on SNLI~\cite{snli}, SciTail~\cite{khot2018scitail}, and GLUE~\cite{GLUE:19}. However, this architecture increases computational demands due to the specific stochastic attention layers for text classification.

The article \cite{PAL:19} proposed different transformer-agnostic ways to work with BERT output in a multi-task setting\footnote{Projective attention layers, presented in the same article as the superior result, are not transformer-agnostic.}. One such way is supplementing the model with an extra BERT layer for each task increases the number of required parameters for GLUE by 67\%, which is computationally heavy. 

Other transformer-agnostic ways proposed in the same work include low-rank task-specific transformation with task-specific self-attention on top of the model (or without it). In our experiments on GLUE, these approaches worked no better than the plain use of transformer output in the linear classifier (while training \textit{bert-base-uncased}).

 Also, utilizing self-attention with a task-embedded module from~\cite{TaskEmbedded2021} instead of plain self-attention in the low-rank transformation did not yield improvements over the plain dense task-specific layers above BERT in our experiments. The task-embedded architecture presented in the same article is still not transformer-agnostic.

Another work~\cite{GhostBert2021} suggested a novel way to extract additional features from the BERT output -- using cheap convolutional ghost modules. Despite this approach being transformer-agnostic, utilizing attention with a ghost module in the low-rank transformation did not yield improvements over the plain dense task-specific layers above BERT in our experiments. This also holds for~\cite{el-nouby2021xcit} architecture from computer vision.

%Article~\cite{Summarization2021} suggests that, when training a task agnostic multi-task model, the biLSTM layer is better than the dense layer for summarization. However, the summarization task is out of the scope of our research. 

Different types of attention, such as ~\cite{reformer}, can reduce computational complexity. However, we did not consider adding them on top of the transformer as they were not meant to increase accuracy. The only exception is~\cite{routing_transformers}, but even that work beats the state-of-the-art only on long sequence tasks.%Different types of attention, such as the ones described in~\cite{reformer,routing_transformers,linformer, performer,scaling-transformers}
The article~\cite{Multi-taskEnsemble:19} proposed an approach to improve the accuracy of any multi-task model via ensembling. We suppose that this approach can work well in parallel with our work. However, we did not test it because it is very computationally demanding.

%Another field of study is the impact of different sampling approaches on multi-task training. Among the proposed policies, we can pay attention to the works~\cite{TaskSamplingPolicy,12in1, GradTS, PAL:19,CA-MTL}. 

At the same time, the performance of the simple transformer-agnostic model is still not fully explored. It is especially true for dialog-specific datasets. To bridge this gap, we aim to examine the performance of transformer-agnostic multi-task models on various kinds of datasets for the tasks applicable in dialog systems.
\section{Our setting}


Our MTL model is based on \texttt{AutoModel}\footnote{The list of supported models: \url{https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\#automodel}} class from HuggingFace, which allows using different transformer-based architectures as a backbone. For our experiments, we utilized BERT-based models because they allow effective transfer learning~\cite{10.1007/978-3-031-19032-2_46, ksquad}, however, exactly the same approach can be applied to any Transformer-based model.

\begin{enumerate}

    \item In the same way as in the original article~\cite{bert}, we return the final hidden states for all tokens and the output of the BERT pooling layer.

    \item We apply the dropout (equal to 0.2) to the pooler output. We flatten the output if it is the sequence labeling task or the multiple-choice task.

    \item After this stage, we apply the linear layer with \texttt{n} output dimension. \texttt{n} equals the number of classes for all tasks but regression and multiple-choice, where \texttt{n} is 1. (In the multiple-choice task, we get several samples per 1 label, so after reshaping the number of real and predicted labels matches). 

    \item Then we apply a loss function: the categorical cross-entropy loss for the single-label classification task or the binary cross-entropy loss for the multi-label classification task. In this paper, we consider only single-label classification.

\end{enumerate}

The multi-task model in our setting requires almost no additional parameters and computational overhead, apart from the linear layers, so its simplicity singles it out. Also, the flexibility of this model allows using it with different kinds of backbones, which positively distinguishes it from~\cite{PAL:19}.

For the distilBERT-like models, our multi-task model takes only ~0.1\% more parameters than the single-task models. This computational overhead varies around this number, depending on the number of tasks, the number of classes, and the backbone model. 
\ifinterspeechfinal
The multi-task transformer-agnostic model is integrated into DeepPavlov~\cite{Burtsev2018DeepPavlovAO}. This model is also successfully used in the Dream dialogue platform~\cite{baymurzina2021dream}. 
\else
We have integrated the multi-task transformer-agnostic model into our open-source library and our dialogue platform\footnote{We will provide links in the camera-ready version.}.
\fi

\section{Datasets}


We explored the performance of the multi-task mode from the subset of datasets necessary for the dialog systems~\cite{wochat,lrec}. We trained models on datasets for five problems, i.e emotion classification, toxicity classification, sentiment classification, intent classification, and topic classification. Due to the scope of our work, we prepared a Russian-language dataset and an English-language dataset for every task. We note that for the Russian-language dataset and the English-language dataset, the indexes of the same classes used by the models were also the same. The sizes of all datasets by class and split can be found in Appendix. 

\subsection{Emotion classification }

As an English language dataset, we used dataset \texttt{go\_emotions}~\cite{emotions} for the emotion classification. We used Ekman-grouped emotions, grouping them into seven types, i.e \textit{anger}, \textit{fear}, \textit{disgust}, \textit{joy}, \textit{surprise}, \textit{sadness}, and \textit{neutral}. After such grouping, we selected only single-label examples. There were 39.5k such training examples. The train/test/validation split of this dataset was approximately 80/10/10.


For the emotion classification, we used the \texttt{go\_emotions}~\cite{emotions} dataset. This dataset consists of short comments from Reddit, such as \textit{LOL. Super cute!} or \textit{Yikes. I admire your patience}. We have grouped all emotion classes from this dataset into seven Ekman types, i.e \textit{anger}, \textit{fear}, \textit{disgust}, \textit{joy}, \textit{surprise}, \textit{sadness}, and \textit{neutral}. After such grouping, we selected only single-label examples. We leave exploring multi-task learning with the multilabel data for future research. - ВСТАВИТЬ ИЗ ТОГО РАЗДЕЛА ЧТО НЕ ВЫШЛО

As a Russian language dataset, we used for this task dataset \texttt{CEDR}~\cite{ru_emotions}\footnote{We retrieved this dataset from the URL \url{https://huggingface.co/datasets/cedr}}. This dataset has 5 classes - \textit{anger}, \textit{fear}, \textit{joy}, \textit{surprise}, and \textit{sadness} - but the samples from this dataset can belong to more than 1 class or (unlike \texttt{go\_emotions}) belong to no class. 

From this dataset, we selected only examples that belong to 1 class or that have no class, labeling no-class examples as \textit{neutral}. The class nomenclature of this dataset was almost the same as for the English dataset, except for the \textit{disgust} class. Nonetheless, as \textit{disgust} examples comprised less than 1.5\% of the English training samples, it didn't impact knowledge transfer much.

The work~\cite{ru_emotions} provided only the train-test split of the \texttt{CEDR} dataset, which is 80/20. We singled out 12.5\% of the training examples from CEDR as the validation set. 

\subsection{Sentiment classification}

As an English language dataset, we used dataset \texttt{DynaSent}(r1)~\cite{sentiment} for the sentiment classification. We used only examples from the first found of the collection, to match the Russian data by difficulty. This single-label dataset with 80.5k training samples has three classes - positive, negative, and neutral. One can contend that classes from the \texttt{go\_emotions} dataset can be grouped by sentiment, as proposed by the work~\cite{emotions}, but also by emotions. However, our experiments show that augmentation of the sentiment dataset with such samples decreases the metrics rather than increasing them. That shows that these datasets are sufficiently different to be used separately, as sentiment-grouping of the emotion dataset is too rough. This dataset has 3.6k validation samples and the same number of test samples. 

As a Russian language dataset, we used the dataset \texttt{RuReviews}~\cite{ru_sentiment} of the product reviews from the large Russian e-commerce website, from the "Women’s Clothes and Accessories" category. Even though this dataset is domain-specific, we chose it because it is open source and it has a relatively large size. As the train/validation/test split of this dataset was not provided, we used the same split that exists for the \texttt{DynaSent}(r1) dataset. 
For the sentiment classification, we utilized \texttt{DynaSent}(r1)~\cite{sentiment} dataset. It contains naturally occurring sentences. i.e. \textit{Need a cheap spatula?} There are three classes in this balanced multi-class dataset -- \textit{positive},  \textit{negative}, and \textit{neural}. % One can contend that classes from the \texttt{go\_emotions} dataset can be grouped by sentiment, as proposed in~\cite{emotions}, but also by emotions. However, our experiments show that augmentation of the sentiment dataset with such samples decreases the metrics rather than increasing them. That shows that these datasets are sufficiently different for separate use, as sentiment-grouping of the emotion dataset is too rough. 


\subsection{Toxicity classification}

For the English language, we used the \texttt{Wiki Talk} dataset~\cite{toxic} for the toxicity classification. This Wikipedia comment dataset had two classes: toxic and not toxic. This dataset has ~127k training samples, ~31k validation samples, and ~62k testing samples.\footnote{We used the cleaned version of this dataset provided by HuggingFace at \url{https://huggingface.co/datasets/OxAISH-AL-LLM/wiki\_toxic}}. Nevertheless, we also performed some additional data cleaning, such as removing quotes. 

For the Russian languages, we used the two-class dataset \texttt{RuToxic}~\cite{ru_toxic} of toxic comments from the largest Russian anonymous imageboard Dvach\footnote{Link to the imageboard: \url{http://2ch.hk}}. This dataset originally had ~162k training samples. As the authors didn't provide the original split in their repository\footnote{Russian toxicity dataset was retrieved from \url{https://github.com/s-nlp/rudetoxifier}}, we split this dataset in the same proportions as the split proportions of the Wiki Talk dataset.
We used the \texttt{Wiki Talk}~\cite{toxic} dataset for the toxicity classification. This Wikipedia comment dataset had two classes: \textit{toxic} and \textit{not toxic}. Unsurprisingly, the dataset contains vulgar slang. However, about 90\% of examples from this dataset are not toxic, i.e \textit{"Hi! so umm i guess yer incharge here hehehe. so wassup?"} We used the preprocessed version of this dataset provided by HuggingFace\footnote{\url{https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic}}.


\subsection{Intent classification and topic classification}

We used dataset \texttt{MASSIVE}~\cite{massive} for the intent classification for the Russian and English languages. All examples in this dataset were labeled simultaneously for 51 languages, including the English and Russian languages. This dataset has 11514 train samples, 2033 validation samples, and 2974 test samples. Every sample belongs to one of 60 intent classes. 

We used the same dataset in the same way for topic classification as well, as this dataset is labeled by intent and by topic. Every sample from this dataset belongs to one of the 18 topic classes.

We also used the multi-class \texttt{SLURP}~\cite{slurp} dataset for the intent classification and topic classification. The \texttt{SLURP} dataset contains the spoken utterances, which aim for the voice assistant, e.g. \textit{play rock playlist} or \textit{show me news from c. n. n.}.
% Every sample is single-label and belongs to one intent and one topic.

To demonstrate the MTL model performance on other domains and datasets, we have also benchmarked our model on the GLUE tasks~\cite{GLUE:19}.
%For every task and every task split, the total number of labels for every class is included in Appendix. 
\section {Experiment description}

For all the experiments described in this article, the optimizer was AdamW~\cite{adam} with betas (0.9, 0.99), and the learning rate was 2e-5.%Ask Vasily for an article that proves that we should not tune the learning rate
We used average accuracy for all tasks as an early stop metric. The training had a validation patience of 3, and the learning rate was halved if the early stopping metric did not improve for 2 epochs. 

The training was usually completed in fewer than 10-15 epochs and never exceeded 25 epochs.% , even though the maximal number of epochs was equal to 100. 
We have set the batch size to 160 to speed up the computations. For all the multi-task experiments, we used plain sampling (the probability of sampling an example from the dataset was proportional to the dataset size). 
% Additionally, we explored the impact of using different sampling methods for multi-task training. We report only the results from the experiments when sampling was plain (the probability of sampling an example from the dataset was proportional to the dataset size). Annealed sampling~\cite{PAL:19} and uniform sampling did not make any improvement over plain sampling in our preliminary experiments.
Мы приводим результаты для каждого запуска в Аппендиксе. TODO = ВСТАВИТЬ ИХ ТУДА


For all the experiments described in this article, the optimizer was AdamW~\cite{adam} with betas (0.9, 0.99), and the learning rate was 2e-5. 
We used average accuracies for all tasks as an early stop metric. The training had validation patience 3, and the learning rate was dropped by 2 times if the early stopping metric did not improve for 2 epochs. 

The training was usually completed in fewer than 10-15 epochs and never exceeded 25 epochs, even though the maximal number of epochs was set to 100.

We set the batch size to 160. We had also tried batch size 32, and the metrics for batch size 160 were just insignificantly better. However, the article ~\cite{tuning_neural_networks} claims that this difference can be eliminated by better finetuning. Anyway, we finally chose batch size 160 because the computations with batch size 160 were performed several times faster.

In the preliminary multi-task experiments, apart from the plain sampling (a sampling mode where the example sampling probability is proportional to the task size), we also tried annealed sampling~\cite{PAL:19} and uniform sampling (the same sampling probability for all tasks). We performed such experiments for Russian and English distilbert-like models, for Russian and English tasks. The results for these sampling modes did not bring out a noticeable improvement, thus we used only plain sampling.

We averaged all the experiment results by three runs. %\footnote{The results for every run can be found at \url{https://github.com/dimakarp1996/Dialogue2023/AllRuns.pdf}. We include also the source code at \url{https://github.com/dimakarp1996/Dialogue2023/source_code.zip}}.


\subsection{Single-task vs Multi-task: full data}

We performed experiments on three different BERT-based backbones: \textit{distilbert-base-cased}~\cite{alina}, \textit{bert-base-cased}, and \textit{bert-large-cased}~\cite{bert}. While \textit{distilbert-base-cased} takes 40\% less memory than \textit{bert-base-cased}, \textit{bert-large-cased} takes 3.1 times as much memory as \textit{bert-base-cased}. These three backbones cover a large variety of possible cases of using neural classifiers for dialog models. %Is it ok or maybe we need to use the short names somewhere?

We also compared every such experiment with analogous single-task experiments (with all the same hyperparameters). All such experiments are presented in Table~\ref{tab:mtl_dialog}. We have averaged results for three restarts with different random seeds.

We have performed similar experiments on the GLUE benchmark with the same hyperparameters.
% \footnote{We also explored different hyperparameters(batch size 32, 5 training epochs, uncased backbones) and it did not significantly alter the results.} 
Table~\ref{tab:mtl_glue} presents the results of these experiment series. For every experiment, we report accuracy / macro-averaged F1.

\begin{table*}
 \caption{Metrics for five dialog tasks. Acc stands for accuracy, F1 stands for F1 score, mode S stands for single-task, and mode M stands for multi-task(with plain sampling). Averaged by three runs.}
 \label{tab:mtl_dialog}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{Mode} & \multirow{2}{*}{Average} & Emotions & Sentiment & Toxic & Intents & Topics & Batches \\
& & & 39.4k & 80.5k & 127.6k & 11.5k & 11.5k & seen \\ \hline
\textit{\multirow{2}{*}{distilbert-base-cased}} & S & \textbf{82.9/78.4} & \textbf{70.3/63.1} & 74.7/74.3 & 91.5/81.2 & \textbf{87.4/82.7} & \textbf{91.0/90.6} & 11390 \\
 & M  & 82.1/77.2 & 67.7/60.7 & \textbf{75.2/75.0} & 90.6/79.8 & 86.3/80.4 & 90.8/90.1 & 14000 \\ \hline
\textit{\multirow{2}{*}{bert-base-cased}} & S & \textbf{83.9/79.7} & \textbf{71.2/64.2} & 76.1/75.8 & \textbf{93.2/83.5} & \textbf{87.9/84.2} & \textbf{91.3/90.7} & 9470 \\
 & M &  83.0/78.4 & 69.0/63.1 & \textbf{76.5/76.4} & 91.4/80.8 & 87.1/81.2 & 91.2/90.6 & 11760 \\ \hline
\textit{\multirow{2}{*}{bert-large-cased}} & S &  \textbf{84.7/80.5} & \textbf{70.9/64.4} & \textbf{80.5/80.4} & \textbf{92.1/82.2} & \textbf{88.4/84.9} & 91.3/90.7 & 8526 \\
 & M  & 83.6/78.7 & 69.0/61.8 & 79.0/78.9 & 91.3/80.9 & 87.3/80.9 & \textbf{91.3/90.8} & 11200 \\ \hline
 \end{tabular}
 }
 
For the English-language tasks, we made the experiments for the backbones \textit{distilbert-base,-cased}~\cite{distilbert} and \textit{bert-base-cased}~\cite{bert}. %large is not reported here

For the Russian-language tasks, we made experiments for the backbones \textit{DeepPavlov/distilrubert-base-cased-conversational}~\cite{distilrubert} and \textit{DeepPavlov/rubert-base-cased-conversational}~\cite{rubert}. As distilled BERTs takes 40\% less memory than BERTs and are 60\% faster, these experiments cover a variety of different model uses for different computational budgets and quality demands. 

The results of the first stage of experiments are presented in Tables 1-2. For every experiment, we provide accuracy / f1-macro. 

\begin{table*}
\caption{Accuracy / f1 macro on the English data for the transformer-agnostic model. English cased models trained on English data, batch size 160, plain sampling. Mode S stands for singletask, mode M stands for multi-task}
\label{en_results}
%\begin{tabular}{|c|c||c|c|c|c|c|c|} \hline
\begin{tabular}{|c|c||c|c|c|c|c|c||c|} \hline
%Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\39.4 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\79.2 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\127.6k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular}  \\
Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\39.4 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\79.2 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\127.6k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Batches \\seen\end{tabular} \\
\hline \hline
\textit{distilbert-base-cased} & S & 82.9/78.4 & 70.3/63.1 & 74.7/74.3 & 91.5/81.2 & 87.4/82.7 & 91.0/90.6 & 11390 \\ \hline
\textit{distilbert-base-cased} & M & 82.1/77.2 & 67.7/60.7 & 75.2/75.0 & 90.6/79.8 & 86.3/80.4 & 90.8/90.1 & 14000 \\ \hline
\textit{bert-base-cased} & S & 83.9/79.7 & 71.2/64.2 & 76.1/75.8 & 93.2/83.5 & 87.9/84.2 & 91.3/90.7 & 9470 \\ \hline
\textit{bert-base-cased} & M & 83.0/78.4 & 69.0/63.1 & 76.5/76.4 & 91.4/80.8 & 87.1/81.2 & 91.2/90.6 & 11760 \\ \hline
% bert-large & S &  & 84.7/80.5 & 70.9/64.4 & 80.5/80.4 & 92.1/82.2 & 88.4/84.9 & 91.3/90.7 & 8526 \\ \hline
\end{tabular}
\end{table*}



\begin{table*}
\caption{Accuracy / f1 macro on the Russian data for the transformer-agnostic model. Russian cased models trained on Russian data, batch size 160, plain sampling. Mode S stands for singletask, and mode M stands for multi-task.}
\label{ru_results}
%\begin{tabular}{|c|c||c|c|c|c|c|c|} \hline
\begin{tabular}{|c|c||c|c|c|c|c|c||c|} \hline
%Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\6.5 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\82.6 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\93.3k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular}  \\
Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\6.5 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\82.6 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\93.3k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular} &\begin{tabular}[c]{@{}l@{}}Batches\\seen\end{tabular} \\
\hline \hline
\textit{DeepPavlov/distilrubert-base-cased-conversational} & S & 86.9/84.1 & 82.2/76.1 & 77.9/78.2 & 97.1/95.4 & 86.7/81.6 & 90.4/89.5 & 8472 \\ \hline
\textit{DeepPavlov/distilrubert-base-cased-conversational} & M & 86.3/82.6 & 81.0/74.6 & 77.7/77.7 & 96.9/95.0 & 85.2/75.9 & 90.7/89.9 & 8540 \\ \hline
\textit{DeepPavlov/rubert-base-cased-conversational} & S & 86.5/83.4 & 80.9/75.3 & 78.0/78.2 & 97.2/95.6 & 86.2/79.1 & 90.0/89.0 & 7999 \\ \hline
\textit{DeepPavlov/rubert-base-cased-conversational} & M & 86.2/82.6 & 80.5/73.8 & 77.6/77.6 & 96.8/95.0 & 85.3/76.9 & 90.5/89.8 & 8113 \\ \hline
% ru single, distilru single  - might need to restart in the future? Or not?
\end{tabular}
\end{table*}



Overall, the performance of the multi-task transformer-agnostic models closely matches the performance of the analogous singletask models. This effect holds for the Russian language as well as for the English language. 

%It is also worth noting that distilbert-like English models almost match bert-like English models by their average metrics, and for the Russian language the average metrics of distilbert-like models are no worse at all than the metrics of bert-like models. 
In the next experiment, we put the main focus on the distilbert-like models for the purpose of speeding up the computations.


\subsection{Multilingual multi-task backbones, the impact of cross-lingual training}

In the second stage of experiments, we utilized only multilingual backbones. Specifically, we used \textit{distilbert-base-multilingual-cased} and \textit{bert-base-multilingual-cased}. Our main goal was:

 \begin{itemize}
\item Compare the performance of the multi-task and singletask models with the multilingual backbones for the Russian language.
\item Check out how the performance of singletask models and the performance of multi-task models varies if we add to them the English language data, and the data are merged by task (for every task, the model is trained on English+Russian training data and validated on Russian data).
\item Check out whether treating English-language tasks as separate tasks, yields any improvements if we perform the validation on the Russian data.
\end{itemize} 


\begin{table*}
\caption{Accuracy / f1 macro on the Russian data for the transformer-agnostic model. Multilingual cased models, batch size 160, plain sampling. Mode S stands for singletask, mode M stands for multi-task, RU stands for the Russian language, and EN stands for the English language. Merged means that Russian and English data are merged by task. Separate means that Russian and English tasks are treated as separate tasks.}
\label{mult_results}
%\begin{tabular}{|c|c|c||c|c|c|c|c|c|} \hline
\scalebox{0.8}{
\begin{tabular}{|c|c|c||c|c|c|c|c|c||c|} \hline
%Model & mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\\6.5 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\\82.6 K\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\\93.3k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\\11.5k\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\\11.5 k\end{tabular}  \\
Model & \begin{tabular}[c]{@{}l@{}}Training\\data\end{tabular} & Mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxic\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\end{tabular} &\begin{tabular}[c]{@{}l@{}}Batches\\seen\end{tabular} \\
\hline \hline
\textit{distilbert-base-multilingual-cased} & RU & S & 84.7/81.0 & 77.4/69.1 & 77.7/77.9 & 96.7/94.8 & 83.5/76.6 & 88.1/86.9 & 10058 \\ \hline
\textit{distilbert-base-multilingual-cased} & RU & M & 84.3/80.2 & 78.1/70.5 & 76.8/76.7 & 96.5/94.4 & 81.9/72.3 & 88.2/87.1 & 9821 \\ \hline
\textit{distilbert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & S & 85.2/81.8 & 78.9/70.2 & 77.4/77.3 & 96.8/94.9 & 84.7/79.1 & 88.4/87.4 & 31843 \\ \hline
\textit{distilbert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & M & 84.5/81.1 & 77.9/70.7 & 76.6/76.7 & 96.5/94.5 & 82.9/76.5 & 88.4/87.2 & 17790 \\ \hline
\textit{distilbert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\separate\end{tabular} & M & 84.4/80.6 & 77.6/70.0 & 76.8/77.1 & 96.5/94.5 & 82.4/73.9 & 88.3/87.2 & 23688 \\ \hline
\textit{bert-base-multilingual-cased} & RU & S & 84.7/80.2 & 76.6/64.2 & 77.8/78.2 & 96.9/95.1 & 83.9/76.3 & 88.4/87.0 & 10884 \\ \hline
\textit{bert-base-multilingual-cased} & RU & M & 84.8/81.4 & 78.4/71.4 & 76.3/76.3 & 96.8/94.8 & 83.7/76.6 & 89.0/87.8 & 12810 \\ \hline
\textit{bert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & S & 85.6/82.3 & 78.9/70.1 & 77.6/77.8 & 96.9/94.9 & 85.0/80.4 & 89.4/88.5 & 23752 \\ \hline
\textit{bert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\merged\end{tabular} & M & 85.2/82.3 & 79.2/72.7 & 76.4/76.6 & 96.7/94.8 & 84.3/79.3 & 89.4/88.3 & 20755 \\ \hline
\textit{bert-base-multilingual-cased} & \begin{tabular}[c]{@{}l@{}}RU+EN,\\separate\end{tabular} & M & 85.0/81.6 & 78.3/71.4 & 77.1/77.0 & 96.7/94.7 & 84.0/76.7 & 89.1/88.0 & 22701 \\ \hline
\end{tabular}
}
\end{table*}

As we see, the results of all settings are pretty similar: using Russian+English data puts us on the plateau, while improvements are only moderate. Treating English tasks as separate tasks does not bring out improvements, and brings out even a small deterioration. 

In the same setting, we also explored whether utilizing English-language tasks as separate tasks is more beneficial than merging the English data and Russian data by task. This approach did not prove to be any better or worse.

The exact exploration of the impact of adding English data where we have limited Russian data (like in most cases) requires additional investigation, which was done in the next series of experiments. The real-world situation is that we usually have a huge body of datasets for English data, but not nearly as much for Russian data. This gives additional practical value to that experiments. 

\subsection{When the adding of English data helps?}

In this experiment series, we explore multi-task settings with merged labels. We study how much improves the performance of multilingual distilbert (multi-task or singletask), trained on some share of Russian train data if we add English training data to this share of Russian train data and validate on the English validation data. 

Specifically, we performed experiments for the following data shares: 0\%,3\%, 5\%, 15 \%, 20\%, 25\%, 50\%, and 100\%. For 0\%, we added to the table the model trained on English train data and validated on Russian validation data, and the model which is trained on English train data and validated on English validation data (but tested still on Russian test data). We restarted the experiments with three random seeds. For every series of experiments, we randomly shuffled the datasets and then selected all subsets at once, while the larger subsets contained all examples from the smaller subsets (like, 10\% subset contains all examples from 5\% and also from 3\%)

We present the averaged results in ~\ref{mult_smalldata_results}, in Appendix. We averaged the results by three runs. For training on the 3-5\% of the Russian data without the English data, we averaged the results by five runs due to the high variability of results. We plot the results below, in~\ref{fig:thresholds_acc}. The task-wise results for the Russian data are also plotted in Appendix.

We also note that in the settings where 100\% share of the English data was used, we performed the experiments also with validation on the Russian data instead of the English data. That change did not impact the scores in any meaningful way.

\begin{figure}[ht]
    \includegraphics[width=\textwidth]{RuMTL/thresholds_acc}
  \caption{Singletask and multi-task accuracy, while using some share of Russian training data alone or augmenting them with the full English data in "merged by task" mode.}\label{fig:thresholds_acc}
\end{figure}
\begin{figure}[ht]
    \includegraphics[width=\textwidth]{RuMTL/accuracy_by_task_n_samples}
  \caption{Singletask and multi-task accuracy, while using some share of Russian training data alone, depending on the number of samples. Accuracies are provided for every task, average accuracy is also provided for comparison to be more convenient.}\label{fig:accuracy_by_task_n_samples}
\end{figure}

%\begin{figure}[ht]
%    \includegraphics[width=\textwidth]{plot}
%  \caption{Singletask and multi-task f1 macro, while using some share of Russian training data alone or augmenting them with the full English data in "merged by task" mode.}\label{fig:thresholds_f1_macro}
%\end{figure}

 
 
 %HOW CAN I MAKE THIS TABLE SMALLER?
\end{table*}
\begin{table*}
\caption{Accuracy / F1 macro. Batch size 160, plain sampling. Mode M stands for multi-task, mode S stands for single-task, and Share stands for the share of training data used. Backbone distilbert-base-cased. Averaged by five runs.}
\label{tab:mtl_dialog_part}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
 \multirow{2}{*}{Mode} &  \multirow{2}{*}{Share} &  \multirow{2}{*}{Average} & Emotions & Sentiment & Toxic & Intents & Topics & Batches \\
& & & 39.4k & 80.5k & 127.6k & 11.5k & 11.5k & seen \\
\hline 
S & 15\% & 78.5/70.9 & 65.8/50.6 & 69.3/68.8 & 92.2/81.2 & 78.7/68.8 & 86.3/85.1 & 2173 \\ \hline
M & 15\% & 77.4/70.6 & 64.0/55.0 & 68.3/67.7 & 91.6/80.0 & 76.9/64.6 & 86.4/85.4 & 4741 \\ \hline
S & 10\% & 77.1/68.4 & 64.6/45.0 & 68.3/67.8 & 92.2/81.0 & 75.5/64.7 & 84.8/83.3 & 1579 \\ \hline
M & 10\% & 75.9/69.1 & 62.6/53.6 & 66.6/65.8 & 91.5/79.7 & 74.3/63.2 & 84.6/83.3 & 4295 \\ \hline
S & 9\% & 76.7/67.4 & 64.6/43.9 & 68.2/67.7 & 91.8/80.4 & 74.4/62.7 & 84.2/82.4 & 1457 \\ \hline
M & 9\% & 75.4/67.8 & 62.1/52.4 & 66.5/65.7 & 91.4/79.5 & 72.4/58.5 & 84.4/83.0 & 3695 \\ \hline
%S & 8\% & 73.8/64.6 & 63.7/43.5 & 67.6/67.0 & 92.0/80.5 & 62.0/49.7 & 83.9/82.1 & 1381 \\ \hline
%M & 8\% & 74.9/67.2 & 61.7/51.4 & 66.7/66.0 & 91.5/79.5 & 71.1/57.3 & 83.6/81.7 & 3511 \\ \hline
%S & 7.5\% & 73.5/64.0 & 63.8/42.5 & 67.4/66.9 & 91.6/80.1 & 61.3/48.6 & 83.6/81.7 & 1293 \\ \hline
%M & 7.5\% & 74.6/66.9 & 61.0/50.2 & 67.0/66.4 & 91.5/79.5 & 70.3/56.9 & 83.2/81.4 & 2995 \\ \hline
S & 7\% & 73.5/64.0 & 63.3/42.1 & 67.9/67.4 & 91.8/80.1 & 61.4/49.4 & 83.3/81.1 & 1251 \\ \hline
M & 7\% & 74.2/66.4 & 61.1/50.4 & 65.8/65.1 & 91.0/78.9 & 70.0/56.3 & 83.1/81.3 & 2882 \\ \hline
S & 5\% & 69.1/59.0 & 62.5/38.9 & 66.9/66.3 & 91.8/79.9 & 42.7/30.8 & 81.6/78.8 & 901 \\ \hline
M & 5\% & 71.7/62.4 & 60.5/48.6 & 64.4/63.4 & 90.8/78.5 & 62.4/44.4 & 80.2/77.3 & 2381 \\ \hline
S & 3\% & 59.6/49.0 & 60.6/37.7 & 65.2/64.5 & 91.8/79.3 & 26.5/16.9 & 54.0/46.5 & 584 \\ \hline
M & 3\% & 68.8/58.1 & 58.6/42.7 & 62.5/61.3 & 91.0/78.3 & 55.5/37.1 & 76.4/71.0 & 1566 \\ \hline
S & 2\% & 44.9/31.5 & 48.7/21.7 & 39.6/26.5 & 91.8/79.0 & 2.6/0.2 & 41.4/30.1 & 274 \\ \hline
M & 2\% & 64.8/52.1 & 57.6/38.4 & 61.4/60.1 & 90.8/78.0 & 44.2/23.5 & 69.9/60.4 & 923 \\ \hline
\end{tabular}
}
\end{table*}
\begin{table*}
\caption{Metrics of the DeepPavlov's MTL model for the GLUE benchmark. M.Corr stands for Matthew's correlation, P/S corr stands for Pearson/Spearman correlation, Acc stands for accuracy, and m/mm means "matched/mismatched". Mode S stands for single-task, and mode M stands for multi-task. Results show that multi-task models either approach the metrics of analogous single-task models or even exceed them.}
\label{tab:mtl_glue}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{3}{*}{Model} & \multirow{3}{*}{Mode}  & Average & CoLA & SST-2 & MRPC &STS-B &QQP&MNLI & QNLI & RTE & AX & Batches\\
       &        & train size  & 8.6k & 67.3k & 2.5k & 5.7k & 363.8k & 392.7k & 104.7k & 2.5k & as MNLI & seen \\ 
       &        & metric  & M.Corr & Acc & F1/Acc & P/S Corr & F1/Acc & Acc(m/mm) & Acc & Acc & M.Corr &  \\ \hline
Human & - & 87.1 & 66.4 & 97.8 & 86.3/80.8 & 92.7/92.6 & 59.5/80.4 & 92.0/92.8 & 91.2 & 93.6 & - & -\\ \hline
%\textit{\multirow{2}{*}{distilbert-base-cased}} & S & 73.1 & \textbf{42.4} & \textbf{92.1} & 85.6/\textbf{80.3} & 78.8/76.8 & \textbf{69.5/88.5} & \textbf{81.3/80.8} & \textbf{87.5} & 49.8 & 29.9 & 70846 \\ 
\textit{\multirow{2}{*}{distilbert-base-cased}} & S & 73.3 & \textbf{42.4} & \textbf{92.1} & 85.6/\textbf{80.3} & 78.8/76.8 & \textbf{69.5/88.5} & \textbf{81.3/80.8} & \textbf{87.5} & 52.1 & 29.9 & 70861 \\ 
 & M & \textbf{74.5} & 36.0 & 91.0 & \textbf{85.7}/79.9 & \textbf{82.6/81.6} & 68.4/87.4 & 80.4/80.3 & 86.0 & \textbf{69.5} & \textbf{30.1} & 88905 \\  \hline
\textit{\multirow{2}{*}{bert-base-cased}} & S & 77.3 & \textbf{53.7} & \textbf{93.2} & \textbf{87.7/82.8} & 83.8/82.2 & \textbf{70.3/88.9} & \textbf{83.8/83.1} & \textbf{90.6} & 62.1 & 32.1 & 42722\\ 
 & M & \textbf{77.8} & 45.8 & 92.9 & 86.8/82.2 & \textbf{85.3/84.7} & 70.2/88.6 & 83.5/82.6 & 90.1 & \textbf{74.5} & \textbf{32.8} & 112613\\  \hline
\textit{\multirow{2}{*}{bert-large-cased}} & S & \textbf{79.5} & \textbf{59.2} & \textbf{94.9} & 85.0/80.6 & \textbf{85.8/84.5} & 70.5/89.1 & \textbf{86.7/85.6} & 92.2 & 70.1 & \textbf{39.4} & 37290 \\ 
 & M & \textbf{79.5} & 50.8 & 94.1 & \textbf{87.3/82.8} & 83.8/83.9 & \textbf{71.0/89.2} & 85.9/85.0 & \textbf{92.4} & \textbf{78.5} & 38.5 & 53343 \\  \hline
\end{tabular}
}
\end{table*}
\subsection{Single-task vs Multi-task: impact of reducing training data}
We also report the impact of reducing training data. Specifically, we have trained the model with the same hyperparameters as in the previous setting, but we have kept only a small share of training data while leaving test and validation data intact. In every such experiment, any percentage of training data contained all training data from lower percentages.%maybe express in the other way?
In this case, the multi-task metrics exceed single-task ones only on small-scale data (like 2-5\% of the whole dataset), and this advantage evaporates even on 9\% of the dataset. 
We did this exploration only for \textit{distilbert-base-cased} to speed up the calculations. Also, for every training data share in this experiment, we have made five restarts for different random seeds and have averaged the metrics. 

We present the detailed results of this experiment in Table~\ref{tab:mtl_dialog_part} and Figure \ref{fig:mtl_dialog_part}. One can see that generally, the multi-task knowledge transfer works better for tasks with a lower number of samples.

\input{EnMTL/Plot.tex}

\section{Discussion} 

Based on the metrics on the diverse set of dialog-related tasks, the proposed multi-task transformer-agnostic model almost matches the single-label model. 
For all explored BERT-based backbones the drop in average accuracy is between 0.8\%-0.9\% on the dialogue tasks.

For the GLUE tasks, the multi-task models even exceed the single-task ones by average accuracy. This applies to GLUE tasks with not large enough training sets (AX, STS-B, and especially RTE) which benefit from knowledge transfer from large-scale tasks (MNLI, QQP).
%Distilbert-like models almost match the bert-like ones by their overall performance, for single-task and multi-task settings.
The training of the multi-task neural network, however, took more training steps than the training of the corresponding single-task models with the same early stopping criteria. The reason is that the training did not stop until the metrics on relatively small-data tasks stopped improving. 
% Therefore, examples from relatively high-size tasks, metrics for which had already reached their saturation points, were seen more frequently than they would have been seen if the model was single-task and trained for any of these tasks. If we train on the data subsets, this effect is more pronounced, probably because the gap between the saturation points from smaller-data tasks and larger-data tasks widens.
From all the tasks, the dropdown is the lowest on the topic classification tasks, which suggests that this task is more prone to knowledge transfer.

For the small-scale data, we can see that if we train on small shares of training data (2-5\%), multi-task models overcome single-task models, for 2\% and 3\% -- by a huge margin. However, even on 9\%, for all dialog tasks, this advantage eliminates.

The multi-task small-scale advantage in accuracy strongly depends on the data size. For the toxicity classification (127k samples), single-task models excel multi-task ones even at 2\% data. For the sentiment and emotion classification (79.2k and 39.3k), the advantage starts at 3\%, for topic classification (11.5k) at 5\%, and for intent classification (11.5k) at 9\%. 

Therefore, the smaller the auxiliary dataset (on the scale of ~200-2000 samples), the larger the advantage of multi-task models. This advantage shows that the knowledge transfer effect is the most noticeable for small-scale datasets.

Also, the difference between metrics of the topic classification and intent classification makes us suppose that the advantage of multi-task training depends not just on the number of samples, but on the number of samples per class. Testing this hypothesis, however, might require additional investigation. 

We also leave exploring whether these conclusions hold for the different task types, for example, sequence tagging and question answering, as a subject of future research. Testing these 
conclusions on other languages or on other transformer-based models (for example, decoder-base ones)  is also a possible future field of study.

.
Multi-task transformer-agnostic models almost match the singletask models by metrics on the dialog tasks. The accuracy gap between the multi-task and singletask monolingual models is about 0.8-0.9\% for the English language and about 0.3-0.6\% for the Russian language. For the multilingual models, the gap remains within the same limit, except for the multilingual BERT trained on Russian data, where the gap evaporates completely.

% We also report that the training of the multi-task neural network for English data took 21-24\% more training steps than the training of the singletask models with the same criteria of early stopping. The reason is that the model did not stop until the metrics on relatively low-size tasks stop improving, therefore examples from relatively high-size tasks that already were trained) were seen more times than they would have been seen if the model was singletask and trained for any of these tasks. Убрать вообще batches seen, раз не видно паттерна?

We also show that if we have Russian and English data for the same tasks, it does not matter match whether we unite data for every task, or we treat them as separate tasks. The choice of sampling, between plain, uniform, and annealed, also did not matter match in our experiments; however, this might also indicate that annealed sampling requires thorough tuning of hyperparameters.

For the small-scale data, we can see that if we train the multilingual distilbert on small shares of Russian training data (2-5\%), multi-task models overcome singletask models in the average accuracy. As shown in Figure 2, this accuracy advantage strongly depends on the dataset size - the smaller the truncated dataset size, the larger the advantage. For intent and topic datasets this advantage evaporates at 1151 training samples, but for the emotion dataset, surprisingly, this advantage holds true with any dataset partition, possibly due to the effect of knowledge transfer from the sentiment task.

The hypothesis of the knowledge transfer dependency of the dataset size is additionally supported by the fact that for experiments with adding English data, multi-task models showed no clear pattern of advantage over singletask ones.

We also find that while having a limited amount of Russian training data (3-10\% RU share in Table 4 from Appendix)  we can additionally improve the multi-task BERT metrics up to several percent if we add the English data to the training sample. 

And in this case, we can also perform the validation on the English validation data, it does not change metrics in a meaningful way compared to the validation on the Russian validation data set. 

And also, the lower share of training data we have, the larger the accuracy gain for adding the English data to the training sample.


\section{Conclusion}

The proposed transformer-agnostic multi-task models yield results matching or approaching the single-task models on most tasks. If we truncate the tasks to the low-size number (200-2000 samples), multi-task models start exceeding the single-task ones. This effect might depend on the number of samples per class.
\fi
The multi-task transformer-agnostic architecture we propose yields just a minor decrease in accuracy and F1 macro on the dialog tasks compared to the singletask architecture. 

Adding English data to the Russian dataset can improve the metrics by up to several percent for the singletask and multi-task training. Also, starting from some data sizes, multi-task architecture starts overcoming the singletask one.


\section{Conclusion}


УБРАТЬ ЭТО- КАК ОПИШУ ХОТЯ БЫ ВСЕ ПОПЫТКИ:
Тут будет пересказ статьи про трансформер-инвариантные модели. 
Статья может быть написана на основе экспериментов по применению этих моделей в DREAM-2. Эти эксперименты описаны здесь \url{https://docs.google.com/document/d/1o7jt2jJ-hyPNYlYzUgnpwkc033DvGWRIuv3FTWI6-Ig/edit}
Полный отчет по этим экспериментам здесь \url{https://docs.google.com/document/d/1d7dlEGOD0Qr6GitVe9QH3UbX39BpYX0ay9rzIP1OZpo/edit}

Еще одна статья может быть написана на основе экспериментов по использованию этих моделей для решения задач multilabel классификации на singlelabel данных. Эти эксперименты описаны здесь
\url{https://docs.google.com/document/d/1bVpWWe-pvu3c8da08DJVkjjafXqG6GasoLuygfHh1XY/edit}
и здесь 
\url{https://docs.google.com/document/d/1imm32KmDXgD-YLW9nf7RNPJOlM32-1Fgaz3xeRVdvvo/edit}
Версии статей добавил как на 1 марта


