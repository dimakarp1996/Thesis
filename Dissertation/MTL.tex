  \chapter{Вёрстка таблиц}\label{ch:ch3} 
  Многозадачные модели
Многозадачное обучение - это метод разделения параметров между моделями, обучающимися выполнять несколько задач. Для нейронной сети его можно реализовать, приравняв значения разных слоев друг к другу.  В нейронных сетях это можно легко сделать, привязав веса разных слоев.  Идея многозадачного обучения была впервые предложена Каруаной в 1993 году \cite{Caruana_1997}. Для нейросетевых методов обработки текста оно впервые было применено в 2008 году Коллобером и Вестоном \cite{Collobert_Weston_2008}. В их модели справочные таблицы (или матрицы вложения слов) разделены между двумя моделями, обученными различным задачам, как показано на рисунке 5.
Использование общих параметров дает моделям возможность обмениваться информацией низкого уровня. Также Коллобер и Вестон в своей статьа выдвинули ряд идей (предобученные представления слов, исользование сверточных сетей), которые были по достоинству оценены позже - на конференции ICML 2018 получили награду "Испытание временем".


Рисунок 9. Пример многозадачного обучения
Пример многозадачного обучения
Авторы данного обзора (Chen et al. 2021) классифицировали архитектуры нейросетевых многозадачных моделей по следующим типам:
Параллельные архитектуры. Для данного типа архитектура одни и те же "общие" слои используются для примеров из каждой задачи, при этом выход "общих" слоев обрабатывается независимо своим специфическим слоем для каждой задачи. Плюсом данного типа архитектур является его достаточно высокая степень универсальности, а минусом - то, что необходимость получать одно и то же представление для каждой задачи может ограничивать адаптационные способности нейросетевой модели. К такому типу архитектур, в частности, принадлежит модель MT-DNN (Liu et al. 2019), которая будет подробнее рассмотрена ниже. 
Иерархические архитектуры,в которых задачи обрабатываются зависимо друг от друга: так, результат классификации примера для одной из задач может использоваться при решении другой из задач как дополнительный входной параметр. Плюсом данного типа архитектур является возможность моделирования глубоких отношений между задачами, минусом - его негибкость. 
Модульные архитектуры. Нейронная сеть в данных архитектурах делится на общие модули и задаче-специфичные модули, где общие модули имеют одни и те же веса для всех задач, а задаче-специфичные модули - свои веса для каждой из задач. Плюсом такого рода архитектур является возможность более "тонко" адаптировать модель для решения нескольких задач, что даёт возможность достигать высокой степени экономии вычислительных ресурсов и хороших результатов, реализованную, в частности, в статье (Maziarka and Danel 2021). Минусом же данного типа архитектур является отсутствие инвариантности по отношению к базовой модели : в отличие от параллельных архитектур, данный тип архитектур заточен под какую-то конкретную базовую нейросетевую модель, что делает замену базовой модели "под капотом" для многозадачных моделей из данного типа архитектур технически сложной. Данную архитектуру имеет, в частности, модель PAL-BERT  (Stickland and Murray 2019), которая будет рассмотрена в следующих разделах. 
Генеративно-состязательные архитектуры. Для данного типа архитектур генератор и дискриминатор обучаются совместно таким образом, что дискриминатор пытается предсказать, из какой задачи пример, по его выдаваемому генератором представлению. А генератор, соответственно, пытается сгенерировать такое представление, чтобы дискриминатор мог предсказать задачу как можно хуже. Такое "состязание" генератора и дискриминатора даёт возможность генератору научиться выдавать представления примера, максимально инвариантные относительно задачи, которые в дальнейшем классифицируются скрытыми слоями на выходе, специфичными для каждой задачи. Подобный тип архитектур распространен достаточно мало в связи со своей негибкостью и нестабильностью обучения генеративно-состязательных сетей. Тем не менее, его неоспоримым преимуществом является возможность использовать большой объем неразмеченных данных для получения векторных представлений задач. 
В данной диссертационной работе исследовались возможности и особенности применения многозадачных нейросетевых моделей для обработки естественного языка. Был сделан упор на 2 нейросетевые архитектуры - модель MT-DNN (Liu et al. 2019) и модель PAL-BERT (Stickland and Murray 2019). Данные модели описаны подробнее в следующих разделах.

Модель MT-DNN
Модель MT-DNN - это многозадачная нейросетевая модель,которая относится к классу параллельных архитектурах. Данная модель, применяя один BERT к поступающему входу(с линейным pooling-слоем в конце), формирует на его базе свой собственный классификатор для каждой из задач. Модель делит все поступающие задачи на разные типы, включающие в себя, в частности:
Классификация текста. Пример - задачи CoLA, SST-2 из набора задач GLUE (Wang et al. 2018).
Классификация пары последовательностей. Пример - задачи RTE, MNLI, QQP, MRPC из набора задач GLUE. 
Задачи регрессии. Пример - задача STS-B из набора задач GLUE. 
Задача попарного ранжирования ответов на вопросы. Авторы оригинальной статьи используют в этом качестве задачу QNLI из набора данных GLUE.
Распознавание именованных сущностей. Пример - задача определения частей речи в наборе данных CONLL (Erik F. Tjong Kim Sang and De Meulder 2003) . 
Модель принимает на вход последовательность     то X =X_{1}….X_{m}, токенизированную способом, аналогичным описанному выше в разделе про BERT способу. Далее с данной последовательность происходят следующие трансформации:
Тренируемый слой L1 преобразует каждый токен в векторное представление, не зависящее от контекста. 
Данное представление подается на вход кодировщику BERT, выходом которого является финальное векторное представление L2. 
На выходе из L2 для каждого типа задач формируются задаче-специфичные слои, преобразующие это векторное представление и классифицирующие его. 
В общих чертах схема данной трансформации для набора данных GLUE приведена на рисунке. 

Рисунок. Схема модели MT-DNN.
В задаче-специфичных слоях для решения задачи классификации,вычисление  вероятности P(x) (вектор вероятностей P_{1},.... P_{n} для принадлежности к классу x_{1},... x_{n}) для векторного представления X токена [CLS] производится по формуле:
$$ P(x) = softmax(W^{T}X +B) $$, где W^{T} матрица с тренируемыми коэффициентами размерности H*n, где H размерность скрытых состояний и n число классов, B тренируемый вектор размерности n
В качестве функции потерь для данной задачи используется кросс-энтропия. 
Для решения задачи определения семантической близости (такой, как STS-B) используется похожая формула:
$$S(x) = (W^{T}X +B) $$, где W^{T} матрица тренируемых весовых коэффициентов размерности H*1, где H размерность скрытых состояний и 1 число классов, B тренируемый весовой коэффициент размерности 1. 
Близость S(x) может принимать значения от -\inf до \inf. В качестве функции потерь используется среднеквадратичная ошибка. 
Для задач классификации пары последовательностей в архитектуре MT-DNN используется стохастическая сеть ответов (stochastic answer network, SAN) (Liu et al. 2018). Данная нейросетевая архитектура, основанная на рекуррентных слоях Gated Recurrent Unit(GRU)(Cho et al. 2014), принимает на вход векторные представления пары последовательностей. На каждом шаге архитектура предсказывает возможные распределения вероятности между классами, что влияет на обновление весов модели. Финальные предсказанные вероятности получаются усреднением предсказанных вероятностей. 
Задача pairwise ranking (ранжирования пары последовательностей) решается аналогично предыдущим задачам, с соответствующими линейными трансформациями на входе и выходе.
Аналогичным же образом, с использованием линейных слоев, нейросетевая архитектура MT-DNN может быть адаптирована к решению задач распознавания именованных сущностей, что и было реализовано автором данной диссертационной работы в библиотеке DeepPavlov.
Как и оригинальный BERT, MT-DNN использует метод стохастического градиентного спуска для оптимизации функции потерь, подробнее описанный в (Bousquet et al. 2004). На каждом этапе модель формирует батч B_{i} для решения целевой задачи i,   дообучая классификаторы и параметры BERT в соответствии со спецификой задачи. 
Авторы оригинальной статьи производили дообучение модели BERT в соответствии со следующими параметрами: скорость обучения 5*10^{-5}, оптимизатор Adamax (Kingma and Ba 2014) размер батча 32, 5 эпох. 
Как показано в оригинальной статье, несмотря на отсутствие адаптации под конкретную задачу, модель MT-DNN превосходит BERT-LARGE на всех задачах, кроме CoLA. Как показано в (Коновалов 2022), отставание на задаче CoLA связано с особенностями данного набора данных. 
Также в оригинальной статье показано, что, после дообучения на каждую конкретную задачу модель MT-DNN показывает дополнительный прирост качества; на задачах с ограниченной обучающей выборкой (MRPC, RTE, SST-2) прирост может достигать 1-2.5%. Это говорит о том, что модель MT-DNN может переиспользовать свои знания, полученные при обучении на задачах  с более крупной обучающей выборкой, для решения задач с относительно маленькой выборкой. 
Данные цифры показывают, что использование MT-DNN позволяет и экономить вычислительные ресурсы, и повышать при этом качество решения разных задач обработки естественного текста за счёт эффекта переноса знаний. 
Именно упрощенная модификация MT-DNN(без использования стохастических сетей ответов) использовалась в данной диссертационной работе и как трансформер-инвариантная нейросетевая архитектура, и как архитектура с одним линейным слоем, для которой исследовались различные варианты псевдоразметки данных. Подробнее данные исследования описаны в следующих главах. 
Модель PAL-BERT
Модель PAL-BERT, предложенная авторами статьи (Stickland and Murray 2019), представляет собой вариант модульной нейросетевой многозадачной архитектуры. У нейросетевой модели в данном варианте есть задаче-специфичные слои с весами, отдельными для каждой конкретной задачичи общие слои. При этом часть задаче-специфичных слоев встраивается непосредственно "в тело" модели, основанной на архитектуре Трансформер. 
Слои, встраиваемые "в тело" такой модели, называются PALs - Projective Attention Layers (проективные слои внимания). Их действие описывается формулой 
$$PAL(h) = V_{d}*g(V_{e}*h)$$, 
где V_{e} матрица кодировщика размерности S*H, H размерность вектора скрытого слоя $$h$$, S<H (авторы статьи предложили S=204 для BERT-LARGE). 
V_{d} матрица декодировщика размерности H*S
и где g - многоголовое внимание
Матрицы V_{d} и V_{e} одни и те же для каждого слоя, но специфичные для каждой конкретной задачи. 
Эти слои добавляются авторами данной нейросетевой архитектуры в модель BERT следующим образом. Если выход каждого следующего слоя стандартной модели BERT $BERT_{i}$ зависит от выхода предыдущего слоя $BERT_{i-1$ по формуле
$$BERT_{i} =LayerNorm(BERT_{i-1})$$,
то для модели PAL-BERT зависимость имеет такой вид:
      $$BERT_{i} =LayerNorm(BERT_{i-1} + PAL(BERT_{i-1)$$

Подобное использование задаче-специфичных проективных слоев внимания можно проиллюстрировать на рисунке.

Рисунок. Использование проективных слоев внимания(PAL1, PAL2) в модели PAL-BERT. LN означает LayerNorm, SA самовнимание.
Использование архитектуры PAL-BERT увеличивает число параметров, необходимых для решения 8 задач GLUE,всего на 13 процентов. 
Авторы оригинальной статьи также применяли многозадачное обучение для модели PAL-BERT с использованием аннеалированного сэмплирования (annealed sampling). При использовании данного типа сэмплирования, вероятность выбора примера из i-того задания P_{i} определяется следующим образом:
  $$P_{i} ~N^{(1-0.8*(e-1))/(E-1)}$$
 где e - номер эпохи, а E - общее число эпох. 
Заметим, что полученные таким образом значения вероятностей нормируются на 1. 
Как показано в оригинальной статье, модель PAL-BERT превосходит модель с добавлением одного линейного слоя "на верхушку" модели. В связи с хорошими показателями и низким вычислительным бюджетом, модель PAL-BERT применялась как многозадачная модель на одном из этапов развития диалоговой системы DREAM, о чем будет подробнее написано ниже. Однако отрицательным аспектом данной архитектуры является её негибкость и необходимость "вручную" подстраивать под каждую модификацию модели Transformer - а их на момент написания диссертации было разработано очень большое число, и регулярно появлялись новые модификации. В связи с необходимостью иметь возможность более "гибко" работать со всеми такими модификациями, было  принято решение в дальнейшем использовать иные имплементации многозадачных нейросетевых моделей в диалоговой системе DREAM. 
Обзор самой диалоговой системы DREAM приведён в следующем разделе. 

 \section{Таблица обыкновенная}\label{sec:ch3/sect1} 
  
 Так размещается таблица: 
  
 \begin{table} [htbp] 
   \centering 
   \changecaptionwidth\captionwidth{15cm} 
   \caption{Название таблицы}\label{tab:Ts0Sib}% 
   \begin{tabular}{| p{3cm} || p{3cm} | p{3cm} | p{4cm}l |} 
   \hline 
   \hline 
   Месяц   & \centering \(T_{min}\), К & \centering \(T_{max}\), К &\centering  \((T_{max} - T_{min})\), К & \\ 
   \hline 
   Декабрь &\centering  253.575   &\centering  257.778    &\centering      4.203  &   \\ 
   Январь  &\centering  262.431   &\centering  263.214    &\centering      0.783  &   \\ 
   Февраль &\centering  261.184   &\centering  260.381    &\centering     \(-\)0.803  &   \\ 
   \hline 
   \hline 
   \end{tabular} 
 \end{table} 
  
 \begin{table} [htbp]% Пример записи таблицы с номером, но без отображаемого наименования 
     \centering 
     \parbox{9cm}{% чтобы лучше смотрелось, подбирается самостоятельно 
         \captiondelim{}% должен стоять до самого пустого caption 
         \caption{}% 
         \label{tab:test1}% 
         \begin{SingleSpace} 
             \begin{tabular}{| c | c | c | c |} 
                 \hline 
                 Оконная функция & \({2N}\)& \({4N}\)& \({8N}\)\\ \hline 
                 Прямоугольное   & 8.72  & 8.77  & 8.77  \\ \hline 
                 Ханна           & 7.96  & 7.93  & 7.93  \\ \hline 
                 Хэмминга        & 8.72  & 8.77  & 8.77  \\ \hline 
                 Блэкмана        & 8.72  & 8.77  & 8.77  \\ \hline 
             \end{tabular}% 
         \end{SingleSpace} 
     } 
 \end{table} 
  
 Таблица~\ref{tab:test2} "--- пример таблицы, оформленной в~классическом книжном 
 варианте или~очень близко к~нему. \mbox{ГОСТу} по~сути не~противоречит. Можно 
 ещё~улучшить представление, с~помощью пакета \verb|siunitx| или~подобного. 
  
 \begin{table} [htbp]% 
     \centering 
     \caption{Наименование таблицы, очень длинное наименование таблицы, чтобы посмотреть как оно будет располагаться на~нескольких строках и~переноситься}% 
     \label{tab:test2}% label всегда желательно идти после caption 
     \renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия. 
     \begin{SingleSpace} 
         \begin{tabular}{@{}@{\extracolsep{20pt}}llll@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям 
             \toprule     %%% верхняя линейка 
             Оконная функция & \({2N}\)& \({4N}\)& \({8N}\)\\ 
             \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
             Прямоугольное   & 8.72  & 8.77  & 8.77  \\ 
             Ханна           & 7.96  & 7.93  & 7.93  \\ 
             Хэмминга        & 8.72  & 8.77  & 8.77  \\ 
             Блэкмана        & 8.72  & 8.77  & 8.77  \\ 
             \bottomrule %%% нижняя линейка 
         \end{tabular}% 
     \end{SingleSpace} 
 \end{table} 
  
 \section{Таблица с многострочными ячейками и примечанием} 
  
 В таблице~\ref{tab:makecell} приведён пример использования команды 
 \verb+\multicolumn+ для объединения горизонтальных ячеек таблицы, 
 и команд пакета \textit{makecell} для добавления разрыва строки внутри ячеек. 
  
 \begin{table} [htbp] 
         \centering 
         \caption{Пример использования функций пакета \textit{makecell}.}% 
         \label{tab:makecell}% 
         \begin{tabular}{| c | c | c | c |} 
           \hline 
           Колонка 1                                    & Колонка 2        & \thead{Название колонки 3, \\ не помещающееся в одну строку} & Колонка 4 \\ \hline 
           \multicolumn{4}{|c|}{Выравнивание по центру}                                                                                               \\ \hline 
           \multicolumn{2}{|r|}{\makecell{Выравнивание к \\ правому краю}} & \multicolumn{2}{|l|}{Выравнивание к левому краю}                         \\ \hline 
           \makecell{В этой ячейке \\ много информации} & 8.72             & 8.55                                                         & 8.44      \\ \cline{3-4} 
           А в этой мало                                & 8.22             & \multicolumn{2}{|c|}{5}                                                  \\ \hline 
         \end{tabular}% 
 \end{table} 
  
 Таблицы~\ref{tab:test3} и~\ref{tab:test4} "--- пример реализации расположения 
 примечания в~соответствии с ГОСТ 2.105. Каждый вариант со своими достоинствами 
 и~недостатками. Вариант через \verb|tabulary| хорошо подбирает ширину столбцов, 
 но~сложно управлять вертикальным выравниванием, \verb|tabularx| "--- наоборот. 
 \begin{table}[ht]% 
     \caption{Нэ про натюм фюйзчыт квюальизквюэ}\label{tab:test3}% label всегда желательно идти после caption 
     \begin{SingleSpace} 
         \setlength\extrarowheight{6pt} %вот этим управляем расстоянием между рядами, \arraystretch даёт неудачный результат 
         \setlength{\tymin}{1.9cm}% минимальная ширина столбца 
         \begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям 
             \toprule     %%% верхняя линейка 
             доминг лаборамюз эи ыам (Общий съём цен шляп (юфть)) & Шеф взъярён & 
             адвыржаряюм & 
             тебиквюэ элььэефэнд мэдиокретатым & 
             Чэнзэрет мныжаркхюм        \\ 
             \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
             Эй, жлоб! Где туз? Прячь юных съёмщиц в~шкаф Плюш изъят. Бьём чуждый цен хвощ! & 
             \({\approx}\) & 
             \({\approx}\) & 
             \({\approx}\) & 
             \( + \) \\ 
             Эх, чужак! Общий съём цен & 
             \( + \) & 
             \( + \) & 
             \( + \) & 
             \( - \) \\ 
             Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку. Ад 
             граэкйж плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат, ат шэа 
             одео & 
             \({\approx}\) & 
             \( - \) & 
             \( - \) & 
             \( - \) \\ 
             Любя, съешь щипцы, "--- вздохнёт мэр, "--- кайф жгуч. & 
             \( - \) & 
             \( + \) & 
             \( + \) & 
             \({\approx}\) \\ 
             Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку. Ад 
             граэкйж плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат, ат шэа 
             одео квюаырэндум. Вёртюты ажжынтиор эффикеэнди эож нэ. & 
             \( + \) & 
             \( - \) & 
             \({\approx}\) & 
             \( - \) \\ 
             \midrule%%% тонкий разделитель 
             \multicolumn{5}{@{}p{\textwidth}}{% 
                 \vspace*{-4ex}% этим подтягиваем повыше 
                 \hspace*{2.5em}% абзацный отступ - требование ГОСТ 2.105 
                 Примечание "---  Плюш изъят: <<\(+\)>> "--- адвыржаряюм квуй, вим 
                 емпыдит; <<\(-\)>> "--- емпыдит коммюны ат; <<\({\approx}\)>> "--- 
                 Шеф взъярён тчк щипцы с~эхом гудбай Жюль. Эй, жлоб! Где туз? 
                 Прячь юных съёмщиц в~шкаф. Экс-граф? 
             } 
             \\ 
             \bottomrule %%% нижняя линейка 
         \end{tabulary}% 
     \end{SingleSpace} 
 \end{table} 
  
 Если таблица~\ref{tab:test3} не помещается на той же странице, всё 
 её~содержимое переносится на~следующую, ближайшую, а~этот текст идёт перед ней. 
 \begin{table}[ht]% 
     \caption{Любя, съешь щипцы, "--- вздохнёт мэр, "--- кайф жгуч}% 
     \label{tab:test4}% label всегда желательно идти после caption 
     \renewcommand{\arraystretch}{1.6}%% Увеличение расстояния между рядами, для улучшения восприятия. 
     \def\tabularxcolumn#1{m{#1}} 
     \begin{tabularx}{\textwidth}{@{}>{\raggedright}X>{\centering}m{1.9cm} >{\centering}m{1.9cm} >{\centering}m{1.9cm} >{\centering\arraybackslash}m{1.9cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям 
         \toprule     %%% верхняя линейка 
         доминг лаборамюз эи ыам (Общий съём цен шляп (юфть)) & Шеф взъярён & 
         адвыр\-жаряюм & 
         тебиквюэ элььэефэнд мэдиокретатым & 
         Чэнзэрет мныжаркхюм        \\ 
         \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5 
         Эй, жлоб! Где туз? Прячь юных съёмщиц в~шкаф Плюш изъят. 
         Бьём чуждый цен хвощ! & 
         \({\approx}\) & 
         \({\approx}\) & 
         \({\approx}\) & 
         \( + \) \\ 
         Эх, чужак! Общий съём цен & 
         \( + \) & 
         \( + \) & 
         \( + \) & 
         \( - \) \\ 
         Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку. 
         Ад граэкйж плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат, 
         ат шэа одео & 
         \({\approx}\) & 
         \( - \) & 
         \( - \) & 
         \( - \) \\ 
         Любя, съешь щипцы, "--- вздохнёт мэр, "--- кайф жгуч. & 
         \( - \) & 
         \( + \) & 
         \( + \) & 
         \({\approx}\) \\ 
         Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку. Ад граэкйж 
         плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат, ат шэа одео 
         квюаырэндум. Вёртюты ажжынтиор эффикеэнди эож нэ. & 
         \( + \) & 
         \( - \) & 
         \({\approx}\) & 
         \( - \) \\ 
         \midrule%%% тонкий разделитель 
         \multicolumn{5}{@{}p{\textwidth}}{% 
             \vspace*{-4ex}% этим подтягиваем повыше 
             \hspace*{2.5em}% абзацный отступ - требование ГОСТ 2.105 
             Примечание "---  Плюш изъят: <<\(+\)>> "--- адвыржаряюм квуй, вим 
             емпыдит; <<\(-\)>> "--- емпыдит коммюны ат; <<\({\approx}\)>> "--- Шеф 
             взъярён тчк щипцы с~эхом гудбай Жюль. Эй, жлоб! Где туз? Прячь юных 
             съёмщиц в~шкаф. Экс-граф? 
         } 
         \\ 
         \bottomrule %%% нижняя линейка 
     \end{tabularx}% 
 \end{table} 
  
 \section{Таблицы с форматированными числами}\label{sec:ch3/formatted-numbers} 
  
 В таблицах~(\labelcref{tab:S:parse,tab:S:align}) представлены примеры использования опции 
 форматирования чисел \texttt{S}, предоставляемой пакетом \texttt{siunitx}. 
  
 \begin{table} 
     \centering 
     \caption{Выравнивание столбцов.}\label{tab:S:parse} 
     \begin{tabular}{SS[table-parse-only]} 
         \toprule 
         {Выравнивание по разделителю} & {Обычное выравнивание} \\ 
         \midrule 
         12.345                        & 12.345                 \\ 
         6,78                          & 6,78                   \\ 
         -88.8(9)                      & -88.8(9)               \\ 
         4.5e3                         & 4.5e3                  \\ 
         \bottomrule 
     \end{tabular} 
 \end{table} 
  
 \begin{table} 
     \caption{Выравнивание с использованием опции \texttt{S}.}\label{tab:S:align} 
     \centering 
     \sisetup{ 
         table-figures-integer = 2, 
         table-figures-decimal = 4 
     } 
     \begin{tabular} 
         {SS[table-number-alignment = center]S[table-number-alignment = left]S[table-number-alignment = right]} 
         \toprule 
         {Колонка 1} & {Колонка 2} & {Колонка 3} & {Колонка 4} \\ 
         \midrule 
         2.3456      & 2.3456      & 2.3456      & 2.3456      \\ 
         34.2345     & 34.2345     & 34.2345     & 34.2345     \\ 
         56.7835     & 56.7835     & 56.7835     & 56.7835     \\ 
         90.473      & 90.473      & 90.473      & 90.473      \\ 
         \bottomrule 
     \end{tabular} 
 \end{table} 
  
 \section{Параграф "--- два}\label{sec:ch3/sect2} 
  
 Некоторый текст. 
  
 \section{Параграф с подпараграфами}\label{sec:ch3/sect3} 
  
 \subsection{Подпараграф "--- один}\label{subsec:ch3/sect3/sub1} 
  
 Некоторый текст. 
  
 \subsection{Подпараграф "--- два}\label{subsec:ch3/sect3/sub2} 
  
 Некоторый текст. 
  
 \clearpage