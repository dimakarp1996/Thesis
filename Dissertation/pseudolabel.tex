\chapter{Вёрстка таблиц}\label{ch:ch3}
Использование псевдоразметки данных в многозадачных моделях для решения задач GLUE
Простейшим способом трансформер-инвариантной модели для решения большого количества задач является использование модели с одним “трансформером” в качестве тела(например, BERT-base-uncased) и одним линейным слоем на все задачи. Данный вид модели максимально экономичен по потребляемой памяти, как и по технической реализации. Тем не менее, при обучении его на нескольких датасетах возникает проблема, так как у каждого датасета есть метки только для классов “своей” задачи. У некоторых датасетов классы могут быть схожими, у других - кардинально различаться. В работе \cite{Karpov_Burtsev_2021} автором диссертации была подробно исследована данная проблема. Эксперименты, проделанные в данной работе, подробно описаны ниже.





Описание экспериментов
В каждом из экспериментов архитектура модели оставалась той же самой, изменялись лишь метки классов, подаваемые на вход модели. Модель тренировалась предсказывать вероятность от 0 до 1 для каждого класса, в multilabel режиме.
Модель оценивалась на следующих классификационных задачах - Multi-Genre Natural Language Inference(далее - MNLI), Quora Question Pairs(далее - QQP), Stanford Sentiment Treebank - 2(далее - SST-2) и Recognizing Textual Entailment(далее - RTE) из набора данных GLUE \cite{Wang_Singh_Michael_Hill_Levy_Bowman_2018}. Первые 3 задачи были выбраны, так как их наборы данных были достаточно велики ( более 50000 примеров для каждой из задач). Четвертая задача была выбрана, чтобы получить возможность исследовать объединение классов при псевдоразметке. Также для каждой из этих задач были воспроизведены результаты из вышеупомянутой статьи(заметим, что оригинальная модель не была multilabel). Примеры из всех задач были перемешаны и выбирались случайным образом. В связи с вычислительными ограничениями, все эксперименты проводились при модели-теле BERT-Base-Uncased \cite{Devlin_Chang_Lee_Toutanova_2019}, аналогичной оригинальной статье, с ним же шло

Условные обозначения
Для формул ниже будут использоваться следующие условные обозначения:
+ и - означают положительный(positive) и отрицательный(negative) классы в наборе данных SST-2 
d и !d означают классы “дубликат”(duplicate) и “не дубликат”(not_duplicate) в наборе данных QQP
e, c , n означают классы “логическое следствие”(entailment), “логическое противоречие”(contradiction) и “нейтральный” (neutral) в наборе данных MNLI
ε, !ε означают классы “логическое следствие”(entailment) и “нет логического следствия”(not_entailment) в наборе данных RTE
$$\mathit{MNLIpred}_{c}$$,$$\mathit{MNLIpred}_{n}$$,$$\mathit{MNLIpred}_{e}$$,$$\mathit{QQPpred}_{d}$$,$$\mathit{QQPpred}_{!d}$$,$$\mathit{SSTpred}_{+}$$,$$\mathit{SSTpred}_{-}$$ означают вероятности, предсказанные моделей, обученных на соответствующих задачах(MNLI, QQP, SST)     для соответствующих меток из верхнего индекса(см.предыдущие пункты). Например, $$\mathit{MNLIpred}_{c}$$- вероятность метки “логическое следствие”, предсказанной моделью, обученной на MNLI
I означает округление предсказанной оригинальной моделью вероятностного вектора: самый большой элемент считается равным 1, остальные зануляются.  Например, $$I(\mathit{MNLIpred}_{e})$$ равно 1 если самый вероятный класс, предсказанный моделью, обученной на MNLI - “логическое следствие”, иначе эта функция равна нулю.
$$\mathit{MNLIpred}^{!e}_{n}$$,$$\mathit{MNLIpred}^{!e}_{c}$$ - вероятности, предсказанные для классов “нейтральный” и “логическое противоречие” модели MNLI при условии, что вероятность класса “логическое следствие” задается равной нулю. После этого вероятности этих классов вычисляются так, как если бы классов было не 3, а всего 2, по формулам
$$\mathit{MNLIpred}^{!e}_{n} = \mathit{MNLIpred}^n/(\mathit{MNLIpred}^n + \mathit{MNLIpred}^c) $$
$$ \mathit{MNLIpred}^{!e}_{c} = \mathit{MNLIpred}^c/(\mathit{MNLIpred}^n + \mathit{MNLIpred}^c) $$
$$prob^{label}_{task}$$  - это вектор с вероятностями от 0 до 1 для каждой метки, которые мы задаем для примера из задачи task, имеющего метку label 
Plabel  - это вероятность P для метки label. P может быть от 0 до 1, например, 0.5label
Вероятностные вектора обозначаются с использованием квадратных скобок.
 
Способы обучения многозадачной модели


Автором были рассмотрены разные способы обучения многозадачных моделей. Эти способы представлены ниже.
Независимые метки   
В данном подходе, модель обучается на данных из всех задач - RTE, MNLI, QQP и SST-2. При этом массивы меток для каждой из задач независимы: для каждого примера вероятность всех классов, кроме изначально заданного в одном из этих датасетов, считается равной нулю.  Всего при данном подходе 9 классов - 3 для задачи MNLI и по 2 для каждой из остальных 3 задач.   
Вероятности, подаваемые на вход модели, для данного метода можно описать следующими формулами:

$$prob^{\varepsilon}_{RTE}  = [1_{\varepsilon}, 0_{!\varepsilon},0_{e},0_{c},0_{n},0_{d}, 0_{!d},0_{+},0_{-}] $$
$$prob^{!\varepsilon}_{RTE} = [0_{\varepsilon}, 1_{!\varepsilon},0_{e},0_{c},0_{n},0_{d}, 0_{!d},0_{+},0_{-}] $$
$$prob^{e}_{MNLI} = [0_{\varepsilon}, 0_{!\varepsilon},1_{e},0_{c},0_{n},0_{d}, 0_{!d},0_{+},0_{-}] $$
$$prob^{c}_{MNLI} = [0_{\varepsilon}, 0_{!\varepsilon},0_{e},1_{c},0_{n},0_{d}, 0_{!d},0_{+},0_{-}] $$
$$prob^{n}_{MNLI} = [0_{\varepsilon}, 0_{!\varepsilon},0_{e},0_{c},1_{n},0_{d}, 0_{!d},0_{+},0_{-}] $$
$$ prob^{d}_{QQP} = [0_{\varepsilon}, 0_{!\varepsilon},0_{e},0_{c},0_{n},1_{d}, 0_{!d},0_{+},0_{-}] $$
$$prob^{!d}_{QQP} = [0_{\varepsilon}, 0_{!\varepsilon},0_{e},0_{c},0_{n},0_{d}, 1_{!d},0_{+},0_{-}] $$
$$prob^{+}_{SST} = [0_{\varepsilon}, 0_{!\varepsilon},0_{e},0_{c},0_{n},0_{d}, 0_{!d},1_{+},0_{-}] $$
$$prob^{-}_{SST} = [0_{\varepsilon}, 0_{!\varepsilon},0_{e},0_{c},0_{n},0_{d}, 0_{!d},0_{+},1_{-}] $$

Мягкие независимые метки


Данный подход аналогичен подходу Независимые метки с одним отличием - для каждого примера, обнуляется не вероятность абсолютно всех классов кроме того, что изначально задан, а только вероятность других классов той же самой задачи. Вероятности других классов других задач считаются одинаковыми, с суммой вероятностей равной 1 для каждой из задач.
Вероятности, подаваемые на вход модели, для данного метода можно описать следующими формулами:
    
$$prob^{\varepsilon}_{RTE}  = [1_{\varepsilon}, 0_{!\varepsilon},1/3_{e},1/3_{c},1/3_{n},1/2_{d}, 1/2_{!d},1/2_{+},1/2_{-}]$$
$$prob^{!\varepsilon}_{RTE} = [0_{\varepsilon}, 1_{!\varepsilon},1/3_{e},1/3_{c},1/3_{n},1/2_{d}, 1/2_{!d},1/2_{+},1/2_{-}]$$
$$prob^{e}_{MNLI} = [1/2_{\varepsilon}, 1/2_{!\varepsilon},1_{e},0_{c},0_{n},1/2_{d}, 1/2_{!d},1/2_{+},1/2_{-}]$$
$$prob^{c}_{MNLI} = [1/2_{\varepsilon}, 1/2_{!\varepsilon},0_{e},1_{c},0_{n},1/2_{d}, 1/2_{!d},1/2_{+},1/2_{-}]$$
$$prob^{n}_{MNLI} = [1/2_{\varepsilon}, 1/2_{!\varepsilon},0_{e},0_{c},1_{n},1/2_{d}, 1/2_{!d},1/2_{+},1/2_{-}]$$
$$prob^{d}_{QQP} = [1/2_{\varepsilon}, 1/2_{!\varepsilon},1/3_{e},1/3_{c},1/3_{n},1_{d}, 0_{!d},1/2_{+},1/2_{-}]$$
$$prob^{!d}_{QQP} = [1/2_{\varepsilon}, 1/2_{!\varepsilon},1/3_{e},1/3_{c},1/3_{n},0_{d}, 1_{!d},1/2_{+},1/2_{-}]$$
$$prob^{+}_{SST} = [1/2_{\varepsilon}, 1/2_{!\varepsilon},1/3_{e},1/3_{c},1/3_{n},1/2_{d}, 1/2_{!d},1_{+},0_{-}]$$
$$prob^{-}_{SST} = [1/2_{\varepsilon}, 1/2_{!\varepsilon},1/3_{e},1/3_{c},1/3_{n},1/2_{d}, 1/2_{!d},0_{+},1_{-}]$$


Дополненные независимые метки
Данный подход аналогичен подходам Независимые метки и Мягкие независимые метки с одним отличием. Для каждого примера, вероятности всех классов из “не своего” датасета не считаются одинаковыми, а определяются в соответствии с предсказаниями модели, обученной на этом “не своем” датасете в рамках воспроизведения оригинальной статьи.
Вероятности, подаваемые на вход модели, для данного метода можно описать следующими формулами:


$$prob^{\varepsilon}_{RTE} = [1_{\varepsilon}, 0_{!\varepsilon},{\mathit{MNLIpred}}_{e},\mathit{MNLIpred}_{c},\mathit{MNLIpred}_{n},\mathit{QQPpred}_{d},\nonumber
 \mathit{QQPpred}_{!d},\mathit{SSTpred}_{+},\mathit{SSTpred}_{-}]$$
$$prob^{!\varepsilon}_{RTE} = [0_{\varepsilon}, 1_{!\varepsilon},{\mathit{MNLIpred}}_{e},\mathit{MNLIpred}_{c},\mathit{MNLIpred}_{n},\mathit{QQPpred}_{d},\nonumber
 \mathit{QQPpred}_{!d},\mathit{SSTpred}_{+},\mathit{SSTpred}_{-}]$$
$$prob^{e}_{MNLI} = [\mathit{RTEpred}_{\varepsilon}, \mathit{RTEpred}_{!\varepsilon},1_{e},0_{c},0_{n},\mathit{QQPpred}_{d},\nonumber
 \mathit{QQPpred}_{!d},\mathit{SSTpred}_{+},\mathit{SSTpred}_{-}]$$
$$prob^{c}_{MNLI} = [\mathit{RTEpred}_{\varepsilon}, \mathit{RTEpred}_{!\varepsilon},0_{e},1_{c},0_{n},\mathit{QQPpred}_{d},\nonumber
 \mathit{QQPpred}_{!d},\mathit{SSTpred}_{+},\mathit{SSTpred}_{-}]$$
$$prob^{n}_{MNLI} = [\mathit{RTEpred}_{\varepsilon}, \mathit{RTEpred}_{!\varepsilon},0_{e},0_{c},1_{n},\mathit{QQPpred}_{d},\nonumber
 \mathit{QQPpred}_{!d},\mathit{SSTpred}_{+},\mathit{SSTpred}_{-}]$$
$$prob^{d}_{QQP} = [\mathit{RTEpred}_{\varepsilon}, \mathit{RTEpred}_{!\varepsilon},\mathit{MNLIpred}_{e},\nonumber
 \mathit{MNLIpred}_{c},\mathit{MNLIpred}_{n},1_{d},0_{!d},\mathit{SSTpred}_{+},\mathit{SSTpred}_{-}]$$
$$prob^{!d}_{QQP} = [\mathit{RTEpred}_{\varepsilon}, \mathit{RTEpred}_{!\varepsilon},\mathit{MNLIpred}_{e},\mathit{MNLIpred}_{c},\nonumber
 \mathit{MNLIpred}_{n},0_{d},1_{!d},\mathit{SSTpred}_{+},\mathit{SSTpred}_{-}]$$
$$prob^{+}_{SST} = [\mathit{RTEpred}_{\varepsilon}, \mathit{RTEpred}_{!\varepsilon},\mathit{MNLIpred}_{e},\mathit{MNLIpred}_{c},\nonumber
 \mathit{MNLIpred}_{n},\mathit{QQPpred}_{d},\mathit{QQPpred}_{!d},1_{+},0_{-}]$$
$$prob^{-}_{SST} = [\mathit{RTEpred}_{\varepsilon}, \mathit{RTEpred}_{!\varepsilon},\mathit{MNLIpred}_{e},\mathit{MNLIpred}_{c},\nonumber
 \mathit{MNLIpred}_{n},\mathit{QQPpred}_{d},\mathit{QQPpred}_{!d},0_{+},1_{-}]$$


Мягкое вероятностное предположение
В данном эксперименте, как и в экспериментах Независимые метки, Мягкие независимые метки и Дополненные независимые метки, модель была обучена на данных из всех четырех задач - RTE, MNLI, QQP и SST-2. Но при этом метки для этих задач считались зависимыми. А именно, количество классов для модели было сокращено до 5: положительный(positive), негативный(negative), логическое следствие(entailment), логическое противоречие(contradiction), нейтральный(neutral). Набор классов из этих датасетов был переведен в вероятности этих 5 классов в соответствие со следующими правилами:
Метки из всех датасетов, кроме SST, считаются на 50% положительных и на 50% отрицательными
Метки из датасета SST считаются имеющими классы “логическое следствие”, “логическое противоречие” и “нейтральный” с вероятностью ⅓ каждый. Им назначаются вероятности классов “положительный” и “отрицательный” в соответствии с оригинальным набором данных.
Меткам из датасета MNLI назначаются вероятности классов “логическое следствие”, “логическое противоречие” и “нейтральный” в соответствии с оригинальным набором данных.
Меткам из датасеты QQP, если у них класс “дубликат”, назначается вероятность класса “логическое следствие” 1 и вероятности классов “логическое противоречие” и “нейтральный” 0. Иначе вероятность класса “логическое следствие” считается равной 0, а вероятности классов “логическое противоречие” и “нейтральный” считаются равными по 0.5.
Меткам из датасета RTE, если у них класс “логическое следствие”, тоже назначается вероятность класса “логическое следствие” 1 и вероятности классов “логическое противоречие” и “нейтральный” 0. Иначе вероятность класса “логическое следствие” считается равной 0, а вероятности классов “логическое противоречие” и “нейтральный” считаются равными по 0.5.


Вероятности, подаваемые на вход модели, для данного метода можно описать следующими формулами:

$$prob^{\varepsilon}_{RTE}  = [{1}_{e}, {0}_{c},{0}_{n},{1/2}_{+},{1/2}_{-}]$$
$$prob^{!\varepsilon}_{RTE}  = [{0}_{e}, {1/2}_{c},{1/2}_{n},{1/2}_{+},{1/2}_{-}]$$
$$prob^{e}_{MNLI}  = [{1}_{e}, {0}_{c},{0}_{n},{1/2}_{+},{1/2}_{-}]$$
$$prob^{c}_{MNLI}  = [{0}_{e}, {1}_{c},{0}_{n},{1/2}_{+},{1/2}_{-}]$$
$$prob^{n}_{MNLI}  = [{0}_{e}, {0}_{c},{1}_{n},{1/2}_{+},{1/2}_{-}]$$
$$ prob^{d}_{QQP}  = [{1}_{e}, {0}_{c},{0}_{n},{1/2}_{+},{1/2}_{-}]$$
$$prob^{!d}_{QQP}  = [{0}_{e}, {1/2}_{c},{1/2}_{n},{1/2}_{+},{1/2}_{-}]$$
$$prob^{SST}_{+} = [{1/3}_{e}, {1/3}_{c},{1/3}_{n},{1}_{+},{0}_{-}]$$
$$prob^{SST}_{-} = [{1/3}_{e}, {1/3}_{c},{1/3}_{n},{0}_{+},{1}_{-}]$$



Мягкие предсказанные метки
Данный подход аналогичен подходу Мягкое вероятностное предположение с одним отличием. Все недостающие вероятности для каждой из задач не считаются равновероятными, а определяются дополнительной разметкой от модели для каждой задачи, а именно:
Если пример не из датасета SST-2, вероятность положительного и отрицательного классов определяется по предсказаниям модели, обученной на датасете SST-2. Иначе, как и в предыдущем подходе, эти вероятности берутся из оригинального датасета.
Меткам из датасета MNLI, как и в предыдущем подходе, назначаются вероятности классов “логическое следствие”, “логическое противоречие” и “нейтральный” в соответствии с оригинальным набором данных.
Меткам из датасеты QQP, если у них класс “дубликат”, как и в предыдущем подходе, назначается вероятность класса “логическое следствие” 1 и вероятности классов “логическое противоречие” и “нейтральный” 0. Иначе вероятность класса “логическое следствие” считается равной 0, а вероятности классов “логическое противоречие” и “нейтральный” определяются по предсказаниям модели, обученной на наборе данных MNLI, нормализованных на сумму предсказанных вероятностей этих 2 классов.
Меткам из датасета RTE, если у них класс “логическое следствие”, как и в предыдущем подходе, назначается вероятность класса “логическое следствие” 1 и вероятности классов “логическое противоречие” и “нейтральный” 0.  Иначе вероятность класса “логическое следствие” считается равной 0, а вероятности классов “логическое противоречие” и “нейтральный” определяются по предсказаниям модели, обученной на наборе данных MNLI, нормализованных на сумму предсказанных вероятностей этих 2 классов.

Вероятности, подаваемые на вход модели, для данного метода можно описать следующими формулами:

$$prob^{\varepsilon}_{RTE}  = [{1}_{e}, {0}_{c},{0}_{n},{\mathit{SSTpred}}_{+},{\mathit{SSTpred}}_{-}]$$
$$prob^{!\varepsilon}_{RTE}  = [{0}_{e}, \mathit{MNLIpred}^{!e}_{c},\mathit{MNLIpred}^{!e}_{n},{\mathit{SSTpred}}_{+},{\mathit{SSTpred}}_{-}]$$
$$prob^{e}_{MNLI}  = [{1}_{e}, {0}_{c},{0}_{n},{\mathit{SSTpred}}_{+},{\mathit{SSTpred}}_{-}]$$
$$prob^{c}_{MNLI}  = [{0}_{e}, {1}_{c},{0}_{n},{\mathit{SSTpred}}_{+},{\mathit{SSTpred}}_{-}]$$
$$prob^{n}_{MNLI}  = [{0}_{e}, {0}_{c},{1}_{n},{\mathit{SSTpred}}_{+},{\mathit{SSTpred}}_{-}]$$
$$prob^{d}_{QQP}  = [{1}_{e}, {0}_{c},{0}_{n},{\mathit{SSTpred}}_{+},{\mathit{SSTpred}}_{-}]$$
$$prob^{!d}_{QQP} = [{0}_{e}, \mathit{MNLIpred}^{!e}_{c},\mathit{MNLIpred}^{!e}_{n},{\mathit{SSTpred}}_{+},{\mathit{SSTpred}}_{-}]$$
$$prob^{SST}_{+} = [\mathit{MNLIpred}_{e}, \mathit{MNLIpred}_{c},\mathit{MNLIpred}_{n},{1}_{+},{0}_{-}]$$
$$prob^{SST}_{-} = [\mathit{MNLIpred}_{e}, \mathit{MNLIpred}_{c},\mathit{MNLIpred}_{n},{0}_{+},{1}_{-}]$$


Жесткие предсказанные метки
Данный подход аналогичен подходу Мягкие предсказанные метки с одним изменением. Для меток, полученных из предсказаний оригинальной модели, максимальная вероятность для каждой задачи округляется до 1, а все остальные вероятности до 0.
Вероятности, подаваемые на вход модели, для данного метода можно описать следующими формулами:
$$prob^{\varepsilon}_{RTE}=[{1}_{e}, {0}_{c},{0}_{n},I({\mathit{SSTpred}}_{+}),{I(\mathit{SSTpred}}_{-})]$$
$$prob^{!\varepsilon}_{RTE}=[{0}_{e},\mathit{MNLIpred}^{!e}_{c},\mathit{MNLIpred}^{!e}_{n},I({\mathit{SSTpred}}_{+}),{Id(\mathit{SSTpred}}_{-})]$$
$$prob^{e}_{MNLI}=[{1}_{e},{0}_{c},{0}_{n},I({\mathit{SSTpred}}_{+}),{I\mathit{SSTpred}}_{-})]$$
$$prob^{c}_{MNLI}=[{0}_{e},{1}_{c},{0}_{n},I({\mathit{SSTpred}}_{+}),{I(\mathit{SSTpred}}_{-})]$$
$$prob^{n}_{MNLI}=[{0}_{e},{0}_{c},{1}_{n},I({\mathit{SSTpred}}_{+}),{I(\mathit{SSTpred}}_{-})]$$
$$prob^{d}_{QQP}=[{1}_{e},{0}_{c},{0}_{n},I(\mathit{SSTpred}_{+}),I(\mathit{SSTpred}_{-})]$$
$$prob^{!d}_{QQP}=[{0}_{e},I(\mathit{MNLIpred}^{!e}_{c}),I(\mathit{MNLIpred}^{!e}_{n}),I(\mathit{SSTpred}_{+}),I(\mathit{SSTpred}_{-})]$$
$$prob^{SST}_{+}=[I(\mathit{MNLIpred}_{e}), I(\mathit{MNLIpred}_{c}),I(\mathit{MNLIpred}_{n}),{1}_{+},{0}_{-}]$$
$$prob^{SST}_{-}=[I(\mathit{MNLIpred}_{e}), I\mathit{MNLIpred}_{c}),I(\mathit{MNLIpred}_{n}),{0}_{+},{1}_{-}]$$




Независимые метки, замороженная голова
Данный подход аналогичен подходу Независимые метки, с тем исключением, что линейный классификационный слой не обучается, а обучается только тело модели. Все формулы для данного подхода аналогичны формулам для подхода Независимые метки.
Мягкие независимые метки, замороженная голова
Данный подход аналогичен подходу Мягкие независимые метки, с тем исключением, что линейный классификационный слой не обучается, а обучается только тело модели. Все формулы для данного подхода аналогичны формулам для подхода Мягкие независимые метки.
Настройки эксперимента
Для каждого из вышеописанных подходов, включая воспроизведение результатов оригинальной статьи, было сделано 4 попытки воспроизведения. Как и в оригинальной статье, эти 4 попытки отличались только скоростями обучения. По образцу этой статьи скорость обучения была 2e-5, 3e-5, 4e-5 и 5e-5 для первой, второй третьей и четвертой попытки соответственно. В качестве финальной была выбрана скорость обучения, для которой точность на валидационном сете была максимальной. Длительность обучения была ограничена 3 эпохами. Полные валидационные данные для всех экспериментов можно увидеть в оригинальной статье.
Результаты на валидационных и тестовых данных приведены в таблицах ниже. Стоит особо отметить, что данные результаты были достигнуты с числом параметров, на 10-13% меньшим, чем в \cite{Stickland_Murray_2019}, и без изменений в базовой архитектуре нейросетевой модели.



Название сеттинга
Среднее по 4 задачам
RTE
QQP
MNLI-m
SST
Базовый(воспроизведенный)
81.3
64.6
90.8
77.3
92.7
Независимые метки
82.8
78.3
90.6
75.8
92.0
Мягкие независимые метки
82.2
69.7
89.5
75.9
92.6
Дополненные независимые метки
82.4
68.1
90.5
75.6
92.4
Мягкое вероятностное предположение
84.2
78.2
90.7
76.2
91.9
Мягкие предсказанные метки
83.2
76.3
90.5
76.0
92.2
Жесткие предсказанные метки
82.9
77.4
90.6
75.3
90.7
Независимые метки, замороженная голова
82.5
76.1
90.5
75.7
91.4
Мягкие независимые метки, замороженная голова
82.6
74.4
90.4
76.7
91.2


Таблица 1. Лучшая точность на валидационных данных (при лучшей скорости обучения из выбираемых, среднее по 3 запускам)

Название сеттинга
Среднее по 4 задачам
RTE
QQP
MNLI-m
MNLI-mm
SST
Базовый(из оригинальной статьи)
78.8
66.4
71.2
84.6
83.4
93.5
Базовый(воспроизведенный)
77.6
62.7
71.0
83.1
82.7
93.5
Независимые метки
79.0
71.5
70.9
82.7
81.7
91.3
Мягкие независимые метки
78.9
69.3
71.3
82.8
82.1
92.6
Дополненные независимые метки
77.6
64.2
71.8
81.2
80.7
93.2
Мягкое вероятностное предположение
79.7
72.7
70.7
83.4
82.3
92.5
Мягкие предсказанные метки
78.8
70.7
70.7
81.7
81.7
92.5
Жесткие предсказанные метки
79.1
71.3
71.1
81.7
81.4
92.6
Независимые метки, замороженная голова
78.2
66.9
71.8
82.6
81.8
91.9
Мягкие независимые метки, замороженная голова
79.1
70.0
71.5
83.0
82.3
92.4


Таблица 2. Лучшая точность на тестовых данных(для Quora Question Pairs - лучшая метрика F1)
Выводы и анализ результатов
Как можно видеть, разные способы дают результаты, похожие на результаты оригинальной модели BERT. Или даже более высокие результаты, если мы описываем задачу RTE. Причина данного преимущества для задачи RTE -  его схожесть с остальными задачами из GLUE, для которых данных гораздо больше, чем для RTE.
 Отсутствие роста показателей для QQP при объединении меток показывает, что объединение меток для этой задачи было слишком грубым. Это показывает ограничения для предложенных способов объединения меток.
На задачах, которые больше всего похожи друг на друга - RTE и MNLI - лучше всего показывает себя метод Мягкое вероятностное предположение. Это доказывает оправданность объединения меток при решении похожих задач. Этот метод превосходит результаты для RTE из оригинальной статьи и отстает от результатов для других задач только на 0.5-1.2%.
На задачах, не похожих на остальные, таких, как SST и QQP, лучше всего показывает себя метод Дополненные независимые метки, что объясняется эффектом переноса знаний. Этот метод должен работать лучше всех вышеописанных методов в условиях, когда задачи достаточно сильно отличаются друг от друга. Именно этот подход применялся при работе над многозадачными моделями в диалоговой системе DREAM, которая будет описана в следующем разделе.

\section{Таблица обыкновенная}\label{sec:ch3/sect1}

Так размещается таблица:

\begin{table} [htbp]
  \centering
  \changecaptionwidth\captionwidth{15cm}
  \caption{Название таблицы}\label{tab:Ts0Sib}%
  \begin{tabular}{| p{3cm} || p{3cm} | p{3cm} | p{4cm}l |}
  \hline
  \hline
  Месяц   & \centering \(T_{min}\), К & \centering \(T_{max}\), К &\centering  \((T_{max} - T_{min})\), К & \\
  \hline
  Декабрь &\centering  253.575   &\centering  257.778    &\centering      4.203  &   \\
  Январь  &\centering  262.431   &\centering  263.214    &\centering      0.783  &   \\
  Февраль &\centering  261.184   &\centering  260.381    &\centering     \(-\)0.803  &   \\
  \hline
  \hline
  \end{tabular}
\end{table}

\begin{table} [htbp]% Пример записи таблицы с номером, но без отображаемого наименования
    \centering
    \parbox{9cm}{% чтобы лучше смотрелось, подбирается самостоятельно
        \captiondelim{}% должен стоять до самого пустого caption
        \caption{}%
        \label{tab:test1}%
        \begin{SingleSpace}
            \begin{tabular}{| c | c | c | c |}
                \hline
                Оконная функция & \({2N}\)& \({4N}\)& \({8N}\)\\ \hline
                Прямоугольное   & 8.72  & 8.77  & 8.77  \\ \hline
                Ханна           & 7.96  & 7.93  & 7.93  \\ \hline
                Хэмминга        & 8.72  & 8.77  & 8.77  \\ \hline
                Блэкмана        & 8.72  & 8.77  & 8.77  \\ \hline
            \end{tabular}%
        \end{SingleSpace}
    }
\end{table}

Таблица~\ref{tab:test2} "--- пример таблицы, оформленной в~классическом книжном
варианте или~очень близко к~нему. \mbox{ГОСТу} по~сути не~противоречит. Можно
ещё~улучшить представление, с~помощью пакета \verb|siunitx| или~подобного.

\begin{table} [htbp]%
    \centering
    \caption{Наименование таблицы, очень длинное наименование таблицы, чтобы посмотреть как оно будет располагаться на~нескольких строках и~переноситься}%
    \label{tab:test2}% label всегда желательно идти после caption
    \renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
    \begin{SingleSpace}
        \begin{tabular}{@{}@{\extracolsep{20pt}}llll@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
            \toprule     %%% верхняя линейка
            Оконная функция & \({2N}\)& \({4N}\)& \({8N}\)\\
            \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
            Прямоугольное   & 8.72  & 8.77  & 8.77  \\
            Ханна           & 7.96  & 7.93  & 7.93  \\
            Хэмминга        & 8.72  & 8.77  & 8.77  \\
            Блэкмана        & 8.72  & 8.77  & 8.77  \\
            \bottomrule %%% нижняя линейка
        \end{tabular}%
    \end{SingleSpace}
\end{table}

\section{Таблица с многострочными ячейками и примечанием}

В таблице~\ref{tab:makecell} приведён пример использования команды
\verb+\multicolumn+ для объединения горизонтальных ячеек таблицы,
и команд пакета \textit{makecell} для добавления разрыва строки внутри ячеек.

\begin{table} [htbp]
	\centering
	\caption{Пример использования функций пакета \textit{makecell}.}%
	\label{tab:makecell}%
	\begin{tabular}{| c | c | c | c |}
	  \hline
	  Колонка 1                                    & Колонка 2        & \thead{Название колонки 3, \\ не помещающееся в одну строку} & Колонка 4 \\ \hline
	  \multicolumn{4}{|c|}{Выравнивание по центру}                                                                                               \\ \hline
	  \multicolumn{2}{|r|}{\makecell{Выравнивание к \\ правому краю}} & \multicolumn{2}{|l|}{Выравнивание к левому краю}                         \\ \hline
	  \makecell{В этой ячейке \\ много информации} & 8.72             & 8.55                                                         & 8.44      \\ \cline{3-4}
	  А в этой мало                                & 8.22             & \multicolumn{2}{|c|}{5}                                                  \\ \hline
	\end{tabular}%
\end{table}

Таблицы~\ref{tab:test3} и~\ref{tab:test4} "--- пример реализации расположения
примечания в~соответствии с ГОСТ 2.105. Каждый вариант со своими достоинствами
и~недостатками. Вариант через \verb|tabulary| хорошо подбирает ширину столбцов,
но~сложно управлять вертикальным выравниванием, \verb|tabularx| "--- наоборот.
\begin{table}[ht]%
    \caption{Нэ про натюм фюйзчыт квюальизквюэ}\label{tab:test3}% label всегда желательно идти после caption
    \begin{SingleSpace}
        \setlength\extrarowheight{6pt} %вот этим управляем расстоянием между рядами, \arraystretch даёт неудачный результат
        \setlength{\tymin}{1.9cm}% минимальная ширина столбца
        \begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
            \toprule     %%% верхняя линейка
            доминг лаборамюз эи ыам (Общий съём цен шляп (юфть)) & Шеф взъярён &
            адвыржаряюм &
            тебиквюэ элььэефэнд мэдиокретатым &
            Чэнзэрет мныжаркхюм	\\
            \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
            Эй, жлоб! Где туз? Прячь юных съёмщиц в~шкаф Плюш изъят. Бьём чуждый цен хвощ! &
            \({\approx}\) &
            \({\approx}\) &
            \({\approx}\) &
            \( + \) \\
            Эх, чужак! Общий съём цен &
            \( + \) &
            \( + \) &
            \( + \) &
            \( - \) \\
            Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку. Ад
            граэкйж плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат, ат шэа
            одео &
            \({\approx}\) &
            \( - \) &
            \( - \) &
            \( - \) \\
            Любя, съешь щипцы, "--- вздохнёт мэр, "--- кайф жгуч. &
            \( - \) &
            \( + \) &
            \( + \) &
            \({\approx}\) \\
            Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку. Ад
            граэкйж плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат, ат шэа
            одео квюаырэндум. Вёртюты ажжынтиор эффикеэнди эож нэ. &
            \( + \) &
            \( - \) &
            \({\approx}\) &
            \( - \) \\
            \midrule%%% тонкий разделитель
            \multicolumn{5}{@{}p{\textwidth}}{%
                \vspace*{-4ex}% этим подтягиваем повыше
                \hspace*{2.5em}% абзацный отступ - требование ГОСТ 2.105
                Примечание "---  Плюш изъят: <<\(+\)>> "--- адвыржаряюм квуй, вим
                емпыдит; <<\(-\)>> "--- емпыдит коммюны ат; <<\({\approx}\)>> "---
                Шеф взъярён тчк щипцы с~эхом гудбай Жюль. Эй, жлоб! Где туз?
                Прячь юных съёмщиц в~шкаф. Экс-граф?
            }
            \\
            \bottomrule %%% нижняя линейка
        \end{tabulary}%
    \end{SingleSpace}
\end{table}

Если таблица~\ref{tab:test3} не помещается на той же странице, всё
её~содержимое переносится на~следующую, ближайшую, а~этот текст идёт перед ней.
\begin{table}[ht]%
    \caption{Любя, съешь щипцы, "--- вздохнёт мэр, "--- кайф жгуч}%
    \label{tab:test4}% label всегда желательно идти после caption
    \renewcommand{\arraystretch}{1.6}%% Увеличение расстояния между рядами, для улучшения восприятия.
    \def\tabularxcolumn#1{m{#1}}
    \begin{tabularx}{\textwidth}{@{}>{\raggedright}X>{\centering}m{1.9cm} >{\centering}m{1.9cm} >{\centering}m{1.9cm} >{\centering\arraybackslash}m{1.9cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
        \toprule     %%% верхняя линейка
        доминг лаборамюз эи ыам (Общий съём цен шляп (юфть)) & Шеф взъярён &
        адвыр\-жаряюм &
        тебиквюэ элььэефэнд мэдиокретатым &
        Чэнзэрет мныжаркхюм	\\
        \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
        Эй, жлоб! Где туз? Прячь юных съёмщиц в~шкаф Плюш изъят.
        Бьём чуждый цен хвощ! &
        \({\approx}\) &
        \({\approx}\) &
        \({\approx}\) &
        \( + \) \\
        Эх, чужак! Общий съём цен &
        \( + \) &
        \( + \) &
        \( + \) &
        \( - \) \\
        Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку.
        Ад граэкйж плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат,
        ат шэа одео &
        \({\approx}\) &
        \( - \) &
        \( - \) &
        \( - \) \\
        Любя, съешь щипцы, "--- вздохнёт мэр, "--- кайф жгуч. &
        \( - \) &
        \( + \) &
        \( + \) &
        \({\approx}\) \\
        Нэ про натюм фюйзчыт квюальизквюэ, аэквюы жкаывола мэль ку. Ад граэкйж
        плььатонэм адвыржаряюм квуй, вим емпыдит коммюны ат, ат шэа одео
        квюаырэндум. Вёртюты ажжынтиор эффикеэнди эож нэ. &
        \( + \) &
        \( - \) &
        \({\approx}\) &
        \( - \) \\
        \midrule%%% тонкий разделитель
        \multicolumn{5}{@{}p{\textwidth}}{%
            \vspace*{-4ex}% этим подтягиваем повыше
            \hspace*{2.5em}% абзацный отступ - требование ГОСТ 2.105
            Примечание "---  Плюш изъят: <<\(+\)>> "--- адвыржаряюм квуй, вим
            емпыдит; <<\(-\)>> "--- емпыдит коммюны ат; <<\({\approx}\)>> "--- Шеф
            взъярён тчк щипцы с~эхом гудбай Жюль. Эй, жлоб! Где туз? Прячь юных
            съёмщиц в~шкаф. Экс-граф?
        }
        \\
        \bottomrule %%% нижняя линейка
    \end{tabularx}%
\end{table}

\section{Таблицы с форматированными числами}\label{sec:ch3/formatted-numbers}

В таблицах~(\labelcref{tab:S:parse,tab:S:align}) представлены примеры использования опции
форматирования чисел \texttt{S}, предоставляемой пакетом \texttt{siunitx}.

\begin{table}
    \centering
    \caption{Выравнивание столбцов.}\label{tab:S:parse}
    \begin{tabular}{SS[table-parse-only]}
        \toprule
        {Выравнивание по разделителю} & {Обычное выравнивание} \\
        \midrule
        12.345                        & 12.345                 \\
        6,78                          & 6,78                   \\
        -88.8(9)                      & -88.8(9)               \\
        4.5e3                         & 4.5e3                  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \caption{Выравнивание с использованием опции \texttt{S}.}\label{tab:S:align}
    \centering
    \sisetup{
        table-figures-integer = 2,
        table-figures-decimal = 4
    }
    \begin{tabular}
        {SS[table-number-alignment = center]S[table-number-alignment = left]S[table-number-alignment = right]}
        \toprule
        {Колонка 1} & {Колонка 2} & {Колонка 3} & {Колонка 4} \\
        \midrule
        2.3456      & 2.3456      & 2.3456      & 2.3456      \\
        34.2345     & 34.2345     & 34.2345     & 34.2345     \\
        56.7835     & 56.7835     & 56.7835     & 56.7835     \\
        90.473      & 90.473      & 90.473      & 90.473      \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Параграф "--- два}\label{sec:ch3/sect2}

Некоторый текст.

\section{Параграф с подпараграфами}\label{sec:ch3/sect3}

\subsection{Подпараграф "--- один}\label{subsec:ch3/sect3/sub1}

Некоторый текст.

\subsection{Подпараграф "--- два}\label{subsec:ch3/sect3/sub2}

Некоторый текст.

\clearpage
