\chapter*{Словарь терминов}             % Заголовок
%TOREMOVE cobot
%условно done - тело и базовая модель ( взаимозаменяемы)
%done датасет %pretraining finetuning pre-training fine-tuning процедура обучения модели или же тренировки  embedding dropout , понятие разбиения, кавычки  skill selector response selector singlelabel SENTIMENT TOXICITY EMOTION FACTOID RETRIEVAL SKILL - РАНЖИРУЮЩИЙ CONVERSATION SKILL - РАЗГОВОРНЫЙ TF-IDF RETRIEVAL, skill
\addcontentsline{toc}{chapter}{Словарь терминов} % Добавляем его в оглавление

\textbf{Open-source} : Находящийся в открытом доступе.

\textbf{Точность (precision)}: отношение числа верно классифицированных как принадлежащих данному классу примеров к общему числу примеров, классифицированных как принадлежащих данному классу. Считается для каждого класса.

\textbf{Чувствительность (recall)}: отношение числа верно классифицированных как принадлежащих данному классу примеров к общему числу примеров, принадлежащих данному классу. Считается для каждого класса.

\textbf{F1-метрика (F1)} : величина такая, что \begin{equation}
F1 = 2*Precision*Recall/(Precision+Recall)
\end{equation}

\textbf{Макро-F1 (Macro-averaged F1)} : Усредненная F1-метрика для каждого из классов.

\textbf{Взвешенный-F1 (Macro-F1 weighted)} : Взвешенная сумма F1-метрик для каждого из классов.

\textbf{Бенчмарк (Benchmark)} : Набор задач, используемый для оценки качества программного решения.

\textbf{Предобучение (pretraining)} : Процесс предварительного обучения модели, который
применяется перед обучением модели на целевом наборе данных (или задаче). 

\textbf{Многоязычная модель (Multilingual model)} : Модель, предобучение которой производилось на достаточно большом количестве языков.

\textbf{Дообучение (finetuning)} : Обучение модели на новом наборе данных (возможно, на другой задаче), после того, как она прошла процесс предобучения.

\textbf{Язык дообучения (finetuning language)} : Язык данных, на которых производится дообучение модели для обработки естественного языка (русский, английским и др.).

\textbf{Диалоговая платформа (Dialog platform)} : находящееся в открытом доступе программное решение, которое может использоваться для создания различных диалоговых систем.

\textbf{Диалоговая система (Dialog system)} : программный продукт, который может в автоматическом режиме поддерживать диалог с пользователем.

\textbf{Конкатенация (Concatentation)} : Операция объединения двух векторов (последовательностей) в один, в результате которой элементы одного из векторов добавляются в конец другого. Происходит от английского слова concatenate.

\textbf{Корреляция (Correlation)} : Статистическая взаимосвязь двух или более величин, которые можно с некой степенью точности считать случайной, такая, что изменениям одной(одних) из величин сопутствуют изменения другой(других) величин.

\textbf{Тональность (Sentiment)} : Эмоциональная окраска текста. Обычно выделяют позитивную, негативную или нейтральную.

\textbf{Токсичность (Toxicity)} : Вид негативной характеристики текста, обычно означает наличие в тексте нецензурных выражений, оскорблений, непристойностей, личностной ненависти и пр.

\textbf{Токен (Token)} : Текстовая единица, представляющая из себя слово целиком или N-грамму символов.

\textbf{Токенизация (Tokenization)} : Процесс разбиения текста на токены. Например, разбиение текста по пробелам и символам пунктуации.

\textbf{Языковая модель (Language model)} : Нейросетевая модель, обученная для решения задачи моделирования языка, то есть предсказания следующего слова/токена в тексте.

\textbf{Векторные представления слов/текстов} : Представление слов/текстов в виде векторов фиксированной длины с вещественными значениями.

\textbf{Векторизация слов/текстов} : Получение их векторных представлений. 

\textbf{Дистиллированная модель} - предобученная модель меньшего размера, чем оригинальная модель, в которую был осуществлен тем или иным способом перенос полученных при предобучении знаний с оригинальной, более сложной модели (т.е дистилляция этой модели).

\textbf{Аннотаторы (Annotators)} : Модели для обработки естественного языка, которые получает на вход текст реплики(возможно, с состоянием диалога), и возвращают некие автоматическим образом полученные характеристики для этой реплики (т.е их аннотации). 

\textbf{Выборщик навыков (Skill Selector)} : Компонента диалоговой платформы, формирующая список навыков, которые будут вызваны для генерации реплик-кандидатов.

\textbf{Выборщик ответа (Response Selector)} : Компонента диалоговой платформы, использующая состояние диалога, реплики-кандидаты и их аннотации для выбора финального ответа, возвращаемого пользователю.

\textbf{Многозадачное обучение (Multi-task learning)} : Обучение модели для решения различных задач одновременно (без модификации архитектуры/весов под каждую конкретную задачу на этапе предсказания).

\textbf{Сэмплирование примеров (Sampling)} : Определение вероятности, с которой примеры  для той или иной задачи будут показаны на каждом шагу при тренировке при многозадачном обучения. 

\textbf{Навык (Skill)} : Элемент диалоговой платформы, который может выдавать вариант следующей реплики для выбора в рамках этой платформы. 

\textbf{Разговорный навык (Conversation Skill)} : Модель, производящая по заданном контексту реплику-гипотезу, которая может являться продолжением диалога.

\textbf{Ранжирующий навык (Retrieval Skill)} : Модель, извлекающая реплику гипотезу по заданному контексту из заданного набора возможных реплик методом ранжирования.

\textbf{Ранжирующий навык TF-IDF (TF-IDF Retrieval Skill)} - Ранжирующий навык, использующий меток ранжирования, основанный на алгоритме TF-IDF~\cite{tfidf}. 

\textbf{Энкодер-агностичная модель (Encoder-agnostic model)} : Модель, куда можно подставить различные типы энкодеров, не  изменяя при этом ее архитектуру. В данной работе рассматриваются только такие модели с энкодерами на основе архитектуры Трансформер (Encoder-agnostic Transformer-based model).

\textbf{Базовая модель/тело многозадачной модели (Backbone)} : тип Трансформера, подставляемый в модель (в контексте данной работы - в энкодер-агностичную модель).

\textbf{Набор данных (Dataset)} : Данные для обучения и/или тестирования модели. Обычно делятся на тренировочную, валидационную и тестовую выборку. 

\textbf{Тренировочная выборка (Training sample)} : Данные, которые показываются модели при прохождении процедуры обучения модели. Для каждого из таких примеров модель обновляет свои параметры с целью приближения своих предсказаний к значениям этих примеров.

\textbf{Валидационная выборка (Validation sample)} : Данные, на которых проверяется качество модели после завершения каждой обучающей эпохи (т.е после того, как модель прошла процедуру обучения для определённого числа примеров, обычно соответствующего размеру тренировочной выборки). Исходя из качества на валидационной выборке, принимается решение о продолжении либо же прекращении обучения модели.

\textbf{Тестовая выборка (Testing sample)} : Данные, на которых проверяется качество модели после завершения ее обучения.

\textbf{Перенос знаний (Knowledge transfer)} : Использование знаний, полученных во время обучения на одной задаче (и/или языке) для обучения модели на другой задаче (и/или языке). Это более общее понятие, чем дообучение и предобучение.

\textbf{Функция потерь (Loss function)} : Функция, оптимизация которой осуществляется на этапе обучения нейронной сети. Цель оптимизации - приближенте предсказаний модели к обучающей выборке.

\textbf{Скорость обучения (Learning rate)} : Гиперпараметр, определяющий масштаб корректировки весов нейронной сети на каждой тренировочной итерации с учётом функции потерь.

\textbf{Размер батча (Batch size)} : Количество обучающих примеров за одну итерацию.

\textbf{Дропаут (Dropout)} : Метод регуляризации, позволяющий уменьшить переобучение модели за счет предупреждения коадаптаций нейронов на тренировочных данных в процессе обучения. В рамках данного метода на каждой обучающей итерации определенный случайно выбранный процент нейронов не задействуется (т.е их веса считаются нулевыми), что способствует лучшей работе оставшегося процента нейронов. 

\textbf{Интент (Intent)} : Намерение, выражаемое одной из сторон диалога.

\textbf{Многометочный (Multilabel)} : Имеющий более чем 1 метку (пример) или поддерживающий работу с примерами, имеющими более чем 1 метку (модель или верхний слой модели)

\textbf{Однометочный (Singlelabel)} : Имеющий более чем 1 метку (пример) или поддерживающий работу с примерами, имеющими более чем 1 метку (модель или верхний слой).

\textbf{Перплексия (Perplexity)} : Мера того, как хорошо модель, предсказывающая вероятность появления каждого токена на каждом шаге, предсказывает вероятность появления образца. Если в образце $w$ n токенов $w_1$,$w_2$...$w_n$, то эта вероятность $PPL(w)$ равняется
\begin{equation} 
PPL(w) = P(w_1w_2... w_n)^{(-1/n)}
\end{equation},
где $P(w_1w_2... w_n)$ - определенная моделью вероятность того, что токен $w_2$ будет следовать сразу после $w_1$, токен $w_3$ сразу после этих двух токенов и так далее до $w_n$. 

% TODO ENGLISH TRANSL