\chapter*{Словарь терминов}             % Заголовок
\addcontentsline{toc}{chapter}{Словарь терминов}  % Добавляем его в оглавление
\iffalse
\textbf{open-source} : находящийся в открытом доступе

\textbf{бенчмарк} : набор задач, используемый для оценки качества программного решения

TF-IDF
екторные представления слов/текстов (Embedding) : представ
ление слова/текста в виде вектора фиксированной длины с вещественными
значениями.
N-грамма : последовательность из N элементов (символов, слов).
Токен (Token) : текстовая единица, представляющая из себя слово цели
ком или N-грамму символов.
Языковая модель (Language Model) : нейросетевая модель, обученная
для решения задачи моделирования языка, то есть предсказания следующего
слова/токена в тексте.
Тональность (Sentiment) : эмоциональная окраска текста. Обычно выде
ляют позитивную, негативную, нейтральную.
Токсичность (Toxicity) : вид негативной характеристики текста, обычно
означает наличие в тексте нецензурных выражений, оскорблений, непристойно
стей, личностной ненависти и пр.
Разговорный навык (Conversational Skill) : модель, производящая по
заданном контексту реплику-гипотезу, которая может являться продолжени
ем диалога.
Здравый смысл (Commonsense) : совокупность взглядов и знаний,
используемых человеком в повседневной жизни, которые принимаются окру
жающими людьми по умолчанию.
Домен (Domen) : специфичность, область применимости или происхож
дения.
Ранжирующий навык (Retrieval Skill): модель, извлекающая реплику
гипотезу по заданному контексту из заданного набора возможных реплик
методом ранжирования.
Поиск по сетке (Grid Search) : алгоритм подбора гипер-параметров, осно
ванный на оценке качества модели для каждой комбинации гипер-параметров
и выборе лучшей комбинации.
Фрейм (Frame) : это некая структура знаний, представляющая инфор
мацию, которую система может извлечь из реплик пользователя, и состоит из
набора слотов, каждый из которых может принимать значения из заданного
набора.
120
Цельная диалоговая система (End-to-end dialogue System) : система,
состоящая из одной модели, которая получает на вход текст реплики и гене
рирует финальный ответ.
Модульная диалоговая система (Module-based Dialogue System) : си
стема, состоящая из нескольких компонент, которая получает на вход текст
реплики и генерирует финальный ответ.
Задаче-ориентированная диалоговая система (Task-oriented
Dialogue System): система, диалог с который ведет для выполнения некото
рой задачи.
Состояние диалога (Dialogue State) : это структурированная инфор
мация, содержащая в себе историю диалога, включая реплики-кандидаты,
аннотации всех реплик и реплик-кандидатов, а также специальные атрибуты
пользователя и системы.
Аннотаторы (Annotators) : набор моделей понимания естественного язы
ка, который получает на вход текст реплики и состояние диалога, обычно
включают в себя исправление опечаток, различные виды классификации текста
и токенов, извлечение сущностей, а также другие модели анализа текста.
Выборщик навыков (Skill Selector) : компонента, формирующая список
навыков, которые будут вызваны для генерации реплик-кандидатов.
Выборщик ответа (Response Selector) : компонента, использующая со
стояние диалога, реплики-кандидаты и их аннотации для выбора финального
ответа, возвращаемого пользователю.
Коэффициент Каппа Коэна (κ) : это статистика, которая используется
для измерения надежности между экспертами (а также надежности внутри
экспертов) при разметке категориальных признаков.
Фраза-подтверждение (Acknowledgement) : фраза, демонстрирующая
понимание того, что сказал пользователь.
Фраза-затравка (Prompt) : фраза, использующаяся для плавного пере
хода между навыками, темами, сценариями.
Yandex Toloka : краудсорсинговая платформа https://toloka.yandex.
ru.
Telegram : мессенджер https://telegram.org/.
сэмплирование
многоязычный
BERT
навык
трансформер-агностичный
Векторное представление слова : word embedding, представление сло
ва в виде вещественного вектора фиксированной длины
N-грамма : последовательность из �� элементов, в контексте данной ра
боты элементами могут быть слова, токены, символы
Униграмма : последовательность из одного элемента
Биграмма : последовательность из двух элемента
Токен : единица текста, может быть словом, символьной N-граммой
Токенизация : процесс разбиения текста на токены. Например, разбиение
текста по пробелам и символам пунктуации
Сабтокен : subtoken, subword unit, подслово, часть слова. В данной работе
понятие сабтокен используется для обозначения результата токенизации текста
с помощью алгоритма BPE
Языковая модель : вероятностная модель распределения следующего
слова в тексте по N предыдущим �� (���� +1|��1, ��2, ..., ���� ). Также, моделирует
вероятность встретить данную последовательность из N подряд идущих слов
�� (��1, ��2, . . . , ���� ). В данной работе под языковой моделью понимается более
общий класс моделей �� (����|����−���� , . . . , ����−1, ����+1, . . . , ����+���� ), где ���� и ���� — длины
левого и правого контекста для слова на позиции ��
Маскированная языковая модель : вероятностная модель рас
пределения слова в позиции �� по полному левому и правому контексту
�� (����|��1, . . . , ����−1, ����+1, . . . , ���� )
Словарь : набор слов (токенов, сабтокенов). Языковой моделью строится
распределение вероятностей элементов словаря.
Корпус : коллекция документов, текстов
Дообучение : fine-tuning, обучение модели на новом наборе данных (воз
можно на другой задаче), которая была ранее уже обучена на другом наборе
данных
Предобучение : процесс предварительного обучения модели, который
применяется перед обучением модели на целевом наборе данных (или задаче)
Перенос знаний : transfer learning, использование знаний, полученных во
время обучения на одной задаче (и/или домене) для обучения модели на другой
задаче (и/или домене). Более общее понятие, чем дообучение и предобучение.
100
Конкатенация : от англ. concatenate, операция объединения двух векто
ров (последовательностей) в один, в которой элементы вектора �� = [��1, . . . , ������ ]
добавляются в конец вектора �� = [��1, . . . , ������ ]:
[��, ��] = [��1, . . . , ������ , ��1, . . . , ������ ],
где ���� и ���� — длины векторов �� и 

%https://mipt.ru/upload/medialibrary/099/dissertatsiya-kuratov.pdf

n-грамма : Последовательность символов/слов из N элементов.
векторное представление слова/текста : Вектор фиксированной дли
ны, представляющий слово/текст.
токен : Слово либо N-грамма символов.
лемматизация : Процесс приведения словоформы к лемме, её нормаль
ной (словарной) форме.
стемминг : Процесс приведения словоформы к слове (основа не обяза
тельно совпадает со словарной формой).
словарь : Набор слово (токенов, сабтокенов).
корпус : Коллекция текстов.
домен : Область применимости или происхождения.
предобучение (Pretraining) : Предварительное обучение модели, приме
няемое до обучения модели на целевой задаче.
дообучение (Fine-tuning) : Обучение предобученной модели на целевой
задаче.
корреляция: Статистическая взаимосвязь двух или более случайных
величин (либо величин, которые можно с некоторой допустимой степенью точ
ности считать таковыми).
языковая модель (Language Model) : Модель обученная для предсказа
ния слова/токена в тексте.
состояние диалога (Dialogue State) : Информация, содержащая в себе
историю диалога, аннотации всех реплик.
learning rate: Коэффициент скорости обучения это гиперпараметр, опре
деляющий порядок как корректировать весы нейронной сети с учётом функции
потерь.
batch size : Количество обучающих примеров за одну итерацию.
dropout : Метод регуляризации, позволяющий уменьшить переобучение
модели за счет предупреждения коадаптаций нейронов на тренировочных дан
ных в процессе обучения.
zero-shot : Условия работы модели, когда на этапе тестирования модель
сталкивается с примерами классов, которые не содержатся в тренировочной
выборке.
105
few-shot: Условия работы модели, при котором предсказания модели ос
новываются на ограниченном числе тренировочных примеров.
слот : Значение обсуждаемого атрибута в диалоге.
интент : Намерение, выражаемые одной из сторон диалога
\fi