
Distilbert base cased and bert base cased. Lr 2e-5. Batch 32. Max seq length 28. 5 epochs, valid patience 3, lr drop patience 2, lr drop div 2

Accuracies and f1 weighted: 


train examples
singletask,distil (acc/ f1)
singletask
( acc/ f1)
multitask,distil 
( acc / f1)
multitask
 ( acc / f1)
factoid(valid)
3.6K
82.03/81.93
85.08/85.06
82.71/82.73
79.66/79.72
factoid(test)
3.6K
82.03/81.93
85.08/85.06
82.71/82.73
79.66/79.72
sentiment (valid)
8k


71.36/68.08
73.47/67.34
68.3/65.68
71.46/66.33
sentiment(test)
8k
73.6/70.56
75.91/71.58
68.94/67.53
73.55/69.28
emo(valid)
21k
84.62/84.35
85.14/84.92
83.64/83.21
82.85/82.69
emo(test)
21k
85.77/85.4
85.92/85.68
83.97/83.13
83.63/83.21
toxic(valid)
170k
94.61/93.38
94.62/93.53
94.01/93.37
94.41/93.65
toxic(test)
170k
94.48/93.28
94.33/93.27
93.77/93.11
94.05/93.36
midas(valid)
<10k
81.39/80.96
82.03/81.80
79.89/79.42
80.37/79.79
midas(test)
<10k
81.39/80.96
82.03/81.80
79.89/79.42
80.37/79.79
topics(valid)
1.8m
86.95/86.95
87.37/87.27(preliminary)
86.74/86.72
87.23/87.11
topics(test)
1.8m
87.16/87.16


86.94/86.92
87.45/87.31
average(valid)


83.49


82.55
82.66
average(test)


84.07


82.7
83.12


все считаем на uncased
Part 2. Distilbert base uncased. We used DynaSent ( part 1 and 2 combined) as sentiment dataset. All other datasets and hyperparams same as in part 1.


                                                         	 
Confusion_matrix [[  65	4	0	4	7	1  107]                                                                             	







train examples
singletask,distil (acc/ f1)
multitask,distil 
( acc / f1)
multitask,distil 
( acc / f1)
no midas
multitask,distil(acc,f1)
no midas and no sentiment
multitask
distil augmented,hard labels
 ( acc / f1)
PRELIMINARY
factoid(valid)
3.6K
80.68/80.54
78.98/79
77.29/77.36
80.0/79.77
81.36/81.14
factoid(test)
3.6K
80.68/80.54
78.78/79
78.31/78.31
80.0/79.8
81.36/81.14
sentiment (valid)
94k


69.93/69.59
70.31/65.2
67.53/66.54
-
44.73/43.77
sentiment(test)
94k
70.84/70.42
72.52/68.63
71.34/68.96
-
44.47/44.62
emo(valid)
21k
82.03/81.06
80.08/79.13
80.38/79.45
79.25/78.89
81.95/81.11
emo(test)
21k
82.01/80.95
80.77/79.43
80.2/78.74
79.98/78.7
82.16/80.95
toxic(valid)
170k
94.5/93.19
94.37/93.04
94.35/93.19
93.64/93.17
94.28/93.2
toxic(test)
170k
94.41/93.11
94.34/93.03
94.02/92.85
94.25/92.91
94.28/93.2
midas(valid)
<10k
82.09/81.04
78.18/77.61
-
-
80.16/79.9
midas(test)
<10k
82.09/81.04
78.18/77.61
-
-
80.16/79.9
topics(valid)
1.8m
84.76/84.66
84.6/84.49
84.52/84.48
84.27/84.16
83.73/83.67
topics(test)
1.8m
84.9/84.81
84.74/84.64
84.68/84.65
84.82/84.69
83.86/83.8
average(valid)
3.6K
82.29
81.08




77.7
average(test)
3.6K
82.48
81.59




77.7










Part 3. Bert base uncased, everything else - same as in part 2




train examples
singletask (acc/ f1)
multitask
( acc / f1)
factoid(valid)


81.02/81.03
79.32/79.37
factoid(test)


81.02/81.83
79.32/79.37
sentiment (valid)


73.56/69.36
71.46/69.34
sentiment(test)


75.62/72.34
73.32/71.85
emo(valid)


82.1/81.33
79.32/79.24
emo(test)


82.35/81.29
79.22/78.65
toxic(valid)


94.51/93.28
94.17/93.15
toxic(test)


94.25/93.01
94.05/92.98
midas(valid)


82.41/82.16
79.68/78.92
midas(test)


82.41/82.16
79.68/78.92
topics(valid)


85.15/85.07 wip
85.06/84.93
topics(test)


85.34/85.26 wip
85.3/85.16
average(valid)




81.5
average(test)




81.82


 конце октября было принято решение переработать коботовские датасеты следующим образом - 
1)Были исключены классы Phatic для задачи cobot_topics и Other для cobot_dialogact_topics, как самые частые и путаемые классы по confusion матрицам, полученных после обучения модели distilbert на singlelabel примерах соответствующих датасетов.
Примечание. Сами матрицы: import numpy as np
import matplotlib.pyplot as plt
cm=[[29825, 529, 161, 151, 183, 58, 57, 70, 41, 70, 57, 113, 47, 35, 70, 86, 48, 23, 38, 1, 0, 0], [885, 4813, 200, 137, 225, 85, 96, 126, 53, 92, 56, 157, 69, 45, 69, 125, 59, 37, 47, 2, 0, 1], [156, 119, 4705, 58, 58, 59, 35, 88, 82, 20, 23, 15, 19, 10, 11, 20, 11, 8, 5, 0, 2, 0], [136, 78, 53, 4743, 24, 30, 9, 42, 8, 16, 8, 12, 13, 2, 13, 6, 4, 14, 1, 1, 4, 1], [224, 161, 59, 40, 3028, 24, 47, 27, 37, 28, 33, 38, 12, 14, 13, 30, 10, 5, 19, 1, 1, 1], [65, 62, 57, 22, 28, 1605, 9, 64, 11, 8, 6, 8, 14, 3, 5, 11, 2, 2, 5, 0, 0, 1], [63, 62, 16, 5, 22, 7, 2342, 20, 3, 14, 7, 2, 6, 22, 10, 37, 1, 7, 6, 0, 0, 1], [45, 67, 77, 31, 17, 14, 16, 1990, 11, 5, 25, 5, 9, 5, 6, 14, 3, 6, 4, 0, 0, 0], [74, 53, 95, 17, 27, 22, 9, 6, 1542, 7, 16, 5, 6, 2, 4, 10, 4, 9, 4, 0, 1, 1], [75, 56, 26, 15, 26, 8, 4, 12, 6, 2505, 12, 9, 3, 2, 14, 5, 8, 5, 0, 1, 0, 0], [63, 77, 36, 19, 34, 6, 20, 43, 15, 8, 1429, 8, 3, 6, 7, 6, 8, 9, 7, 0, 0, 0], [227, 181, 35, 25, 59, 20, 8, 16, 7, 12, 6, 890, 37, 6, 7, 38, 11, 7, 3, 0, 0, 0], [100, 81, 33, 15, 23, 36, 9, 20, 7, 9, 4, 23, 1086, 1, 10, 17, 3, 2, 3, 0, 0, 1], [64, 42, 15, 6, 23, 4, 41, 3, 1, 2, 4, 7, 1, 1155, 5, 11, 0, 7, 4, 0, 0, 0], [93, 56, 20, 16, 18, 7, 17, 10, 7, 28, 5, 2, 6, 3, 1907, 3, 6, 2, 5, 0, 0, 1], [91, 90, 31, 20, 40, 12, 40, 21, 5, 7, 9, 20, 11, 6, 8, 755, 2, 3, 12, 1, 0, 0], [57, 52, 12, 6, 24, 5, 3, 9, 1, 7, 2, 5, 0, 1, 2, 2, 844, 3, 1, 0, 0, 0], [53, 28, 25, 26, 15, 7, 24, 10, 6, 6, 13, 2, 3, 5, 7, 12, 2, 392, 1, 1, 4, 0], [50, 25, 9, 5, 20, 4, 4, 0, 5, 3, 6, 4, 1, 6, 5, 8, 0, 1, 187, 0, 0, 0], [2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 221, 0, 0], [0, 0, 3, 7, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 144, 0], [1, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 97]]
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
disp = ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=combined_classes['cobot_topics'])
fig, ax = plt.subplots(figsize=(35,35))
disp.plot(ax=ax)


cm=[[52185, 395, 395, 204, 169, 5, 199, 402, 137, 38, 98], [661, 5054, 11, 2, 2, 0, 6, 17, 5, 2, 8], [496, 9, 6025, 27, 30, 0, 26, 20, 6, 7, 0], [249, 1, 13, 2454, 9, 0, 5, 5, 0, 2, 1], [230, 0, 38, 3, 1737, 0, 3, 8, 5, 3, 1], [9, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [239, 4, 35, 2, 5, 0, 2164, 1, 6, 1, 1], [642, 5, 23, 7, 8, 0, 6, 4785, 5, 5, 4], [185, 4, 7, 6, 7, 0, 2, 6, 1837, 5, 2], [44, 1, 12, 0, 3, 0, 4, 3, 5, 387, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
disp = ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=combined_classes['cobot_dialogact_topics'])
fig, ax = plt.subplots(figsize=(30,30))
disp.plot(ax=ax)


cm=[[24537, 2365, 716, 80, 1, 62, 45, 73, 19, 2], [2187, 29106, 428, 120, 1, 58, 116, 50, 10, 6], [768, 464, 9575, 51, 0, 11, 226, 72, 28, 8], [122, 175, 57, 1980, 0, 2, 23, 8, 2, 1], [3, 6, 2, 0, 0, 0, 0, 0, 0, 0], [83, 57, 7, 2, 0, 1057, 2, 2, 0, 11], [40, 124, 214, 14, 0, 5, 5048, 8, 7, 2], [87, 75, 68, 5, 0, 0, 10, 588, 3, 1], [17, 13, 40, 6, 0, 0, 19, 7, 188, 0], [6, 7, 5, 0, 0, 9, 2, 1, 0, 480]]
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
disp = ConfusionMatrixDisplay(confusion_matrix=np.array(cm), display_labels=combined_classes['cobot_dialogact_intents'])
fig, ax = plt.subplots(figsize=(30,30))
disp.plot(ax=ax)
)
Примечание 2. Была неудачная попытка убрать из cobot topics классы Phatic и Other, и использовать Other как класс-исключение, если вероятности всех классов слишком малы. Идея не сработала ( 0.729<0.806)
2)Все multi-label примеры были приведены к single-label формату. Для каждого примера ( где метка - Коботовское предсказание) выбирался самый редкий класс. Это было связано с тем, что во-первых, multilabel встречался в нескольких процентах случаев, а во-вторых, с тем, что даже в этих нескольких процентах он, как правило, был вида нормальны класс+мусорный класс, т.е по сути он учил модель оверфититься на мусорные классы other/phatic.
3)Для всех utterances, которые могли в имевшемся коботовском датасете(фраза+история из 3 реплик) быть классифицированы разным способом в зависимости от своей истории (что налюбдалось лишь для ~3% примеров) выбирался наиболее часто встречаемый класс после приведению к пункту 2. Примеры, где этот самый часто встречаемый класс относился к пункту 1, исключались.
4)Был переразбит датасет в соотношении 70/15/15 - 70 процентов тренировочных примеров, 15 тестовых, 15 валидационных.

5)Эмо датасет - юзались все классы.
Here are the dataset sizes of each class before and after preprocessing. For sizes before preprocessing, only the single-label examples were counted




Accuracy / f1 weighted for different tasks ( for factoid: accuracy/f1/rocauc); all singletask predicts from noncobot tasks were taken from the previous table/ Singletask batch size was 32 for all tasks in singletask mode, 640 while using distilbert and 320 while using ordinary BERT

Also, given a huge disparity of dataset sizes as well as the limited usability of Dilya datasets, we trained another multitask setting without it

Cobot singletask configs - cobot_da_topics_singlelabel2.json, cobot_da_intents_singlelabel2.json, cobot_topics_singlelabel2.json,
distil_cobot_da_topics_singlelabel2.json, distil_cobot_da_intents_singlelabel2.json, distil_cobot_topics_singlelabel2.json,

Также был поставлен на обучение сеттинг без задачи Topic classification, в связи с ее большими размерами и малой применимостью в DREAM
{"test": {"eval_examples_count": 194605, "metrics": {"multitask_accuracy": 0.7631, "accuracy_emo": 0.5358, "f1_weighted_emo": 0.6281, "accuracy_sentiment": 0.7403, "f1_weighted_sentiment": 0.7386, "accuracy_toxic": 0.9064, "f1_weighted_toxic": 0.6445, "accuracy_factoid": 0.8102, "f1_weighted_factoid": 0.8078, "roc_auc_factoid": 0.8001, "accuracy_midas": 0.7925, "f1_weighted_midas": 0.7897, "accuracy_topics": 0.7827, "f1_weighted_topics": 0.7834, "accuracy_da_topics": 0.7689, "f1_weighted_da_topics": 0.7682, "accuracy_da_intents": 0.768, "f1_weighted_da_intents": 0.7676}, "time_spent": "0:57:33"}}


Task / model
Train size
Singletask, distilbert base uncased,batch 32
Multitask, distilbert base uncased, 640 
Multitask, distilbert base uncased, without Dilya task, batch 640
Singletask, bert base uncased, batch 32
Multitask, bert base uncased,batch 320
Multitask, bert base uncased, without Dilya task, batch 320
Source results ( bert base uncased)
batch 32
Emotion classification (go_emotions)
multi-label
43к
61.65/68.87
46.71/56.66
50.86/59.52
61.77/68.49
51.13/61.29
53.58/62.81
/64
Toxic classification(Kaggle,multilabel)
170k
91.37/61.8
90.84/63.18
91.45/61.51
91.06/64.52
90.46/65.18
90.64/64.45
90.9/28.0
Sentiment classification(DynaBench, v1+v2)
94k
74.66/74.52
73.06/72.75
72.99/72.8
76.33/72.45
74.61/74.5
74.03/73.86
76.94/76.88?
Factoid classification(Yahoo)
3.6k
82.37/82.43
81.69/81.73/81.69
83.73/83.72/83.45
85.08/85.01
83.73/83.43/82.52
81.02/80.78/80.01
86.44/86.27
Midas classification(Midas, with history)
7.1k
82.09/81.84
74.55/73.78
75.99/75.37
82.25/81.89
76.84/76.16
79.25/78.97
?/79.28
Topics classification(Dilya)
1.8m
87.64/87.53
86.8/86.73


87.97/87.81
87.5/87.4


87.3/88.6
Cobot topics classification
216k
80.6/80.74
77.19/77.19
77.65/77.63
80.73/80.72
78.08/78
78.27/78.34
100/100(trained on cobot labels)
Cobot dialogact topics classification
127k
76.99/76.89
76.6/76.45
76.52/76.41
77.14/77.05
76.89/76.76
76.89/76.82
100/100(trained on cobot labels)
Cobot dialogact intents classification
318k
77.06/76.96
76.55/76.47
76.35/76.31
77.11/77.06
76.88/76.83
76.8/76.76
100/100(trained on cobot labels)
Total(9in1)


79.38/76.84
76.00/73.88


79.94/77.22
76.92/75.51




Total(8in1)


78.34/75.51
74.65/72.28
75.69/72.90
78.93/75.90
75.60/74.01
76.31


Memory used, Mb


2417*9=21753
2420
2420
3499*9=31491
3501
3501


Test inference time, sec




3082




5450  ~ 1.76*3082








Зеленым выделена модель, которая сейчас в Пулл-реквесте https://github.com/deeppavlov/dream/pull/207

Другой эксперимент. Наборы данных для классификации эмоций и токсичности приведены к сингл лебел формату. Для токсичности как в диссере. Для эмоций - удаляя все примеры с более чем 1 меткой, что сократило датасет до 39.5к




Task / model
can we report about this dataset in a paper?
dataset modification?
Train size
Singletask, distilbert base uncased,batch 32
Singletask, distilbert base uncased,batch 640
Multitask, distilbert base uncased, batch 640
Multitask distilbert base uncased, batch 640, concatenation 
CLS + SPEC TOKEN,
adding N_TASK of special tokens to input after CLS
Singletask, bert base uncased, batch 32
Singletask, bert base uncased, batch 320
Multitask, bert base uncased
batch 320
Emotion classification (go_emotions)




converted to multi-class
39.5к
70.43/70.22
70.47/70.30
68.18/67.86
67.81/67.4
70.67/70.51
71.48/71.16
67.27/67.23
Toxic classification(Kaggle


+non-toxic class
170k
94.53/93.39
94.53/93.64
93.84/93.5
93.73/93.46
94.23/93.31
94.54/93.15
93.94/93.4
Sentiment classification(DynaBench, v1+v2)


no
94k
73.06/72.75
74.75/74.63
72.55/72.21
73.44/73.19
74.61/74.5
75.95/75.88
75.65/75.62
Factoid classification(Yahoo)


no
3.6k
81.69/81.73/81.69
81.69/81.66
81.02/81.07/81.24
80.34/80.38/80.32
83.73/83.43/82.52
84.41/84.44
80.34/80.09
Midas classification(Midas, with history)


only semantic classes
excluded classes: 
7.1k
74.55/73.78
80.53/79.81
72.73/71.56
73.48/72.74
76.84/76.16
82.3/82.03


77.01/76.38
Topics classification(Dilya)


no
1.8m
86.8/86.73
87.48/87.43
86.98/86.9
87.16/87.05
87.5/87.4
88.09/88.1
87.43/87.47
Cobot topics classification
NO
converted to single label no history, removed 1 widespread garbage class Phatic
216k
77.19/77.19
79.88/79.9
77.31/77.36
77.45/77.43
78.08/78
80.68/80.67
78.21/78.22
Cobot dialogact topics classification
NO
converted to single label no history, removed 1 widespread garbage class Other
127k
76.6/76.45
76.81/76.71
76.92/76.79
76.97/76.85
76.89/76.76
77.02/76.97
76.86/76.74
Cobot dialogact intents classification
NO
converted to single label no history, classes functional_request from midas excluded
    : ["abandon", "nonsense","opening", "closing", "hold", "back-channeling","uncertain", "non_compliant", "correction"]
318k
76.55/76.47
77.07/77.7
76.83/76.76
76.71/76.65
76.88/76.83
77.28/77.72
76.96/76.89
Total(9in1)






79.04/78.75


78.48/78.22
78.57/
79.93/79.65


79.3/79.11
Memory used, Mb






2417*9=21753


2420


3499*9=31491


3501
Test inference time, sec










3082






5450  ~ 1.76*3082

Зеленым выделена модель, которая сейчас в пулл-реквесте https://github.com/deeppavlov/dream/pull/213


Добавление. Модель из ПР-207, инфер модели, обученной на эмо датасете, на тесте кобот топиков, распределение числа предсказанных лейблов
{'disgust': 46718, 'sadness': 46680, 'surprise': 46692, 'neutral': 34159, 'joy': 38153, 'anger': 5537, 'fear': 1}



То же для модели из ПР-213
{'fear': 952, 'joy': 10891, 'neutral': 25005, 'anger': 2572, 'surprise': 5691, 'sadness': 1326, 'disgust': 281}









Another option. Multilabel emo, toxic and topics


Task / model
Train size
Singletask, distilbert base uncased,batch 640
Multitask, distilbert base uncased, 640 
Emotion classification (go_emotions)
multi-label
43к
59.94/67.44
48.96/58.37
Toxic classification(Kaggle,multilabel)
170k
91.43/64.12
90.5/64.62
Sentiment classification(DynaBench, v1+v2)
94k
74.75/74.63
75.04/74.9
Factoid classification(Yahoo)
3.6k
81.69/81.66
83.05/83.04/82.76
Midas classification(Midas, with history)
7.1k
80.53/79.81
73.8/72.65
Topics classification(Dilya),multilabel despite having single labels
1.8m
85.63/87.04
79.41/83.47
Cobot topics classification
216k
79.88/79.9
78.44/78.33
Cobot dialogact topics classification
127k
76.81/76.71
77.1/76.95
Cobot dialogact intents classification
318k
77.07/77.7
77.02/76.94
Total(9in1)


78.64/76.55
75.92/74.36
Total(8in1)


77.76/75.25
75.48/73.22
Memory used, Mb


2417*9=21753


Test inference time, sec








Was not merged due to the poor results on the valid set. Explanation 
По просьбе Дили составил датасет из 30 примеров для тестирования моделей, обученных на ее датасете dp_topics. Примеры взяты частично из тестов DREAM, частично - составлены самостоятельно.  19 примеров были singlelabel, 11 multilabel.
На этом датасете я проинферил:
Single task модель из библиотеки DeepPavlov, обученную на Дилином датасете
 Multitask модель из текущего ПР, обученную на 9 задачах, включая Дилин датасет. При этом Дилин датасет считался singlelabel
Модель аналогичную 2), но при этом Дилин датасет при обучении считался multilabel.
Результат для singlelabel примеров.
Модель из DeepPavlov - accuracy 47.3% (9 раз правильно, 10 раз неправильно)
Multitask модель из текущего ПР - accuracy 63.1% (12 раз  правильно, 7 раз неправильно)
 3.  Multitask модель с Дилиным датасетом как multilabel -accuracy 36.8% (2 раза правильно, 6 раз частично правильно, 11 раз неправильно)
Результат для multilabel примеров.
Модель из DeepPavlov - accuracy 31.8% ( 7 раз частично правильно, 4 раза неправильно)
Multitask модель из текущего ПР - accuracy 40.9% (9 раз частично правильно, 2 раза неправильно)
Multitask модель с Дилиным датасетом как multilabel - accuracy 43.9%( 2 раза правильно, 6 раз частично правильно ,3 раза неправильно)
Примечание. Частично правильно - это когда верный класс определяется вместе с неверными, или из нескольких верных классов определяются не все.
Выводы.
Обучение модели с использованием Дилиного датасета как multilabel себя не оправдывает, так как это понижает качество модели на singlelabel примерах сильнее, чем увеличивает на multilabel. Особенно если участь, что singlelabel примеров в реальных диалогах гораздо больше.
Так как некоторые примеры взяты из тестов бота, мы не можем позволить себе на них ошибаться(а мы ошибаемся). Поэтому мы не можем заменить модели cobot topics и cobot dialogact topics ни одной из этих моделей.
Дополнительным препятствием для классификации ряда критически важных примеров является недостаточно широкая номенклатура классов. Тут про это написано подробнее.
Следствие из п.2-3 - Дилин датасет надо улучшать.
P.S. Моё мнение про то, как именно надо улучшать Дилин датасет. Если это диалоговый датасет, то и фразы должны быть короткими, как в диалогах; фразы из более чем 1 предложения должны иметь более чем 1 метку, если их вообще стоит оставлять в датасетах; набор классов надо также расширить, как обсуждали с Дилей)
P.P.S. Файл с примерами - в комментариях. (edited) 






In DREAM, new multitask “9 in 1” model was trained on the following datasets:
Sentiment classification - on DynaBench (94k samples). Note: in previous multitask models SST was used(8k samples) what led to the overfit. Head is single-label.
Factoid classification - on YAHOO dataset(3.6k samples) as before. Headl is single-label.
Emotion classification - on go_emotion dataset(42k samples). Head is single-label, as using multilabel head yielded worse results.(So we used only singlelabel samples.) Note - in previous multitask models custom dataset was used, which also led to overfitting
Midas classification - on Midas dataset(~9k samples). Head is singlelabel, only semantic classes were used as if in the DREAM now. Note - it is the first time we add this head to the multitask DREAM model!
Topic classification - on Dilya’s dataset (1.8m samples). Head is single-label, as using multilabel head proved to be inconsistent. Note - this model still is insufficient for passing tests, so we still need Cobot replacement classifiers. Also, class names for this classifier and for Cobot replacement classifiers are different, so special functions for every such topic were added to support this difference.
Toxic classification - on Kaggle dataset(170k samples). Note - to make the classifier single-label, non_toxic class was added to this dataset, as if in the previous multitask models.
Cobot topics - on the private DREAM-2 dataset, from which one the most frequent “garbage” class (Phatic) was excluded, and all multilabel examples were converted to the single-label format. All these measures made model less likely to overfit on the “garbage” classes, thus improving it’s quality on the real-world data. Cleaned dataset size: 216k samples
Cobot dialogact topics - on the private DREAM-2 dataset, from which one the most frequent “garbage” class (other) was excluded, all multilabel examples were converted to the single-label format, and history support was also removed. All these measures made model less likely to overfit on the “garbage” classes, thus improving it’s quality on the real-world data. Cleaned dataset size: 127k samples
Cobot dialogact intents - on the private DREAM-2 dataset, from which all multilabel examples were converted to the single-label format, and history support was also removed. All these measures made model less likely to overfit on the “garbage” classes, thus improving it’s quality on the real-world data. Cleaned dataset size: 318k samples
There also were tried the ideas of improving architecture: using task_specific tokens in concatenation with the CLS for classification, or instead of the CLS. Which was not successful. However, increasing of the batch size from 32 to 640(for distil model) or 320(for ordinary model) yielded an improvement. 
More detailed description of experiment, with raw data, can be found here.

In the setting of 25-12-2022, only midas classifier utilized history. Training the model without history from scratch, almost didn;t impact performance and paradoxically yielded some improvement for Midas(setting 3 VS setting 2). And it allowed to use 2x less max sequence length and have only 1 model prediction to cache, which decreased the prediction time from 0.73 sec to 0.55 sec/
We show the results below

Setting




1
2
3
4
5
Task / model
dataset modification?
Train size
Singletask, distilbert base uncased,batch 640
Multitask, distilbert base uncased, batch 640
Multitask, distilbert base uncased, batch 640, all tasks trained without history
Singletask, bert base uncased, batch 320
Multitask, bert base uncased
batch 320
Emotion classification (go_emotions)


converted to multi-class
39.5к
70.47/70.30
68.18/67.86
67.59/67.32
71.48/71.16
67.27/67.23
Toxic classification(Kaggle
+non-toxic class
1.62m
94.53/93.64
93.84/93.5
93.86/93.41
94.54/93.15
93.94/93.4
Sentiment classification(DynaBench, v1+v2)
no
94k
74.75/74.63
72.55/72.21
72.22/71.9
75.95/75.88
75.65/75.62
Factoid classification(Yahoo)
no
3.6k
81.69/81.66
81.02/81.07/81.24
80.0/79.86/79.25
84.41/84.44
80.34/80.09
Midas classification
only semantic classes
7.1k
80.53/79.81(with history)
72.73/71.56 (with history)
62.26 /60.68 (without history)
73.69/73.26(without history)
82.3/82.03(with history)


77.01/76.38(with history)
Topics classification(Dilya)
no
1.8m
87.48/87.43
86.98/86.9
87.01/87.05
88.09/88.1
87.43/87.47
Cobot topics classification
converted to single label no history, removed 1 widespread garbage class Phatic
216k
79.88/79.9
77.31/77.36
77.45/77.35
80.68/80.67
78.21/78.22
Cobot dialogact topics classification
converted to single label no history, removed 1 widespread garbage class Other
127k
76.81/76.71
76.92/76.79
76.8/76.7
77.02/76.97
76.86/76.74
Cobot dialogact intents classification
converted to single label no history
318k
77.07/77.7
76.83/76.76
76.65/76.57
77.28/77.72
76.96/76.89
Total(9in1)


to count
to count
78.48/78.22
78.36/78.15
to count
79.3/79.11
Memory used, Mb




2418*9=21762
2420
2420
3499*9=31491
3501
Test inference time, sec ( for the tests)






0.76
0.55


~ 1.33


For the sake of achieving the best trade-off between the memory use, inference time and test metrics, the model in green (Multitask, distilbert base uncased, batch 640) was used. It is now merged to dev as PR-213.


New multitask: GPU memory economy
If we treated absolutely all models as singletask, 6 models that are being replaced by the current combined classifier in DREAM(emo,toxic,sentiment and 3 cobot models) would have taken ~3500*6 ~ 21000 Mb of the GPU memory. Midas classifier would have taken ~3500 Mb of the GPU memory, as it takes in current dev.  We don’t count topic classifier model as it is unclear what kind of singletask topic classifiers we would have used. If we would have used distilbert-like topic classifier, it would have taken ~2418 Mb of the GPU memory. In this case, replacements of all singletask models would have taken ~27000 Mb of the GPU memory.  Compared to this setting, our Multitask gives ~91% GPU memory economy. 
This economy is caused by the replacement of many BERTs by one BERT, and also by the transformer-agnosticity that helped to quickly add distilbert-base-uncased instead of the BERT-base-uncased.
CPU memory use for the multitask model: 2909 Mb. If treating absolutely all models as singletask, the economy estimate is 2594*8 + 2594*0.6 = 22308 Mb. Compared to this seitting, our Multitask gives ~87% CPU memory economy.
Compared to the previous dev ( where multitask 6in1 bert-base is already used), our multitask gives ~75% GPU memory economy, ~57% CPU economy and ~80-85% postannotation inference time economy. (Inference time economy is due to the fact that current multitask is much faster than Midas thanks to the transformer-agnosticity, and we no longer need to use them both).



