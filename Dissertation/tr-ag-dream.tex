
Distilbert base cased and bert base cased. Lr 2e-5. Batch 32. Max seq length 28. 5 epochs, valid patience 3, lr drop patience 2, lr drop div 2

Accuracies and f1 weighted: 


train examples
singletask,distil (acc/ f1)
singletask
( acc/ f1)
multitask,distil 
( acc / f1)
multitask
 ( acc / f1)
factoid(valid)
3.6K
82.03/81.93
85.08/85.06
82.71/82.73
79.66/79.72
factoid(test)
3.6K
82.03/81.93
85.08/85.06
82.71/82.73
79.66/79.72
sentiment (valid)
8k


71.36/68.08
73.47/67.34
68.3/65.68
71.46/66.33
sentiment(test)
8k
73.6/70.56
75.91/71.58
68.94/67.53
73.55/69.28
emo(valid)
21k
84.62/84.35
85.14/84.92
83.64/83.21
82.85/82.69
emo(test)
21k
85.77/85.4
85.92/85.68
83.97/83.13
83.63/83.21
toxic(valid)
170k
94.61/93.38
94.62/93.53
94.01/93.37
94.41/93.65
toxic(test)
170k
94.48/93.28
94.33/93.27
93.77/93.11
94.05/93.36
midas(valid)
<10k
81.39/80.96
82.03/81.80
79.89/79.42
80.37/79.79
midas(test)
<10k
81.39/80.96
82.03/81.80
79.89/79.42
80.37/79.79
topics(valid)
1.8m
86.95/86.95
87.37/87.27(preliminary)
86.74/86.72
87.23/87.11
topics(test)
1.8m
87.16/87.16


86.94/86.92
87.45/87.31
average(valid)


83.49


82.55
82.66
average(test)


84.07


82.7
83.12


все считаем на uncased
Part 2. Distilbert base uncased. We used DynaSent ( part 1 and 2 combined) as sentiment dataset. All other datasets and hyperparams same as in part 1.


                                                         	 
Confusion_matrix [[  65	4	0	4	7	1  107]                                                                             	







train examples
singletask,distil (acc/ f1)
multitask,distil 
( acc / f1)
multitask,distil 
( acc / f1)
no midas
multitask,distil(acc,f1)
no midas and no sentiment
multitask
distil augmented,hard labels
 ( acc / f1)
PRELIMINARY
factoid(valid)
3.6K
80.68/80.54
78.98/79
77.29/77.36
80.0/79.77
81.36/81.14
factoid(test)
3.6K
80.68/80.54
78.78/79
78.31/78.31
80.0/79.8
81.36/81.14
sentiment (valid)
94k


69.93/69.59
70.31/65.2
67.53/66.54
-
44.73/43.77
sentiment(test)
94k
70.84/70.42
72.52/68.63
71.34/68.96
-
44.47/44.62
emo(valid)
21k
82.03/81.06
80.08/79.13
80.38/79.45
79.25/78.89
81.95/81.11
emo(test)
21k
82.01/80.95
80.77/79.43
80.2/78.74
79.98/78.7
82.16/80.95
toxic(valid)
170k
94.5/93.19
94.37/93.04
94.35/93.19
93.64/93.17
94.28/93.2
toxic(test)
170k
94.41/93.11
94.34/93.03
94.02/92.85
94.25/92.91
94.28/93.2
midas(valid)
<10k
82.09/81.04
78.18/77.61
-
-
80.16/79.9
midas(test)
<10k
82.09/81.04
78.18/77.61
-
-
80.16/79.9
topics(valid)
1.8m
84.76/84.66
84.6/84.49
84.52/84.48
84.27/84.16
83.73/83.67
topics(test)
1.8m
84.9/84.81
84.74/84.64
84.68/84.65
84.82/84.69
83.86/83.8
average(valid)
3.6K
82.29
81.08




77.7
average(test)
3.6K
82.48
81.59




77.7










Part 3. Bert base uncased, everything else - same as in part 2




train examples
singletask (acc/ f1)
multitask
( acc / f1)
factoid(valid)


81.02/81.03
79.32/79.37
factoid(test)


81.02/81.83
79.32/79.37
sentiment (valid)


73.56/69.36
71.46/69.34
sentiment(test)


75.62/72.34
73.32/71.85
emo(valid)


82.1/81.33
79.32/79.24
emo(test)


82.35/81.29
79.22/78.65
toxic(valid)


94.51/93.28
94.17/93.15
toxic(test)


94.25/93.01
94.05/92.98
midas(valid)


82.41/82.16
79.68/78.92
midas(test)


82.41/82.16
79.68/78.92
topics(valid)


85.15/85.07 wip
85.06/84.93
topics(test)


85.34/85.26 wip
85.3/85.16
average(valid)




81.5
average(test)




81.82







In DREAM, new multitask “9 in 1” model was trained on the following datasets:
Sentiment classification - on DynaBench (94k samples). Note: in previous multitask models SST was used(8k samples) what led to the overfit. Head is single-label.
Factoid classification - on YAHOO dataset(3.6k samples) as before. Headl is single-label.
Emotion classification - on go_emotion dataset(42k samples). Head is single-label, as using multilabel head yielded worse results.(So we used only singlelabel samples.) Note - in previous multitask models custom dataset was used, which also led to overfitting
Midas classification - on Midas dataset(~9k samples). Head is singlelabel, only semantic classes were used as if in the DREAM now. Note - it is the first time we add this head to the multitask DREAM model!
Topic classification - on Dilya’s dataset (1.8m samples). Head is single-label, as using multilabel head proved to be inconsistent. Note - this model still is insufficient for passing tests, so we still need Cobot replacement classifiers. Also, class names for this classifier and for Cobot replacement classifiers are different, so special functions for every such topic were added to support this difference.
Toxic classification - on Kaggle dataset(170k samples). Note - to make the classifier single-label, non_toxic class was added to this dataset, as if in the previous multitask models.
Cobot topics - on the private DREAM-2 dataset, from which one the most frequent “garbage” class (Phatic) was excluded, and all multilabel examples were converted to the single-label format. All these measures made model less likely to overfit on the “garbage” classes, thus improving it’s quality on the real-world data. Cleaned dataset size: 216k samples
Cobot dialogact topics - on the private DREAM-2 dataset, from which one the most frequent “garbage” class (other) was excluded, all multilabel examples were converted to the single-label format, and history support was also removed. All these measures made model less likely to overfit on the “garbage” classes, thus improving it’s quality on the real-world data. Cleaned dataset size: 127k samples
Cobot dialogact intents - on the private DREAM-2 dataset, from which all multilabel examples were converted to the single-label format, and history support was also removed. All these measures made model less likely to overfit on the “garbage” classes, thus improving it’s quality on the real-world data. Cleaned dataset size: 318k samples
There also were tried the ideas of improving architecture: using task_specific tokens in concatenation with the CLS for classification, or instead of the CLS. Which was not successful. However, increasing of the batch size from 32 to 640(for distil model) or 320(for ordinary model) yielded an improvement. 
More detailed description of experiment, with raw data, can be found here.

In the setting of 25-12-2022, only midas classifier utilized history. Training the model without history from scratch, almost didn;t impact performance and paradoxically yielded some improvement for Midas(setting 3 VS setting 2). And it allowed to use 2x less max sequence length and have only 1 model prediction to cache, which decreased the prediction time from 0.73 sec to 0.55 sec/
We show the results below

Setting




1
2
3
4
5
Task / model
dataset modification?
Train size
Singletask, distilbert base uncased,batch 640
Multitask, distilbert base uncased, batch 640
Multitask, distilbert base uncased, batch 640, all tasks trained without history
Singletask, bert base uncased, batch 320
Multitask, bert base uncased
batch 320
Emotion classification (go_emotions)


converted to multi-class
39.5к
70.47/70.30
68.18/67.86
67.59/67.32
71.48/71.16
67.27/67.23
Toxic classification(Kaggle
+non-toxic class
1.62m
94.53/93.64
93.84/93.5
93.86/93.41
94.54/93.15
93.94/93.4
Sentiment classification(DynaBench, v1+v2)
no
94k
74.75/74.63
72.55/72.21
72.22/71.9
75.95/75.88
75.65/75.62
Factoid classification(Yahoo)
no
3.6k
81.69/81.66
81.02/81.07/81.24
80.0/79.86/79.25
84.41/84.44
80.34/80.09
Midas classification
only semantic classes
7.1k
80.53/79.81(with history)
72.73/71.56 (with history)
62.26 /60.68 (without history)
73.69/73.26(without history)
82.3/82.03(with history)


77.01/76.38(with history)
Topics classification(Dilya)
no
1.8m
87.48/87.43
86.98/86.9
87.01/87.05
88.09/88.1
87.43/87.47
Cobot topics classification
converted to single label no history, removed 1 widespread garbage class Phatic
216k
79.88/79.9
77.31/77.36
77.45/77.35
80.68/80.67
78.21/78.22
Cobot dialogact topics classification
converted to single label no history, removed 1 widespread garbage class Other
127k
76.81/76.71
76.92/76.79
76.8/76.7
77.02/76.97
76.86/76.74
Cobot dialogact intents classification
converted to single label no history
318k
77.07/77.7
76.83/76.76
76.65/76.57
77.28/77.72
76.96/76.89
Total(9in1)


to count
to count
78.48/78.22
78.36/78.15
to count
79.3/79.11
Memory used, Mb




2418*9=21762
2420
2420
3499*9=31491
3501
Test inference time, sec ( for the tests)






0.76
0.55


~ 1.33


For the sake of achieving the best trade-off between the memory use, inference time and test metrics, the model in green (Multitask, distilbert base uncased, batch 640) was used. It is now merged to dev as PR-213.


New multitask: GPU memory economy
If we treated absolutely all models as singletask, 6 models that are being replaced by the current combined classifier in DREAM(emo,toxic,sentiment and 3 cobot models) would have taken ~3500*6 ~ 21000 Mb of the GPU memory. Midas classifier would have taken ~3500 Mb of the GPU memory, as it takes in current dev.  We don’t count topic classifier model as it is unclear what kind of singletask topic classifiers we would have used. If we would have used distilbert-like topic classifier, it would have taken ~2418 Mb of the GPU memory. In this case, replacements of all singletask models would have taken ~27000 Mb of the GPU memory.  Compared to this setting, our Multitask gives ~91% GPU memory economy. 
This economy is caused by the replacement of many BERTs by one BERT, and also by the transformer-agnosticity that helped to quickly add distilbert-base-uncased instead of the BERT-base-uncased.
CPU memory use for the multitask model: 2909 Mb. If treating absolutely all models as singletask, the economy estimate is 2594*8 + 2594*0.6 = 22308 Mb. Compared to this seitting, our Multitask gives ~87% CPU memory economy.
Compared to the previous dev ( where multitask 6in1 bert-base is already used), our multitask gives ~75% GPU memory economy, ~57% CPU economy and ~80-85% postannotation inference time economy. (Inference time economy is due to the fact that current multitask is much faster than Midas thanks to the transformer-agnosticity, and we no longer need to use them both).



