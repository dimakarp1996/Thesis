\section*{Общая характеристика работы}

\newcommand{\actuality}{\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\underline{\textbf{\tasksTXT}}}
\newcommand{\novelty}{\underline{\textbf{\noveltyTXT}}}
\newcommand{\appropriation}{\underline{\textbf{\appropriationTXT}}}
\newcommand{\influence}{\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов \dots

%\underline{\textbf{Объем и структура работы.}} Диссертация состоит из~введения,
%четырех глав, заключения и~приложения. Полный объем диссертации
%\textbf{ХХХ}~страниц текста с~\textbf{ХХ}~рисунками и~5~таблицами. Список
%литературы содержит \textbf{ХХX}~наименование.
\section*{Содержание работы}
Во \underline{\textbf{введении}} обосновывается актуальность
исследований, проводимых в~рамках данной диссертационной работы,
приводится обзор научной литературы по изучаемой проблеме,
формулируется цель, ставятся задачи работы, излагается научная новизна
и практическая значимость представляемой работы. 

\underline{\textbf{Первая глава}} является обзорной. В этой главе дается определение нейросетевых методов машинного обучения для задач обработки естественного языка. 
Дается представление о различных базовых понятиях, таких, как метод обратного распространения ошибки, полносвязная архитектура и токенизация. 

Нейросетевые методы машинного обучения - это методы, основанные на использовании искусственных нейронных сетей. В данной работе рассматривается класс искусственных нейронных сетей, представляющих собой совокупность слоев с функциями активации, таких, что подаваемые на вход данные проходят через различные слои по очереди, где каждый слой представляет собой многомерную функцию многих переменных. Итоговый выход нейронной сети подается в функцию потерь, после чего функция потерь оптимизируется методом обратного распространения ошибки. 

Метод обратного распространения ошибки - один из методов «обучения с учителем», то есть подход, при котором модель учится решать задачу, чтобы соответствовать набору примеров входных/выходных данных. Для определения того, насколько ответ, данный нейронной сетью, соответствует требуемому, вводится функция потерь. Далее выполняется поиск точки минимума функции потерь в пространстве параметров искусственной нейронной сети для данного набора примеров входных данных. 

Все самые главные достижения в области нейронных сетей в 21 веке были связаны именно с применением нейросетевых подходов. 

Одним из классических видов нейронных сетей являются полносвязные нейронные сети - сети, состоящие из полносвязных слоев. Будем называть полносвязным слоем с M нейронами взвешенную сумму значений входного вектора x размерности N, к каждому элементу которой затем применяется функция активации $\sigma(y)$:

\begin{equation}
\begin{split} 
\color{black}z=\sigma(y)\\
\color{black}y=W_{0}+W_{1}X
\end{split}
\label{nn:0}
\end{equation}
где $W_{1}$ - матрица весов (weights) полносвязного слоя размерности $N*M$, $W_{0}$ - матрица биасов (bias) полносвязного слоя размерности M, $\sigma$ - некая нелинейная функция активации.

где $i$ - индекс $K$-мерного вектора $z$.
Для регуляризации в таких слоях (как и в других, более сложных) применяется также дропаут, предложенный в~\cite{dropout}. При использовании данного метода некий процент элементов выходного вектора (как правило, 10-20\%) приравнивается к нулю. Такая техника мешает «переобучению» нейронной сети, улучшая тем самым ее обобщающую способность.

Токенизация текста - это его разбиение на элементарные единицы(токены) перед предобработкой. Один токен соответствует одному слову либо же одной более мелкой его единице(букве или слогу) в зависимости от метода токенизации. Для нижеописанного метода Word2Vec токеном является 1 слово, для нижеописанной архитектуры BERT - слово либо слог.

Помимо этого, в первой главе разбираются нейросетевые архитектуры, использовавшиеся в данной диссертационной работе. В частности, разбирается архитектура Трансформер и нейросетевая модель {BERT}, основанная на данной архитектуре.

Последний раздел главы посвящен многозадачным нейросетевым моделям. В нём даётся определение многозадачного обучения и приводится классификация многозадачных нейросетевых архитектур.

Многозадачное обучение - это метод разделения параметров между моделями, обучающимися выполнять несколько задач. Все имеющиеся многозадачные нейросетевые архитектуры можно разделить на четыре типа:
\begin{itemize}
\item[*] Параллельные архитектуры. Для данного типа архитектур одни и те же «общие» слои используются для примеров из каждой задачи, при этом выход «общих» слоев обрабатывается независимо своим специфическим слоем для каждой задачи. 
\item[*] Иерархические архитектуры. Для данного типа архитектур задачи обрабатываются зависимо друг от друга: так, результат классификации примера для одной из задач может использоваться при решении другой из задач как дополнительный входной параметр.
\item[*] Модульные архитектуры. Нейронная сеть в данных архитектурах делится на общие модули и задаче-специфичные модули, где общие модули имеют одни и те же веса для всех задач, а задаче-специфичные модули - свои веса для каждой из задач.
\item[*] Генеративно-состязательные архитектуры. Для данного типа архитектур генератор и дискриминатор обучаются совместно таким образом, что дискриминатор пытается предсказать, из какой задачи пример, по его выдаваемому генератором представлению. А генератор, соответственно, пытается сгенерировать такое представление, чтобы дискриминатор мог предсказать задачу как можно хуже. 
\end{itemize}
В завершении первого раздела приводится подробный обзор двух архитектур, на которых основывались последующие разделы данной диссертационной работы. А именно, модель {MT-DNN}, имеющая параллельную архитектуру, и модель {PAL-BERT}, имеющая модульную архитектуру. 

\underline{\textbf{Вторая глава}} посвящена обзору диалоговой платформы DREAM. В главе подробно рассмотрена структура этой диалоговой платформы, ее эволюция в течение конкурсов «Alexa Prize Socialbot Grand Challenge 3» и «Alexa Prize Socialbot Grand Challenge 4», в полуфинал которых вышла платформа.  Проводимые автором работы над этой платформой частично продолжали проводившуюся ранее автором работу над диалоговой моделью «с персоной»~\cite{Болотин_Карпов_Рашков_Шкурак_2019}.

%Именно потребности этой платформы стимулировали создание многозадачных нейросетевых моделей, описанных в данной диссертационной работе, в этой платформе они и получали свое прикладное применение. 
В главе показано, что диалоговая платформа DREAM пригодна для изучения прикладного применения многозадачных нейросетевых моделей.

\underline{\textbf{Третья глава}} посвящена многозадачным нейросетевым моделям с одним линейным слоем - простейшему типу многозадачных моделей. Такие модели имеют архитектуру, аналогичную оригинальной модели BERT, при этом в финальном линейном слое разные нейроны отвечают за разные задачи.

В данной главе проводились эксперименты на следующих задачах из набора данных GLUE:
\begin{enumerate}
    \item[*] QQP - задача определения того, является ли большой при
\end{enumerate}


Для четырех задач из набора задач GLUE(MNLI, QQP, SST-2, RTE) сравниваются различные способы псевдоразметки данных при обучении многозадачной модели с одним линейным слоем.

\begin{table}[htbp]
\centering
\caption {Лучшая точность на тестовых данных (при лучшей скорости обучения из выбираемых, среднее по 3 запускам)}
\label{tab:ps2}% label всегда желательно идти после caption
\resizebox{\textwidth}{!}
{%
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
Название эксперимента &Среднее& \({RTE}\)& \({QQP}\)&\({MNLI-m}\)&\({MNLI-mm}\)&\({SST}\)\\
\hline
\hline
Базовый(из оригинальной статьи)& 78.8\% & 66.4\%  & 71.2\% & 84.6\% & 83.4\% & 93.5\% \\
\hline
Базовый(воспроизведенный)& 77.6\% & 62.7\% & 71.0\% & 83.1\% & 82.7\% & 93.5\% \\
\hline
Независимые метки &79.0\% & 71.5\% & 70.9\% & 82.7\% & 81.7\% & 91.3\% \\
\hline
Мягкие независимые метки  &78.9\% & 69.3\% & 71.3\% & 82.8\% & 82.1\% & 92.6\% \\
\hline
Дополненные независимые метки &77.6\% & 64.2\%&\textbf{ 71.8\%} & 81.2\% & 80.7\% & \textbf{93.2\%} \\
\hline
Мягкое вероятностное предположение  &\textbf{79.7\%} & \textbf{72.7\%} & 70.7\% &\textbf{ 83.4\%} &\textbf{82.3\%} & 92.5\% \\
\hline
Мягкие предсказанные метки  & 78.8\% & 70.3\% & 70.7\% & 81.7\% & 81.7\% & 92.5\% \\
\hline
Жесткие предсказанные метки & 79.1\% & 71.3\% &71.1\% & 81.7\% & 81.4\% & 92.6\% \\
\hline
\begin{tabular}[c]{@{}l@{}}Независимые метки,\\замороженная голова\end{tabular}   & 78.2\% & 66.9\%& \textbf{71.8\%} & 82.6\% & 81.8\% & 91.9\% \\
\hline
\begin{tabular}[c]{@{}l@{}}Мягкие независимые метки,\\замороженная голова\end{tabular}  &79.1\% & 70.0\% & 71.5\% & 83.0\% &\textbf{ 82.3\%} & 92.4\% \\
\hline
\end{tabular}
}
\end{table}



Делается вывод, что если задачи достаточно сильно отличаются друг от друга, то самый лучший способ обучения таких моделей - получение вероятностей для каждого примера, не имеющего метки для той или иной задачи, по предсказаниям модели, учившейся только для этой задачи. Именно этот метод подробнее описан в \textbf{четвертой главе}.

\underline{\textbf{Четвёртая глава}} посвящена трансформер-агностичным многозадачным моделям. В частности, подробно описывается архитектура трансформер-агностичной многозадачной модели. 
Модель позволяет более гибко подстраиваться под каждую задачу. В отличие от модели, описанной в предыдущей главе, она не требует параллельной разметки. В главе подробно раскрыты преимущества данной модели над многозадачной моделью с одним линейным слоем.

Отдельный раздел содержит описание экспериментов, которые не сработали - модификация [CLS]-выхода модели BERT, использование задаче-специфичных тренируемых токенов и методов дистилляции при помощи приближения весов, а также эксперименты с разными методами сэмплирования. 

Исследование пригодности трансформер-агностичных многозадачных нейросетевых моделей для решения диалоговых задач проводилось для пяти задач - классификация эмоций, тональности, токсичности, интентов и тематическая классификация. Для этих задач использовалилсь параллельные русскоязычные и англоязычные наборы данных - CEDR и go\_emotions для классификации эмоций, RuReviews и DynaSent для классификации тональности, RuToxic и Wiki Talk для классификации токсичности и MASSIVE для классификации интентов и тематической классификации.

В главе показано на данных наборах данных, что как для русского, так и для английского языка предложенные многозадачные модели либо незначительно хуже однозадачных, либо не хуже. Выводы также проверены на наборе данных GLUE. Показано также, что при уменьшении обучающей выборки многозадачные модели с какого-то достаточно маленького размера данных, порядка 200-2000 примеров на задачу, начинают превосходить однозадачные модели. В особенности за счет не самых больших наборов данных из выборки. Помимо этого, показано, что добавление англоязычных данных к русскоязычным эффективнее делать для многоязычных моделей, объединяя данные для каждой задачи, а не считая каждые такие данные отдельный задачей, при условии соответствия номенклатуры классов. И что само это добавление улучшает качество многоязычной модели на русскоязычных данных - от 1 до 5 процентов, чем изначально русскоязычных данных меньше, тем улучшение сильнее.

Результаты данной главы представлены в работах автора~\cite{rumtl,enmtl}.

\underline{\textbf{Пятая глава}} расширяет работу, проделанную в \textbf{четвертой главе}. В данной главе проводятся исследования русскоязычного тематического набора данных - YAQTopics.  Данный набор состоит из 76 тематических классов и имеет более 500 тысяч примеров - пар «вопрос-ответ» из сервиса «Яндекс.Кью»~\cite{yandex_q}. Этот набор данных существенно превосходит другие существовавшие до него русскоязычные наборы данных для разговорной тематической классификации - как по числу примеров, так и по числу тематических классов. В главе использовалась однометочная часть этого набора. 

\begin{table}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{|c||c|c|c|c|c|} 
\textbf{тип данных}  & \multicolumn{2}{c|}{\textbf{однометочные}} & \multicolumn{2}{c|}{\textbf{многометочные}} & \multirow{2}{*}{\textbf{равноразмерные}}\\
\cline{1-5}
\textbf{класс}  & \multicolumn{1}{c|}{все} & \multicolumn{1}{c|}{отвеченные} & \multicolumn{1}{c|}{все} & \multicolumn{1}{c|}{отвеченные} & \\\hline \hline
\input{Dissertation/rutopics_sizes_data.tex}
\end{tabular}
}
\caption{Размеры набора данных {YAQTopics} по классу и части}
%\centering
\label{tab:rutopics:sizes}
\end{table}

Для оценки качества данного набора данных использовался набор данных MASSIVE. Валидация проводилась на совпадающих 6 классах из валидационной части русскоязычного набора данных MASSIVE, а тестирование - на объединении тренировочных и тестовых частей этого набора. Все эксперименты проводились на 4 моделях - русскоязычный BERT от DeepPavlov, дистиллированный BERT от DeepPavlov, русскоязычный BERT от Сбербанка и многоязычный BERT от авторов оригинальной модели.

Показано при оценке на наборе данных MASSIVE~\cite{massive}, что данный набор данных хорошо подходит для тематической классификации(точность на вопросах из 6 классов {YAQTopics} 85 процентов для русскоязычных моделей на отвеченных однометочных данных). 

При этом самой информативной частью данного набора, повзоляющей определить тему, являются вопросы, так как конкатенация к ним ответов не давала стойких улучшений, использование же ответов вместо вопросов лишь ухудшало показатели. Данный вывод справедлив для всех базовых моделей. Использование суммаризованных ответов вместо ответов не изменяет этот вывод.

Было также показано на 5-кратной кроссвалидации, что все рассмотренные выше модели показывают точность выше 70 процентов даже на всём 76-классовом наборе однометочных данных из YAQTopics.

Также на примере обучения многоязычной модели \textit{bert-base-multilingual-cased} на данном наборе данных (см. Таблицу~\ref{tab:rutopics:crosslingual} можно сделать вывод, что при переносе знаний с русскоязычного набора данных YAQTopics на другие языки (51 язык) качество модели для каждого языка хорошо коррелирует с приближенным размером предобучающей выборки для этого языка ( корреляция Спирмена 0.773 с пи-значением 2.997e-11). При этом корреляция качества модели для каждого языка с генеалогической близостью этого языка к русскому не является статистически значимой.
\begin{table*}
\caption{Точность (f1) модели \textit{bert-base-multilingual-cased} на объединенном тестовом наборе данных {MASSIVE} для всех языков. Модель обучалась на версии \textbf{Q} набора данных {YAQTopics}. \textbf{Код} означает код языка(ISO 639-1), \textbf{N} означает число статей в Википедии на этом языке на 11 октября 2018 года, \textbf{Дистанция} означает лингвистическую дистанцию между этим языком и русским, посчитанную в соответствии с работой~\cite{lang_sim}. Усреднено по трем запускам.}
\label{tab:rutopics:crosslingual}
\centering
   \scalebox{0.5}{
\begin{tabular}{|c|c|c||c|c|c|} \hline
\multirow{2}{*}{\textbf{Язык}}  & \multirow{2}{*}{\textbf{Код}} & \multirow{2}{*}{\textbf{Дистанция}} & \multirow{2}{*}{\textbf{Число статей}}  &  \multicolumn{2}{c|}{\textbf{Метрики}} \\ %\hline
\cline{5-6}
& & & & Точность & Макро-F1 \\ \hline \hline
русский & ru & 0 & 1,501,878 & 80.8 & 79.8\\
китайский (Тайвань) & zh-TW & 92.2 & 1,025,366 & 79.6 & 79.1\\
китайский & zh & 92.2 & 1,025,366 & 78.0 & 77.7\\
английский & en & 60.3 & 5,731,625 & 75.2 & 75.6\\
японский & ja & 93.3 & 1,124,097 & 72.4 & 70.5\\
словенский & sl & 4.2 & 162,453 & 70.3 & 69.0\\
шведский & sv & 59.5 & 3,763,579 & 70.2 & 69.6\\
малайский & ms & n/c & 320,631 & 68.9 & 67.7\\
итальянский & it & 45.8 & 1,466,064 & 68.8 & 68.0\\
индонезийский & id & 91.2 & 440,952 & 68.7 & 67.5\\
нидерландский & nl & 64.6 & 1,944,129 & 68.7 & 68.5\\
португальский & pt & 61.6 & 1,007,323 & 68.6 & 68.7\\
испанский & es & 51.7 & 1,480,965 & 68.2 & 68.0\\
датский & da & 66.2 & 240,436 & 67.8 & 66.7\\
французский & fr & 61.0 & 2,046,793 & 65.5 & 65.5\\
персидский & fa & 72.4 & 643,750 & 65.2 & 64.2\\
турецкий & tr & 86.2 & 316,969 & 64.5 & 62.4\\
вьетнамский & vi & 95.0 & 1,190,187 & 64.3 & 65.1\\
норвежский букмол & nb & 67.2 & 495,395 & 64.3 & 64.0\\
польский & pl & 5.1 & 1,303,297 & 64.2 & 62.2\\
азербайджанский & az & 87.7 & 138,538 & 63.9 & 63.1\\
каталанский & ca & 60.3 & 591,783 & 61.4 & 60.4\\
венгерский & hu & 87.2 & 437,984 & 61.3 & 60.0\\
иврит & he & 88.9 & 231,868 & 60.9 & 59.5\\
хинди & hi & 69.8 & 127,044 & 60.7 & 58.7\\
корейский & ko & 89.5 & 429,369 & 60.4 & 59.6\\
румынский & ro & 55.0 & 388,896 & 57.1 & 53.9\\
урду & ur & 66.7 & 140,939 & 56.4 & 55.9\\
арабский & ar & 86.5 & 619,692 & 56.2 & 55.7\\
каннада & kn & 90.8 & 23,844 & 56.1 & 53.0\\
филиппинский & tl & 91.9 & 80,992 & 55.0 & 51.3\\
телугу & te & 96.7 & 69,354 & 53.7 & 49.3\\
финский & fi & 88.9 & 445,606 & 53.3 & 51.3\\
бирманский & my & 86.0 & 39,823 & 52.5 & 49.7\\
африкаанс & af & 64.8 & 62,963 & 52.4 & 50.3\\
тамильский & ta & 94.7 & 118,119 & 52.4 & 50.1\\
немецкий & de & 64.5 & 2,227,483 & 52.2 & 51.6\\
албанский & sq & 69.4 & 74,871 & 51.5 & 47.2\\
латышский & lv & 49.1 & 88,189 & 49.6 & 48.4\\
малаялам & ml & 96.7 & 59,305 & 48.7 & 46.3\\
армянский & hy & 77.8 & 246,571 & 48.1 & 47.5\\
бенгальский & bn & 66.3 & 61,294 & 47.3 & 45.3\\
тайский & th & 89.5 & 127,010 & 46.5 & 44.9\\
греческий & el & 75.3 & 153,855 & 46.3 & 44.8\\
грузинский & ka & 96.0 & 124,694 & 39.2 & 38.1\\
яванский & jv & 95.4 & 54,964 & 38.7 & 37.1\\
монгольский & mn & 86.2 & 18,353 & 36.6 & 33.7\\
исландский & is & 68.9 & 45,873 & 32.6 & 29.9\\
суахили & sw & 95.1 & 45,806 & 31.0 & 28.0\\
валлийский & cy & 75.5 & 101,472 & 28.5 & 25.3\\
кхмерский & km & 97.1 & 6,741 & 16.1 & 8.6\\
амхарский & am & 86.6 & 14,375 & 12.1 & 5.0\\
\end{tabular}
}
\end{table*}

Результаты данной работы представлены в статье~\cite{rutopics}.

\underline{\textbf{Шестая глава}} посвящена прикладному использованию многозадачных моделей, описанных в данной работе.

Первой версией многозадачных моделей были модели с одним линейным слоем. Эти модели использовались в диалоговой платформе DREAM для замены облачных классификаторов реплик и классификаторов качества диалога от Amazon. Модели обучались на предсказаниях соответствующих моделей от Amazon, которые были сделаны в течение первого из двух конкурсов Alexa Prize, в котором автор работы принимал участие. Данные конкурса были разбиты в соотношении 90/8/2 на тренировочную, валидационную и тестовую выборку. Тестирование для задач классификации тональности, эмоций и токсичности проводилось на их оригинальных наборах тестовых данных (упомянутых во второй главе), тестирование для задач Cobot Topics, Cobot DialogAct Topics и Cobot DialogAct Intents - на тестовой подвыборке

Заметим, что разметка для всех используемых примеров была параллельной, т.к все классификаторы от Amazon отрабатывали для каждой из реплик. Эти модели поддерживали задачи классификации токсичности, тональности и эмоций, а также замену классификаторов Cobot Topics, Cobot DialogAct Topics, Cobot DialogAct Intents. Т.е использованный подход был аналогичен подходу «Жесткие независимые метки» из третьей главы, но без объединения меток. 

Результаты этой модели представлены ниже.

\begin{table}[htbp]
    \caption{Точность(взвешенный макро-F1) для многозадачной классификации для различных моделей. Однозадачные модели означает оригинальные модели, 6 в 1 - многозадачную модель с одним линейным слоем, обученную на аннотациях всех упомянутых в таблице классификаторов, 3 в 1(кобот) - многозадачную модель с одним линейным слоем, обученную только на аннотациях классификаторов cobot topics, cobot dialogact topics и cobot dialogact intents, 3 в 1(не кобот) - многозадачную модель с одним линейным слоем, обученную только на аннотациях остальных классификаторов(классификаторы эмоций, тональности и токсичности).}
    \label{mtldream:1}
    \centering
    \scalebox{0.65}{
    \begin{tabular}{c|c|c|c|c} 
    \hline
    \textbf{Модель} & \textbf{Однозадачные модели} & \textbf{6 в 1} & \textbf{3 в 1(Кобот)} & \textbf{3 в 1(Не кобот)}\\ 
    \hline
    cobot topics   & --- & 0.84~(0.83) & 0.82~(0.84) & --- \\
    \hline
    cobot dialogact topics  & --- & 0.76~(0.64) & 0.78~(0.66) & --- \\ 
    \hline
    cobot dialogact intents & --- & 0.69~(0.65) & 0.70~(0.67) & --- \\ 
    \hline
    классификация эмоций  & 0.92~(0.75) & 0.82~(0.60) & --- & 0.85~(0.67) \\
    \hline
    классификация тональности & 0.72~(0.68) & 0.60~(0.57) & --- & 0.66~(0.62) \\ 
    \hline
    классификация токсичности & 0.92~(0.60) & 0.92~(0.59) & --- & 0.93~(0.60)\\ 
    \hline
    \end{tabular}}
\end{table}

Аналогичная модель была обучена также для замены модуля от Amazon, оценивавшего качество диалога по пяти метрикам - ответ интересный, ответ развлекает пользователя, ответ понятный, ответ ошибочный, ответ по теме. Обученный на массиве диалогов из Alexa Prize Challenge 4, линейный слой предсказывал вектор из 5 величин от 0 до 1, метрик для мониторинга считалась средним квадратичным отклонением.

Использование данной модели дало СКО(среднеквадратичное отклонение) показателей, равное 0.31, на тестовой выборке.

Второй версией, работавшей в диалоговой платформе DREAM в течение продолжительного времени, являлась модель на основе PAL-BERT - модели, описанной в первой главе. Эта модель показала более высокое качество, чем модель с одним линейным слоем, даже если им подавались на вход одни и те же псевдоразмеченные данные Alexa Prize, как можно видеть из следующей таблицы.
\begin{table}[htbp]
\centering
\caption {Точность (f1) с диалоговой историей для многозадачной модели с 1 линейным слоем и PAL-BERT на псевдоразмеченных данных из Alexa Prize Challenge 4, оценка на «чистых» тестовых данных для не-коботовских задач и на псевдоразмеченных для коботовских задач.}
\label{mtldream:4}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c||c|c|c|c|} \hline
Задача | Модель & \begin{tabular}[c]{@{}l@{}}7 в 1\\ без истории\end{tabular} & \begin{tabular}[c]{@{}l@{}}7 в 1\\ без истории \\ жесткие метки\end{tabular} & \begin{tabular}[c]{@{}l@{}}7 в 1\\ PAL-BERT \\ без истории \\ жесткие метки\end{tabular} & \begin{tabular}[c]{@{}l@{}}Оригинальные\\ модели\end{tabular} \\

\hline
\hline
cobot topics & \textbf{0.819(0.8)} & 0.802(0.781) & 0.818(0.795) & 1(1) \\
\hline
cobot dialogact topics & 0.805(0.626) & 0.799(0.616) & \textbf{0.814(0.632)} & 1(1) \\
\hline
cobot dialogact intents & 0.752(0.635) & 0.745(0.626) & \textbf{0.767(0.63)} & 1(1) \\
\hline
Классификация эмоций & 0.401(0.245) & 0.72(0.641) & \textbf{0.788(0.754)} & 0.92(0.751) \\
\hline
Классификация тональности & 0.683(0.607) & 0.727(0.609) & \textbf{0.733(0.585)} & 0.721(0.681) \\
\hline
Классификация токсичности & 0.932(0.194) & 0.931(0.18) & \textbf{0.935(0.186)} & 0.922(0.596) \\
\hline
Классификация фактоидности & 0.805(0.806) & 0.816(0.814) & \textbf{0.829(0.831)} & 0.886(0.884) \\
\hline
\end{tabular}
}
\end{table}

Преимущество PAL-BERT над моделью с одним линейным слоем сохранилось и после добавления в обе модели обучения на псевдоразмеченных данных, не относящихся к Alexa Prize (а именно, на псевдоразмеченных обучающих данных для не-Коботовских задач). Это позволило приблизить метрики PAL-BERT к метрикам оригинальных моделей.

Результаты этой модели в финальной серии экспериментов с PAL-BERT представлены ниже.


\begin{table}[htbp]
\centering
\caption {Точность (f1) для оценки моделей в третьей серии экспериментов. Для не-Коботовских задач при оценке используются оригинальные тестовые наборы данных, для коботовских - тестовая часть разбиения данных.}
\label{mtldream:5}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c||c|c|c|c|c|c|c|} \hline
Модель/Задача & \begin{tabular}[c]{@{}l@{}}7 в 1\\ без истории\\базовый\end{tabular} &\begin{tabular}[c]{@{}l@{}}7 в 1\\ с историей\\базовый\end{tabular} & \begin{tabular}[c]{@{}l@{}}7 в 1\\ PAL-BERT\\без псевдоразметки\end{tabular} & \begin{tabular}[c]{@{}l@{}}7 в 1\\ PAL-BERT \\ псевдоразметка \\ только \\ неКоботовских \\данных\end{tabular} & \begin{tabular}[c]{@{}l@{}}7 в 1\\ PAL-BERT \\ полная \\псевдоразметка\end{tabular} & \begin{tabular}[c]{@{}l@{}}7 в 1\\ PAL-BERT \\псевдоразметка \\только \\для тональности\\ и фактоидности\end{tabular} & \begin{tabular}[c]{@{}l@{}} Оригинальные \\ модели \end{tabular}\\
\hline
\hline
cobot topics & \textbf{0.701(0.666)} & 0.568(0.533) & 0.833(0.81) & 0.831(0.808) & \textbf{0.863(0.843)} & 0.828(0.81) & 1(1) \\
\hline
cobot dialogact topics & 0.756(0.519) & 0.852(0.667) & \textbf{0.871(0.704)} & 0.869(0.704) & \textbf{0.906(0.804)} & 0.868(0.698) & 1(1) \\
\hline
cobot dialogact intents & 0.515(0.406) & 0.728(0.516) & \textbf{0.768(0.563)} & 0.765(0.561) & \textbf{0.828(0.685)} & 0.753(0.554) & 1(1) \\
\hline
Классификация эмоций & 0.905(0.88) & 0.917(0.883) & \textbf{0.927(0.906)} & 0.924(0.893) & 0.923(0.897) & 0.926(0.91) & 0.92(0.751) \\
\hline
Классификация тональности & 0.72(0.633) & 0.713(0.657) & \textbf{0.706(0.648)} & 0.727(0.659) & 0.713(0.647) & \textbf{0.754(0.664)} & 0.721(0.681) \\
\hline
Классификация токсичности & 0.938(0.199) & 0.932(0.21) & \textbf{0.928(0.253)} & 0.932(0.298) & 0.932(0.269) & \textbf{0.939(0.259)} & 0.922(0.596) \\
\hline
Классификация фактоидности & 0.789(0.809) & 0.794(0.817) & \textbf{0.834(0.831)} & 0.846(0.844) & \textbf{0.869(0.866)} & 0.854(0.853) & 0.886(0.884) \\
\hline
\end{tabular}
}
\end{table}


В дальнейшем данная модель была заменена трансформер-агностичной моделью, описанной в четвёртой главе. Это было связано в первую очередь с изменениями технических требований системы DREAM, в число которых вошла возможность быстро подставлять разные базовые модели в многозадачную архитектуру. Данные требования привели к переходу на параллельную архитектуру - а именно, трансформер-агностичную модель.


В трансформер-агностичную модель была добавлена классификация семантических интентов MIDAS и тематическая классификация DeepPavlov Topics, также были актуализированы либо дополнительно предобработаны данные для других задач. 

Трансформер-агностичность модели позволила заменить обычную модель дистиллированной, дополнительно выиграв в памяти.

\begin{table}[htbp]
\centering
\caption {Точность/взвешенный макро-F1) для оценки моделей в экспериментах с трансформер-агностичными моделями. Для не-Коботовских задач при оценке используются оригинальные тестовые наборы данных, для коботовских - тестовая часть разбиения данных. Мы обозначаем как distilbert модель \textit{distilbert-base-uncased}, как bert модель \textit{bert-base-uncased}. "Однозадачный" означает, что отдельная модель обучалась для каждой из задач.}
\label{tab:tr-ag-dream}% label всегда желательно идти после caption
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|} \hline
Задача/модель & Размер обучающей выборки &\begin{tabular}[c]{@{}l@{}}distilbert\\однозадачный\\с историей в MIDAS\end{tabular} & \begin{tabular}[c]{@{}l@{}}distilbert\\многозадачный\\с историей в MIDAS\end{tabular}  & \begin{tabular}[c]{@{}l@{}}distilbert\\многозадачный\\без истории в MIDAS\end{tabular} & \begin{tabular}[c]{@{}l@{}}bert\\однозадачный\\с историей в MIDAS\end{tabular} & \begin{tabular}[c]{@{}l@{}}bert\\многозадачный\\с историей в MIDAS\end{tabular}\\ \hline \hline
Классификация эмоций              & 39.5к & 70.47/70.30 & 68.18/67.86 & 67.59/67.32         & 71.48/71.16 & 67.27/67.23 \\ \hline
Классификация токсичности            & 1.62M & 94.53/93.64 & 93.84/93.5  & 93.86/93.41         & 94.54/93.15 & 93.94/93.4 \\ \hline
Классификация тональности            & 94k  & 74.75/74.63 & 72.55/72.21 & 72.22/71.9          & 75.95/75.88 & 75.65/75.62 \\ \hline
Классификация фактоидности           & 3.6k & 81.69/81.66 & 81.02/81.07 & 80.0/79.86          & 84.41/84.44 & 80.34/80.09 \\ \hline
Классификация интентов MIDAS          & 7.1k & 80.53/79.81 & 72.73/71.56~ & 73.69/73.26 & 82.3/82.03  & 77.01/76.38 \\ \hline
Тематическая классификация(DeepPavlov Topics) & 1.8M & 87.48/87.43 & 86.98/86.9  & 87.01/87.05         & 88.09/88.1  & 87.43/87.47 \\ \hline
Cobot topics~                  & 216k & 79.88/79.9  & 77.31/77.36 & 77.45/77.35         & 80.68/80.67 & 78.21/78.22 \\ \hline
Cobot dialogact topics~             & 127k & 76.81/76.71 & 76.92/76.79 & 76.8/76.7          & 77.02/76.97 & 76.86/76.74 \\ \hline
Cobot dialogact intents             & 318k & 77.07/77.7  & 76.83/76.76 & 76.65/76.57         & 77.28/77.72 & 76.96/76.89 \\ \hline
Средее для 9 задач                   & 4218k & 80.36/80.20    & 78.48/78.22 & 78.36/78.15         & 81.31/81.12  & 79.3/79.11 \\ \hline
Видеопамяти использовано, Мб               &    & 2418*9=21762 & 2420     & 2420             & 3499*9=31491 & 3501    \\ \hline
\end{tabular}
}
\end{table}

По сравнению с классификаторами в версии диалоговой платформы DREAM до внедрения многозадачной трансформер-агностичной модели (т.е основанной на модели PAL-BERT) многозадачная трансформер-агностичная модель дала экономию видеопамяти в 75 процентов, экономию оперативной памяти в 57 процентов и экономию времени на классификацию в 80-85 процентов.
 
Такая большая экономия времени на классификацию в основном связана с эффектом от трансформер-агностичности.\footnote{Хотя, конечно, роль сыграло и то, что для задачи классификации интентов MIDAS больше не требовался отдельный классификатор.} Если при использовании PAL-BERT для каждой задачи было необходимо получать предсказания многозадачной модели «с нуля», даже если они принимают одну и ту же фразу на вход, то при использовании многозадачной трансформер-агностичной модели появилась возможность один раз получить выход базового трансформера для этой фразы и дальше для всех других задачах, принимающих ее на вход, работать с этим выходом только линейными слоями, которые на порядки быстрее.

При этом, по сравнению с многозадачной моделью с одним линейным слоем, многозадачная трансформер-агностичная модель имеет ряд качественных преимуществ. Так, она не требует параллельной псевдоразметки под все задачи, что упрощает обучение модели. Проведение параллельной псевдоразметки может вносить нежелательные искажения в обучающую выборку, как показал опыт применения обученных на псевдоразмеченных данных моделей в диалоговой платформе DREAM. Помимо этого, многозадачная трансформер-агностичная модель может поддерживать не только многометочную, но и однометочную классификацию, а также ряд других задач, включающий в себя распознавание именованных сущностей или выбор между несколькими вариантами ответа.

Данные преимущества носили при выборе между этими двумя архитектурами многозадачных моделей решающий характер, несмотря на их сопоставимые показатели по расходу вычислительных ресурсов.

Трансформер-агностичная модель, помимо диалоговой платформы DREAM, была также внедрена в библиотеку DeepPavlov~\cite{dp_2023}(версия 1.1).

%Можно сослаться на свои работы в автореферате. Для этого в файле
%\verb!Synopsis/setup.tex! необходимо присвоить положительное значение
%счётчику \verb!\setcounter{usefootcite}{1}!. В таком случае ссылки на
%работы других авторов будут подстрочными.
%Изложенные в третьей главе результаты опубликованы в~\cite{vakbib1, vakbib2}.
%Использование подстрочных ссылок внутри таблиц может вызывать проблемы.

В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\urlstyle{rm}                               % ссылки URL обычным шрифтом
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
  \renewcommand{\bibname}{\large \bibtitleauthor}
  \nocite{*}
  \insertbiblioauthor           % Подключаем Bib-базы
  %\insertbiblioexternal   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
  % Цитирования.
  %  * Порядок перечисления определяет порядок в библиографии (только внутри подраздела, если `\insertbiblioauthorgrouped`).
  %  * Если не соблюдать порядок «как для \printbibliography», нумерация в `\insertbiblioauthor` будет кривой.
  %  * Если цитировать каждый источник отдельной командой --- найти некоторые ошибки будет проще.
  %
  %% authorvak
  \nocite{dream1_trudy}%
  %
  %% authorwos
  %
  %% authorscopus
  \nocite{pseudolabel}%
  \nocite{rutopics}
  \nocite{rumtl}
  \nocite{enmtl}
  %
  %% authorconf
  %
  %% authorother
  \nocite{dream1}%
  \nocite{dream2}%
  \nocite{Дуплякин_Дмитрий_Ондар_Ушаков_2021}

  \ifnumgreater{\value{usefootcite}}{0}{
    \begin{refcontext}[labelprefix={}]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  }{
  \ifnum \value{citeexternal}>0
    \begin{refcontext}[labelprefix=A]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  \else
    \ifnum \value{bibgrouped}>0
      \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
    \else
      \insertbiblioauthor      % Вывод всех работ автора
    \fi
  \fi
  %  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
 % \begin{refcontext}[labelprefix={}]    \insertbiblioexternal            % Вывод списка литературы, на которую ссылались в тексте автореферата
 % \end{refcontext}
  }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
\urlstyle{tt}                               % возвращаем установки шрифта ссылок URL
