\section*{Общая характеристика работы}

\newcommand{\actuality}{\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\underline{\textbf{\tasksTXT}}}
\newcommand{\novelty}{\underline{\textbf{\noveltyTXT}}}
\newcommand{\influence}{\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов \dots

%\underline{\textbf{Объем и структура работы.}} Диссертация состоит из~введения,
%четырех глав, заключения и~приложения. Полный объем диссертации
%\textbf{ХХХ}~страниц текста с~\textbf{ХХ}~рисунками и~5~таблицами. Список
%литературы содержит \textbf{ХХX}~наименование.

\section*{Содержание работы}
Во \underline{\textbf{введении}} обосновывается актуальность
исследований, проводимых в~рамках данной диссертационной работы,
приводится обзор научной литературы по изучаемой проблеме,
формулируется цель, ставятся задачи работы, излагается научная новизна
и практическая значимость представляемой работы. 

Во \underline{\textbf{второй главе}} разбираются нейросетевые архитектуры, имеющие отношение к данной диссертационной работе. Это архитектура Трансформер~\cite{vaswani_2017}, и это нейросетевая модель \textbf{BERT}~\cite{devlin_2018}, основанная на данной архитектуре.

\underline{\textbf{Третья глава}} посвящена многозадачным нейросетевым моделям. В первом разделе третьей главы дается определение многозадачного обучения, и дается деление всех имеющихся многозадачных архитектур на четыре типа - параллельные, модульные, иерархические и генеративно-состязательные. В следующих двух разделах приводится подробный обзор двух архитектур, на которых основывались последующие разделы данной диссертационной работы. А именно, модель \textbf{MT-DNN}~\cite{mtdnn}, имеющая параллельную архитектуру, и модель \textbf{PAL-BERT}~\cite{stickland_2019}, имеющая модульную архитектуру. 

\underline{\textbf{Четвертая глава}} посвящена обзору диалоговой платформы DREAM. Именно потребности этой платформы стимулировали создание многозадачных нейросетевых моделей, описанных в данной диссертационной работе, в этой платформе они и получали свое прикладное применение. В главе подробно рассмотрена структура этой диалоговой платформы, ее эволюция в течение конкурсов "Alexa Prize Socialbot Grand Challenge 3" и "Alexa Prize Socialbot Grand Challenge 4". Отдельное внимание уделено личному вкладу автора данной диссертационной работы в каждую из версий диалоговой платформы DREAM. 

Вкладом в первую версию являются аннотаторы Emotion Classification и Dialog Termination, навыки открытого домена TF-IDF Retrieval и генеративный навык, а также навыки закрытого домена Book Skill, Coronavirus Skill и (в своей основной части) Emotion Skill, которые были разработаны автором данной диссертационной работы самостоятельно. Первая версия диалоговой платформы DREAM описана в работах~\cite{dream1,dream1_trudy}.

К вкладу автора во вторую версию относится в первую очередь обучение и интеграция многозадачной нейросетевой модели с одним линейным слоем. Эти работы подробно описаны в \textbf{восьмой главе}.  Также к вкладу автора относятся обучение модели классификации интентов на основе семантических классов из набора данных MIDAS~\cite{midas}, разработка сценарного навыка Grounding Skill, значительное участие в разработке навыка закрытого домена Gossip, значительное улучшение сценарных навыков Book Skill(на основе Wikidata), Emotion Skill и аннотатора Intent Catcher. Помимо этого, автор принимал активное участие в отладке других навыков и аннотаторов на основе ежедневного анализа диалогов системы DREAM и ее личного тестирования. 

Вторая версия диалоговой системы DREAM описана в работе~\cite{dream2}. Технические решения, применяемые при работе над второй версии диалоговой системы DREAM, использовались также в сторонней работе автора, на которую получено свидетельство о депонировании ~\cite{Дуплякин_Дмитрий_Ондар_Ушаков_2021}. 


\underline{\textbf{Пятая глава}} посвящена многозадачным нейросетевым моделям с одним линейным слоем - простейшему типу многозадачных моделей. Для четырех задач из бенчмарка GLUE~\cite{wang_2018}(MNLI, QQP, SST, RTE) сравниваются различные способы псевдоразметки данных при обучении многозадачной модели с одним линейным слоем. Делается вывод, что если задачи достаточно сильно отличаются друг от друга, то самый лучший способ обучения таких моделей - получение вероятностей для каждого примера, не имеющего метки для той или иной задачи, по предсказаниям модели, учившейся только для этой задачи. Именно этот метод подробнее описан в \textbf{восьмой главе}.

\underline{\textbf{Шестая глава}} посвящена трансформер-агностичным многозадачным моделям. В частности, в этой главе приводится архитектура трансформер-агностичной многозадачной модели. Данная модель позволяет более гибко подстраиваться под каждую задачу,по сравнению с описанной в пятой главе, не требуя параллельной разметОтдельный раздел вы

Была 

Глава 6. Трансформер-агностичные модели . . . . . . . . . . . . . 69
6.0.1 Архитектура трансформер-агностичной многозадачной
модели . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6.0.2 Какие эксперименты не сработали . . . . . . . . . . . . . . 71
6.0.3 Преимущество трансформер-агностичной многозадачной
модели над многозадачной моделью с одним линейным
слоем . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73

6.1 Наборы данных . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
6.1.1 Классификация эмоций . . . . . . . . . . . . . . . . . . . . 75
6.1.2 Классификация тональности . . . . . . . . . . . . . . . . . 76
6.1.3 Классификация токсичности . . . . . . . . . . . . . . . . . 77
6.1.4 Классификация интентов и тематическая классификация 78
6.2 Настройки экспериментов . . . . . . . . . . . . . . . . . . . . . . . 78
6.3 Многозадачные и однозадачные модели - эксперименты на
полном наборе данных . . . . . . . . . . . . . . . . . . . . . . . . . 79
6.3.1 Эффект уменьшения размера обучающей выборки
(англоязычные данные) . . . . . . . . . . . . . . . . . . . . 81
6.3.2 Многоязычные многозадачные модели - эффект
кросс-языкового обучения . . . . . . . . . . . . . . . . . . . 83
6.3.3 Насколько помогает добавление англоязычных данных? . 85
6.4 Выводы и анализ результатов . . . . . . . . . . . . . . . . . . . . 86
\underline{\textbf{Седьмая глава}} расширяет работу, проделанную в \textbf{шестой главе}. В данной главе предлагается новый русскоязычный тематический набор данных - \texttt{YAQTopics}.  Данный набор состоит из 76 тематических классов и имеет более 500 тысяч примеров - триплетов "вопрос-ответ-суммаризованный ответ" из сервиса "Яндекс.Кью"~\cite{yandex_q}. Этот набор данных существенно превосходит другие существовавшие для него русскоязычные наборы данных для разговорной тематической классификации - как по числу примеров, так и по числу тематических классов. В главе использовалась однометочная часть этого набора. 

\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{|c||c|c|c|c|c|} \hline
\textbf{тип данных}  & \multicolumn{2}{c|}{\textbf{однометочные}} & \multicolumn{2}{c|}{\textbf{многометочные}} & \multirow{2}{*}{\textbf{равноразмерные}} \\
\cline{1-5}
\textbf{класс}  & \multicolumn{1}{c|}{все} & \multicolumn{1}{c|}{отвеченные} & \multicolumn{1}{c|}{все} & \multicolumn{1}{c|}{} & \\\hline \hline
\textbf{Размер набора данных} & 360,572 & 265,516 & 172,008 & 138,388 & 139,751\\ \hline
\textbf{Размер шестиклассового поднабора данных} & 23,992 & 16,857 & 22,238 & 20,610 & 7,335\\ \hline
\textit{Новости} & 945 & 605 & 912 & 718 & 354\\ \hline
\textit{Музыка} & 9,504 & 5,793 & 4,466 & 3,296 & 2,412\\ \hline
\textit{Еда, напитки и кулинария} & 5,729 & 4,734 & 14,117 & 11,101 & 2,503\\ \hline
\textit{Погода} & 889 & 480 & 218 & 143 & 179\\ \hline
\textit{Транспорт} & 2,432 & 1,622 & 1,936 & 1,391 & 655\\ \hline
\textit{Медиа и коммуникации} & 4,493 & 2,628 & 5,589 & 3,961 & 1,232\\ \hline
\end{tabular}
}
\caption{Размеры набора данных \texttt{YAQTopics} по классу и разбиению}
%\centering
\label{tab:rutopics:sizes2}
\end{table}


Показано при оценке на наборе данных \texttt{MASSIVE}~\cite{massive}, что данный набор данных хорошо подходит для тематической классификации(точность выше 85 процентов для русскоязычных моделей). При этом самой информативной частью данного набора, повзоляющей определить тему, являются вопросы, так как добавление к вопросам ответов или суммаризованных ответов не давало стойких улучшений, использование же ответов или суммаризованных ответов вместо вопросов давало ухудшения.



На примере обучения многоязычной модели \textit{bert-base-multilingual-cased}~\cite{devlin_2018} на данном наборе данных можно сделать вывод, что при переносе знаний с русскоязычного набора данных \texttt{YAQTopics} на другие языки (51 язык) качество модели для каждого языка хорошо коррелирует с приближенным размером предобучающей выборки для этого языка ( корреляция Спирмена 0.74 с пи-значением 5.1е-10). При этом корреляция качества модели для каждого языка с генеалогической близостью этого языка к русскому, вычисленной в соответствии с работой~\cite{langsim}, не является статистически значимой.

Результаты данной работы представлены в статье~\cite{rutopics}.

\underline{\textbf{Восьмая глава}} посвящена прикладному использованию многозадачных моделей, описанных в данной работе.

Первой версией многозадачных моделей были модели с одним линейным слоем. Эти модели использовались в диалоговой платформе DREAM для замены облачных классификаторов реплик и классификаторов качества диалога от Amazon. Модели обучались на предсказаниях соответствующих моделей от Amazon, которые были сделаны в течение первого из двух конкурсов Alexa Prize, в котором автор работы принимал участие. Заметим, что разметка для всех используемых примеров была параллельной, т.к все классификаторы от Amazon отрабатывали для каждой из реплик.

Второй версией, работавшей в диалоговой платформе DREAM в течение продолжительного времени, являлась модель на основе PAL-BERT. Эта модель показала более высокое качество, чем модель с одним линейным слоем

Второй

Данная архитектура была также внедрена в библиотеку DeepPavlov~\cite{dp_2023}(версия 1.1).
Глава 8. Использование в диалоговой платформе DREAM
многозадачных моделей . . . . . . . . . . . . . . . . . . . . . 91
8.1 Использование многозадачных моделей с одним линейным слоем 91
8.1.1 Использование многозадачных моделей с одним
линейным слоем для объединения и замены
классификаторов реплик . . . . . . . . . . . . . . . . . . . 91
8.1.2 Использование многозадачных моделей с одним
линейным слоем для замены модели для оценки диалога . 94
8.2 Использование модели PAL-BERT в диалоговой платформе
DREAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
8.3 Выводы . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
8.4 Использование многозадачной трансформер-агностичной
модели в диалоговой платформе DREAM . . . . . . . . . . . . . . 101
8.4.1 Экономия памяти GPU, CPU и быстродействия . . . . . . 1

Можно сослаться на свои работы в автореферате. Для этого в файле
\verb!Synopsis/setup.tex! необходимо присвоить положительное значение
счётчику \verb!\setcounter{usefootcite}{1}!. В таком случае ссылки на
работы других авторов будут подстрочными.
Изложенные в третьей главе результаты опубликованы в~\cite{vakbib1, vakbib2}.
Использование подстрочных ссылок внутри таблиц может вызывать проблемы.

В \underline{\textbf{четвертой главе}} приведено описание

В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

При использовании пакета \verb!biblatex! список публикаций автора по теме
диссертации формируется в разделе <<\publications>>\ файла
\verb!common/characteristic.tex!  при помощи команды \verb!\nocite!

\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\urlstyle{rm}                               % ссылки URL обычным шрифтом
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
  \renewcommand{\bibname}{\large \bibtitleauthor}
  \nocite{*}
  \insertbiblioauthor           % Подключаем Bib-базы
  %\insertbiblioexternal   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
  % Цитирования.
  %  * Порядок перечисления определяет порядок в библиографии (только внутри подраздела, если `\insertbiblioauthorgrouped`).
  %  * Если не соблюдать порядок "как для \printbibliography", нумерация в `\insertbiblioauthor` будет кривой.
  %  * Если цитировать каждый источник отдельной командой --- найти некоторые ошибки будет проще.
  %
  %% authorvak
  \nocite{vakbib1}%
  \nocite{vakbib2}%
  %
  %% authorwos
  \nocite{wosbib1}%
  %
  %% authorscopus
  \nocite{scbib1}%
  %
  %% authorconf
  \nocite{confbib1}%
  \nocite{confbib2}%
  %
  %% authorother
  \nocite{bib1}%
  \nocite{bib2}%

  \ifnumgreater{\value{usefootcite}}{0}{
    \begin{refcontext}[labelprefix={}]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  }{
  \ifnum \value{citeexternal}>0
    \begin{refcontext}[labelprefix=A]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  \else
    \ifnum \value{bibgrouped}>0
      \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
    \else
      \insertbiblioauthor      % Вывод всех работ автора
    \fi
  \fi
  %  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
  \begin{refcontext}[labelprefix={}]    \insertbiblioexternal            % Вывод списка литературы, на которую ссылались в тексте автореферата
  \end{refcontext}
  }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
\urlstyle{tt}                               % возвращаем установки шрифта ссылок URL
