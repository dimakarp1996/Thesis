\section*{Общая характеристика работы}

\newcommand{\actuality}{\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\underline{\textbf{\tasksTXT}}}
\newcommand{\novelty}{\underline{\textbf{\noveltyTXT}}}
\newcommand{\appropriation}{\underline{\textbf{\appropriationTXT}}}
\newcommand{\influence}{\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов \dots

%\underline{\textbf{Объем и структура работы.}} Диссертация состоит из~введения,
%четырех глав, заключения и~приложения. Полный объем диссертации
%\textbf{ХХХ}~страниц текста с~\textbf{ХХ}~рисунками и~5~таблицами. Список
%литературы содержит \textbf{ХХX}~наименование.
\section*{Содержание работы}
Во \underline{\textbf{введении}} обосновывается актуальность
исследований, проводимых в~рамках данной диссертационной работы,
приводится обзор научной литературы по изучаемой проблеме,
формулируется цель, ставятся задачи работы, излагается научная новизна
и практическая значимость представляемой работы. 

\underline{\textbf{Первая глава}} является обзорной. В этой главе дается определение нейросетевых методов машинного обучения для задач обработки естественного языка. Дается представление о различных базовых понятиях, таких, как метод обратного распространения ошибки, полносвязная архитектура и токенизация. 

Помимо этого, в первой главе разбираются нейросетевые архитектуры, имеющие отношение к данной диссертационной работе. В частности, разбирается архитектура Трансформер и нейросетевая модель {BERT}, основанная на данной архитектуре. 

Последний раздел главы посвящен многозадачным нейросетевым моделям. В нем дается определение многозадачного обучения, и дается деление всех имеющихся многозадачных архитектур на четыре типа - параллельные, модульные, иерархические и генеративно-состязательные. После этого в разделе приводится подробный обзор двух архитектур, на которых основывались последующие разделы данной диссертационной работы. А именно, модель {MT-DNN}, имеющая параллельную архитектуру, и модель {PAL-BERT}, имеющая модульную архитектуру. 

\underline{\textbf{Вторая глава}} посвящена обзору диалоговой платформы DREAM. В главе подробно рассмотрена структура этой диалоговой платформы, ее эволюция в течение конкурсов «Alexa Prize Socialbot Grand Challenge 3» и «Alexa Prize Socialbot Grand Challenge 4», в полуфинал которых вышла платформе.  Проводимые автором работы над этой платформой частично продолжали проводившуюся ранее автором работу над диалоговой моделью «с персоной»~\cite{Болотин_Карпов_Рашков_Шкурак_2019}.

%Именно потребности этой платформы стимулировали создание многозадачных нейросетевых моделей, описанных в данной диссертационной работе, в этой платформе они и получали свое прикладное применение. 
В главе показано, что диалоговая платформа DREAM пригодна для изучения прикладного применения многозадачных нейросетевых моделей.

\underline{\textbf{Третья глава}} посвящена многозадачным нейросетевым моделям с одним линейным слоем - простейшему типу многозадачных моделей. Для четырех задач из набора задач GLUE(MNLI, QQP, SST, RTE) сравниваются различные способы псевдоразметки данных при обучении многозадачной модели с одним линейным слоем. Делается вывод, что если задачи достаточно сильно отличаются друг от друга, то самый лучший способ обучения таких моделей - получение вероятностей для каждого примера, не имеющего метки для той или иной задачи, по предсказаниям модели, учившейся только для этой задачи. Именно этот метод подробнее описан в \textbf{шестой главе}.

\underline{\textbf{Четвёртая глава}} посвящена трансформер-агностичным многозадачным моделям. В частности, подробно описывается архитектура трансформер-агностичной многозадачной модели. Модель позволяет более гибко подстраиваться под каждую задачу. В отличие от модели, описанной в предыдущей главе, она не требует параллельной разметки. В главе подробно раскрыты преимущества данной модели над многозадачной моделью с одним линейным слоем.

Отдельный раздел содержит описание экспериментов, которые не сработали - модификация [CLS]-выхода модели BERT, использование задаче-специфичных тренируемых токенов и методов дистилляции при помощи приближения весов, а также эксперименты с разными методами сэмплирования. 

Исследование пригодности трансформер-агностичных многозадачных нейросетевых моделей для решения диалоговых задач проводилось для пяти задач - классификация эмоций, тональности, токсичности, интентов и тематическая классификация. Для этих задач использовалилсь параллельные русскоязычные и англоязычные наборы данных - CEDR и go\_emotions для классификации эмоций, RuReviews и DynaSent для классификации тональности, RuToxic и Wiki Talk для классификации токсичности и MASSIVE для классификации интентов и тематической классификации.

В главе показано на данных наборах данных, что как для русского, так и для английского языка предложенные многозадачные модели либо незначительно хуже однозадачных, либо не хуже. Выводы также проверены на наборе данных GLUE. Показано также, что при уменьшении обучающей выборки многозадачные модели с какого-то достаточно маленького размера данных, порядка 200-2000 примеров на задачу, начинают превосходить однозадачные модели. В особенности за счет не самых больших наборов данных из выборки. Помимо этого, показано, что добавление англоязычных данных к русскоязычным эффективнее делать для многоязычных моделях, объединяя данные для каждой заадчи, а не считая каждые такие данные отдельный задачей, при условии соответствия номенклатуры классов. И что само это добавление улучшает качество многоязычной модели на русскоязычных данных - от 1 до 5 процентов, чем изначально русскоязычных данных меньше, тем улучшение сильнее.


Результаты данной главы представлены в работах автора ~\cite{rumtl,enmtl}.

\underline{\textbf{Пятая глава}} расширяет работу, проделанную в \textbf{шестой главе}. В данной главе проводятся исследования русскоязычного тематического набора данных - YAQTopics.  Данный набор состоит из 76 тематических классов и имеет более 500 тысяч примеров - пар «вопрос-ответ» из сервиса «Яндекс.Кью»~\cite{yandex_q}. Этот набор данных существенно превосходит другие существовавшие до него русскоязычные наборы данных для разговорной тематической классификации - как по числу примеров, так и по числу тематических классов. В главе использовалась однометочная часть этого набора. 

Показано при оценке на наборе данных MASSIVE~\cite{massive}, что данный набор данных хорошо подходит для тематической классификации(точность выше 85 процентов для русскоязычных моделей). При этом самой информативной частью данного набора, повзоляющей определить тему, являются вопросы, так как добавление к вопросам ответов или суммаризованных ответов не давало стойких улучшений, использование же ответов или суммаризованных ответов вместо вопросов давало ухудшения.



На примере обучения многоязычной модели \textit{bert-base-multilingual-cased} на данном наборе данных можно сделать вывод, что при переносе знаний с русскоязычного набора данных YAQTopics на другие языки (51 язык) качество модели для каждого языка хорошо коррелирует с приближенным размером предобучающей выборки для этого языка ( корреляция Спирмена 0.773 с пи-значением 2.997e-11). При этом корреляция качества модели для каждого языка с генеалогической близостью этого языка к русскому не является статистически значимой.

Результаты данной работы представлены в статье~\cite{rutopics}.

\underline{\textbf{Шестая глава}} посвящена прикладному использованию многозадачных моделей, описанных в данной работе.

Первой версией многозадачных моделей были модели с одним линейным слоем. Эти модели использовались в диалоговой платформе DREAM для замены облачных классификаторов реплик и классификаторов качества диалога от Amazon. Модели обучались на предсказаниях соответствующих моделей от Amazon, которые были сделаны в течение первого из двух конкурсов Alexa Prize, в котором автор работы принимал участие. Заметим, что разметка для всех используемых примеров была параллельной, т.к все классификаторы от Amazon отрабатывали для каждой из реплик. Эти модели поддерживали сначала задачи классификации токсичности, тональности и эмоций, а также замену классификаторов Cobot Topics, Cobot DialogAct Topics, Cobot DialogAct Intents.

Второй версией, работавшей в диалоговой платформе DREAM в течение продолжительного времени, являлась модель на основе PAL-BERT. Эта модель показала более высокое качество, чем модель с одним линейным слоем, особенно после псевдоразметки данных. На этапе интеграции этой модели была добавлена классификация фактоидности.

В дальнейшем данная модель была заменена трансформер-агностичной моделью, описанной в четвёртой главе. В данную модель была добавлена классификация семантических интентов MIDAS и тематическая классификация DeepPavlov Topics. Также была произведена трансформация обучающей выборки для повышения качества модели, а трансформер-агностичность позволила заменить обычную модель дистиллированной, дополнительно выиграв в памяти.

По сравнению с классификаторами в версии диалоговой платформы DREAM до внедрения многозадачной трансформер-агностичной модели (т.е основанной на модели PAL-BERT) многозадачная трансформер-агностичная модель дала экономию видеопамяти в 75 процентов, экономию оперативной памяти в 57 процентов и экономию времени на классификацию в 80-85 процентов.
 
Такая большая экономия времени на классификацию в основном связана с эффектом от трансформер-агностичности\footnote{Хотя, конечно, роль сыграло и то, что для задачи классификации интентов MIDAS больше не требовался отдельный классификатор.}. Если при использовании PAL-BERT для каждой задачи было необходимо получать предсказания многозадачной модели «с нуля», даже если они принимают одну и ту же фразу на вход, то при использовании многозадачной трансформер-агностичной модели появилась возможность один раз получить выход базового трансформера для этой фразы и дальше для всех других задачах, принимающих ее на вход, работать с этим выходом только линейными слоями, которые на порядки быстрее. 

Трансформер-агностичная модель, помимо диалоговой платформы DREAM, была также внедрена в библиотеку DeepPavlov~\cite{dp_2023}(версия 1.1).

%Можно сослаться на свои работы в автореферате. Для этого в файле
%\verb!Synopsis/setup.tex! необходимо присвоить положительное значение
%счётчику \verb!\setcounter{usefootcite}{1}!. В таком случае ссылки на
%работы других авторов будут подстрочными.
%Изложенные в третьей главе результаты опубликованы в~\cite{vakbib1, vakbib2}.
%Использование подстрочных ссылок внутри таблиц может вызывать проблемы.

В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\urlstyle{rm}                               % ссылки URL обычным шрифтом
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
  \renewcommand{\bibname}{\large \bibtitleauthor}
  \nocite{*}
  \insertbiblioauthor           % Подключаем Bib-базы
  %\insertbiblioexternal   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
  % Цитирования.
  %  * Порядок перечисления определяет порядок в библиографии (только внутри подраздела, если `\insertbiblioauthorgrouped`).
  %  * Если не соблюдать порядок «как для \printbibliography», нумерация в `\insertbiblioauthor` будет кривой.
  %  * Если цитировать каждый источник отдельной командой --- найти некоторые ошибки будет проще.
  %
  %% authorvak
  \nocite{dream1_trudy}%
  %
  %% authorwos
  %
  %% authorscopus
  \nocite{pseudolabel}%
  \nocite{rutopics}
  \nocite{rumtl}
  \nocite{enmtl}
  %
  %% authorconf
  %
  %% authorother
  \nocite{dream1}%
  \nocite{dream2}%
  \nocite{Дуплякин_Дмитрий_Ондар_Ушаков_2021}

  \ifnumgreater{\value{usefootcite}}{0}{
    \begin{refcontext}[labelprefix={}]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  }{
  \ifnum \value{citeexternal}>0
    \begin{refcontext}[labelprefix=A]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  \else
    \ifnum \value{bibgrouped}>0
      \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
    \else
      \insertbiblioauthor      % Вывод всех работ автора
    \fi
  \fi
  %  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
 % \begin{refcontext}[labelprefix={}]    \insertbiblioexternal            % Вывод списка литературы, на которую ссылались в тексте автореферата
 % \end{refcontext}
  }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
\urlstyle{tt}                               % возвращаем установки шрифта ссылок URL
