\section*{Общая характеристика работы}

\newcommand{\actuality}{\underline{\textbf{\actualityTXT}}}
\newcommand{\progress}{\underline{\textbf{\progressTXT}}}
\newcommand{\aim}{\underline{{\textbf\aimTXT}}}
\newcommand{\tasks}{\underline{\textbf{\tasksTXT}}}
\newcommand{\novelty}{\underline{\textbf{\noveltyTXT}}}
\newcommand{\appropriation}{\underline{\textbf{\appropriationTXT}}}
\newcommand{\influence}{\underline{\textbf{\influenceTXT}}}
\newcommand{\methods}{\underline{\textbf{\methodsTXT}}}
\newcommand{\defpositions}{\underline{\textbf{\defpositionsTXT}}}
\newcommand{\reliability}{\underline{\textbf{\reliabilityTXT}}}
\newcommand{\probation}{\underline{\textbf{\probationTXT}}}
\newcommand{\contribution}{\underline{\textbf{\contributionTXT}}}
\newcommand{\publications}{\underline{\textbf{\publicationsTXT}}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

%Диссертационная работа была выполнена при поддержке грантов \dots

%\underline{\textbf{Объем и структура работы.}} Диссертация состоит из~введения,
%четырех глав, заключения и~приложения. Полный объем диссертации
%\textbf{ХХХ}~страниц текста с~\textbf{ХХ}~рисунками и~5~таблицами. Список
%литературы содержит \textbf{ХХX}~наименование.
\section*{Содержание работы}
Во \underline{\textbf{введении}} обосновывается актуальность
исследований, проводимых в~рамках данной диссертационной работы,
приводится обзор научной литературы по изучаемой проблеме,
формулируется цель, ставятся задачи работы, излагается научная новизна
и практическая значимость представляемой работы. 

\underline{\textbf{Первая глава}} является обзорной. В этой главе дается определение нейросетевых методов машинного обучения для задач обработки естественного языка. 
Дается представление о различных базовых понятиях, таких, как метод обратного распространения ошибки, полносвязная архитектура и токенизация. 

Нейросетевые методы машинного обучения - это методы, основанные на использовании искусственных нейронных сетей. В данной работе рассматривается класс искусственных нейронных сетей, представляющих собой совокупность слоев с функциями активации, таких, что подаваемые на вход данные проходят через различные слои по очереди, где каждый слой представляет собой многомерную функцию многих переменных. Итоговый выход нейронной сети подается в функцию потерь, после чего функция потерь оптимизируется методом обратного распространения ошибки. 

Метод обратного распространения ошибки - один из методов «обучения с учителем», то есть подход, при котором модель учится решать задачу, чтобы соответствовать набору примеров входных/выходных данных. Для определения того, насколько ответ, данный нейронной сетью, соответствует требуемому, вводится функция потерь. Далее выполняется поиск точки минимума функции потерь в пространстве параметров искусственной нейронной сети для данного набора примеров входных данных. 

Все самые главные достижения в области нейронных сетей в 21 веке были связаны именно с применением нейросетевых подходов. 

Одним из классических видов нейронных сетей являются полносвязные нейронные сети - сети, состоящие из полносвязных слоев. Будем называть полносвязным слоем с M нейронами взвешенную сумму значений входного вектора x размерности N, к каждому элементу которой затем применяется функция активации $\sigma(y)$:

\begin{equation}
\begin{split} 
\color{black}z=\sigma(y)\\
\color{black}y=W_{0}+W_{1}X
\end{split}
\label{nn:0}
\end{equation}
где $W_{1}$ - матрица весов (weights) полносвязного слоя размерности $N*M$, $W_{0}$ - матрица биасов (bias) полносвязного слоя размерности M, $\sigma$ - некая нелинейная функция активации.

где $i$ - индекс $K$-мерного вектора $z$.
Для регуляризации в таких слоях (как и в других, более сложных) применяется также дропаут, предложенный в~\cite{dropout}. При использовании данного метода некий процент элементов выходного вектора (как правило, 10-20\%) приравнивается к нулю. Такая техника мешает «переобучению» нейронной сети, улучшая тем самым ее обобщающую способность.

Токенизация текста - это его разбиение на элементарные единицы(токены) перед предобработкой. Один токен соответствует одному слову либо же одной более мелкой его единице(букве или слогу) в зависимости от метода токенизации. Для нижеописанного метода Word2Vec токеном является 1 слово, для нижеописанной архитектуры BERT - слово либо слог.

Помимо этого, в первой главе разбираются нейросетевые архитектуры, использовавшиеся в данной диссертационной работе. В частности, разбирается архитектура Трансформер и нейросетевая модель {BERT}, основанная на данной архитектуре.

Последний раздел главы посвящен многозадачным нейросетевым моделям. В нём даётся определение многозадачного обучения и приводится классификация многозадачных нейросетевых архитектур.

Многозадачное обучение - это метод разделения параметров между моделями, обучающимися выполнять несколько задач. Все имеющиеся многозадачные нейросетевые архитектуры можно разделить на четыре типа:
\begin{itemize}
\item[*] Параллельные архитектуры. Для данного типа архитектур одни и те же «общие» слои используются для примеров из каждой задачи, при этом выход «общих» слоев обрабатывается независимо своим специфическим слоем для каждой задачи. 
\item[*] Иерархические архитектуры. Для данного типа архитектур задачи обрабатываются зависимо друг от друга: так, результат классификации примера для одной из задач может использоваться при решении другой из задач как дополнительный входной параметр.
\item[*] Модульные архитектуры. Нейронная сеть в данных архитектурах делится на общие модули и задаче-специфичные модули, где общие модули имеют одни и те же веса для всех задач, а задаче-специфичные модули - свои веса для каждой из задач.
\item[*] Генеративно-состязательные архитектуры. Для данного типа архитектур генератор и дискриминатор обучаются совместно таким образом, что дискриминатор пытается предсказать, из какой задачи пример, по его выдаваемому генератором представлению. А генератор, соответственно, пытается сгенерировать такое представление, чтобы дискриминатор мог предсказать задачу как можно хуже. 
\end{itemize}
В завершении первого раздела приводится подробный обзор двух архитектур, на которых основывались последующие разделы данной диссертационной работы. А именно, модель {MT-DNN}, имеющая параллельную архитектуру, и модель {PAL-BERT}, имеющая модульную архитектуру. 

\underline{\textbf{Вторая глава}} посвящена обзору диалоговой платформы DREAM. В главе подробно рассмотрена структура этой диалоговой платформы, ее эволюция в течение конкурсов «Alexa Prize Socialbot Grand Challenge 3» и «Alexa Prize Socialbot Grand Challenge 4», в полуфинал которых вышла платформа.  Проводимые автором работы над этой платформой частично продолжали проводившуюся ранее автором работу над диалоговой моделью «с персоной»~\cite{Болотин_Карпов_Рашков_Шкурак_2019}.

%Именно потребности этой платформы стимулировали создание многозадачных нейросетевых моделей, описанных в данной диссертационной работе, в этой платформе они и получали свое прикладное применение. 
В главе показано, что диалоговая платформа DREAM пригодна для изучения прикладного применения многозадачных нейросетевых моделей.

\underline{\textbf{Третья глава}} посвящена многозадачным нейросетевым моделям с одним линейным слоем - простейшему типу многозадачных моделей. В таких моделях 



Для четырех задач из набора задач GLUE(MNLI, QQP, SST, RTE) сравниваются различные способы псевдоразметки данных при обучении многозадачной модели с одним линейным слоем. Делается вывод, что если задачи достаточно сильно отличаются друг от друга, то самый лучший способ обучения таких моделей - получение вероятностей для каждого примера, не имеющего метки для той или иной задачи, по предсказаниям модели, учившейся только для этой задачи. Именно этот метод подробнее описан в \textbf{четвертой главе}.

\underline{\textbf{Четвёртая глава}} посвящена трансформер-агностичным многозадачным моделям. В частности, подробно описывается архитектура трансформер-агностичной многозадачной модели. Модель позволяет более гибко подстраиваться под каждую задачу. В отличие от модели, описанной в предыдущей главе, она не требует параллельной разметки. В главе подробно раскрыты преимущества данной модели над многозадачной моделью с одним линейным слоем.

Отдельный раздел содержит описание экспериментов, которые не сработали - модификация [CLS]-выхода модели BERT, использование задаче-специфичных тренируемых токенов и методов дистилляции при помощи приближения весов, а также эксперименты с разными методами сэмплирования. 

Исследование пригодности трансформер-агностичных многозадачных нейросетевых моделей для решения диалоговых задач проводилось для пяти задач - классификация эмоций, тональности, токсичности, интентов и тематическая классификация. Для этих задач использовалилсь параллельные русскоязычные и англоязычные наборы данных - CEDR и go\_emotions для классификации эмоций, RuReviews и DynaSent для классификации тональности, RuToxic и Wiki Talk для классификации токсичности и MASSIVE для классификации интентов и тематической классификации.

В главе показано на данных наборах данных, что как для русского, так и для английского языка предложенные многозадачные модели либо незначительно хуже однозадачных, либо не хуже. Выводы также проверены на наборе данных GLUE. Показано также, что при уменьшении обучающей выборки многозадачные модели с какого-то достаточно маленького размера данных, порядка 200-2000 примеров на задачу, начинают превосходить однозадачные модели. В особенности за счет не самых больших наборов данных из выборки. Помимо этого, показано, что добавление англоязычных данных к русскоязычным эффективнее делать для многоязычных моделей, объединяя данные для каждой задачи, а не считая каждые такие данные отдельный задачей, при условии соответствия номенклатуры классов. И что само это добавление улучшает качество многоязычной модели на русскоязычных данных - от 1 до 5 процентов, чем изначально русскоязычных данных меньше, тем улучшение сильнее.

Результаты данной главы представлены в работах автора ~\cite{rumtl,enmtl}.

\underline{\textbf{Пятая глава}} расширяет работу, проделанную в \textbf{четвертой главе}. В данной главе проводятся исследования русскоязычного тематического набора данных - YAQTopics.  Данный набор состоит из 76 тематических классов и имеет более 500 тысяч примеров - пар «вопрос-ответ» из сервиса «Яндекс.Кью»~\cite{yandex_q}. Этот набор данных существенно превосходит другие существовавшие до него русскоязычные наборы данных для разговорной тематической классификации - как по числу примеров, так и по числу тематических классов. В главе использовалась однометочная часть этого набора. 

Показано при оценке на наборе данных MASSIVE~\cite{massive}, что данный набор данных хорошо подходит для тематической классификации(точность выше 85 процентов для русскоязычных моделей). При этом самой информативной частью данного набора, повзоляющей определить тему, являются вопросы, так как добавление к вопросам ответов или суммаризованных ответов не давало стойких улучшений, использование же ответов или суммаризованных ответов вместо вопросов давало ухудшения.



На примере обучения многоязычной модели \textit{bert-base-multilingual-cased} на данном наборе данных можно сделать вывод, что при переносе знаний с русскоязычного набора данных YAQTopics на другие языки (51 язык) качество модели для каждого языка хорошо коррелирует с приближенным размером предобучающей выборки для этого языка ( корреляция Спирмена 0.773 с пи-значением 2.997e-11). При этом корреляция качества модели для каждого языка с генеалогической близостью этого языка к русскому не является статистически значимой.

Результаты данной работы представлены в статье~\cite{rutopics}.

\underline{\textbf{Шестая глава}} посвящена прикладному использованию многозадачных моделей, описанных в данной работе.

Первой версией многозадачных моделей были модели с одним линейным слоем. Эти модели использовались в диалоговой платформе DREAM для замены облачных классификаторов реплик и классификаторов качества диалога от Amazon. Модели обучались на предсказаниях соответствующих моделей от Amazon, которые были сделаны в течение первого из двух конкурсов Alexa Prize, в котором автор работы принимал участие. Заметим, что разметка для всех используемых примеров была параллельной, т.к все классификаторы от Amazon отрабатывали для каждой из реплик. Эти модели поддерживали сначала задачи классификации токсичности, тональности и эмоций, а также замену классификаторов Cobot Topics, Cobot DialogAct Topics, Cobot DialogAct Intents.

Второй версией, работавшей в диалоговой платформе DREAM в течение продолжительного времени, являлась модель на основе PAL-BERT. Эта модель показала более высокое качество, чем модель с одним линейным слоем, особенно после псевдоразметки данных. На этапе интеграции этой модели была добавлена классификация фактоидности.

В дальнейшем данная модель была заменена трансформер-агностичной моделью, описанной в четвёртой главе. В данную модель была добавлена классификация семантических интентов MIDAS и тематическая классификация DeepPavlov Topics. Также была произведена трансформация обучающей выборки для повышения качества модели, а трансформер-агностичность позволила заменить обычную модель дистиллированной, дополнительно выиграв в памяти.

По сравнению с классификаторами в версии диалоговой платформы DREAM до внедрения многозадачной трансформер-агностичной модели (т.е основанной на модели PAL-BERT) многозадачная трансформер-агностичная модель дала экономию видеопамяти в 75 процентов, экономию оперативной памяти в 57 процентов и экономию времени на классификацию в 80-85 процентов.
 
Такая большая экономия времени на классификацию в основном связана с эффектом от трансформер-агностичности\footnote{Хотя, конечно, роль сыграло и то, что для задачи классификации интентов MIDAS больше не требовался отдельный классификатор.}. Если при использовании PAL-BERT для каждой задачи было необходимо получать предсказания многозадачной модели «с нуля», даже если они принимают одну и ту же фразу на вход, то при использовании многозадачной трансформер-агностичной модели появилась возможность один раз получить выход базового трансформера для этой фразы и дальше для всех других задачах, принимающих ее на вход, работать с этим выходом только линейными слоями, которые на порядки быстрее. 

Трансформер-агностичная модель, помимо диалоговой платформы DREAM, была также внедрена в библиотеку DeepPavlov~\cite{dp_2023}(версия 1.1).

%Можно сослаться на свои работы в автореферате. Для этого в файле
%\verb!Synopsis/setup.tex! необходимо присвоить положительное значение
%счётчику \verb!\setcounter{usefootcite}{1}!. В таком случае ссылки на
%работы других авторов будут подстрочными.
%Изложенные в третьей главе результаты опубликованы в~\cite{vakbib1, vakbib2}.
%Использование подстрочных ссылок внутри таблиц может вызывать проблемы.

В \underline{\textbf{заключении}} приведены основные результаты работы, которые заключаются в следующем:
\input{common/concl}

\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=false}}{} % не рекомендуется применять пакет микротипографики к автоматически генерируемому списку литературы
\urlstyle{rm}                               % ссылки URL обычным шрифтом
\ifnumequal{\value{bibliosel}}{0}{% Встроенная реализация с загрузкой файла через движок bibtex8
  \renewcommand{\bibname}{\large \bibtitleauthor}
  \nocite{*}
  \insertbiblioauthor           % Подключаем Bib-базы
  %\insertbiblioexternal   % !!! bibtex не умеет работать с несколькими библиографиями !!!
}{% Реализация пакетом biblatex через движок biber
  % Цитирования.
  %  * Порядок перечисления определяет порядок в библиографии (только внутри подраздела, если `\insertbiblioauthorgrouped`).
  %  * Если не соблюдать порядок «как для \printbibliography», нумерация в `\insertbiblioauthor` будет кривой.
  %  * Если цитировать каждый источник отдельной командой --- найти некоторые ошибки будет проще.
  %
  %% authorvak
  \nocite{dream1_trudy}%
  %
  %% authorwos
  %
  %% authorscopus
  \nocite{pseudolabel}%
  \nocite{rutopics}
  \nocite{rumtl}
  \nocite{enmtl}
  %
  %% authorconf
  %
  %% authorother
  \nocite{dream1}%
  \nocite{dream2}%
  \nocite{Дуплякин_Дмитрий_Ондар_Ушаков_2021}

  \ifnumgreater{\value{usefootcite}}{0}{
    \begin{refcontext}[labelprefix={}]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  }{
  \ifnum \value{citeexternal}>0
    \begin{refcontext}[labelprefix=A]
      \ifnum \value{bibgrouped}>0
        \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
      \else
        \insertbiblioauthor      % Вывод всех работ автора
      \fi
    \end{refcontext}
  \else
    \ifnum \value{bibgrouped}>0
      \insertbiblioauthorgrouped    % Вывод всех работ автора, сгруппированных по источникам
    \else
      \insertbiblioauthor      % Вывод всех работ автора
    \fi
  \fi
  %  \insertbiblioauthorimportant  % Вывод наиболее значимых работ автора (определяется в файле characteristic во второй section)
 % \begin{refcontext}[labelprefix={}]    \insertbiblioexternal            % Вывод списка литературы, на которую ссылались в тексте автореферата
 % \end{refcontext}
  }
}
\ifdefmacro{\microtypesetup}{\microtypesetup{protrusion=true}}{}
\urlstyle{tt}                               % возвращаем установки шрифта ссылок URL
