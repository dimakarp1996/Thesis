%% Согласно ГОСТ Р 7.0.11-2011:
%% 5.3.3 В заключении диссертации излагают итоги выполненного исследования, рекомендации, перспективы дальнейшей разработки темы.
%% 9.2.3 В заключении автореферата диссертации излагают итоги данного исследования, рекомендации и перспективы дальнейшей разработки темы.
%ПРОСТО СКОПИРОВАЛ ПОЛОЖЕНИЯ ВЫНОСИМЫЕ ДЛЯ ЗАЩИТЫ
\begin{enumerate}
\item Предложенные многозадачные трансформер-агностичные модели модели демонстрируют более высокую степень экономии памяти, чем однозадачные модели ( прирост числа параметров порядка 0.1\%). Эти модели были применены в диалоговой платформе {DREAM} и библиотеке DeepPavlov.
\item Предложенные многозадачные трансформер-агностичные модели демонстрируют уровень качества, приближающийся к качеству однозадачных моделей. Просадка среднего качества на исследованных наборах данных не превышает один процент.
\item Предложенный тематический набор русскоязычных данных {YAQTopics} пригоден для решения задачи тематической классификации разговорных данных.
\item Для задач с относительно маленьким объемом данных по сравнению с другими задачами и при этом достаточно большой степенью похожести на какие-то задачи, имеющиеся бОльший объем данных, многозадачные модели показывают прирост метрик. Этот прирост усиливается для каждой задачи, если обучающая выборка достаточно мала.
\item Если номенклатура классов для английского и русского языка соответствует друг другу, то добавление англоязычных данных к русскоязычным для той же задачи помогает поднять качество многоязычной нейросетевой модели. Чем меньше русскоязычных данных, тем прирост качества выше, как для таких многозадачных моделях, так и для однозадачных.
\item В условиях предыдущего пункта перенос знаний для многозадачных моделей происходит эффективнее, если данные добавляются в рамках одной задачи, а не в рамках разных.
\item Для многоязычных нейросетевых моделей качество переноса знаний на разные языки на тематических данных сильно коррелирует с размером предобучающей выборки для каждого языка, но при этом не коррелирует с генеалогической близостью этого языка к языку дообучения.
\item Трансформер-агностичность многозадачной нейросетевой модели повышает ее гибкость и удобство для стройки.
\item Предложенные схемы псевдоразметки данных повышают качество многозадачных моделей на тестовой выборке, но создает риск переобучения в случае возникновения дисбаланса классов.
  \item Исходный код всех упомянутых в предыдущих пунктах моделей и программ опубликован в открытом доступе, как набор данных YAQTopics, как часть библиотеки DeepPavlov и диалоговой платформы {DREAM} или же как дополнительные материалы к статьям.
\end{enumerate}
