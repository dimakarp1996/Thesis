\begin{frame}[noframenumbering,plain]
    \setcounter{framenumber}{1}
    \frametitle{Knowledge Transfer Between Tasks and Languages in the Encoder-agnostic Transformer-based Models}
    Authors: Dmitry Karpov, Vasily Konovalov
    MIPT, Dolgoprudny, Russia
    
\end{frame}

\begin{frame}{Encoder-agnostic models: task}
\begin{enumerate}
    \item Examine the knowledge transfer in the encoder-agnostic multitask neural models between different dialog tasks. 
    \item Estimate the dependence of this knowledge transfer from the training sample size
 \end{enumerate}
\end{frame}

\begin{frame}{Encoder-agnostic models: architecture}
Shortly: separate task-specific linear layer for every task, which is applied to the encoder output. 
Positive sides of this architecture:
\begin{itemize}
  \item Computational and architectural simplicity
  \item Expandability on the different task types
  \item No need in pseudolabeling
  \item Possibility of the quick encoder replacement
\end{itemize}
The architecture is integrated into the open-source DeepPavlov library.
\end{frame}

\begin{frame}{Encoder-agnostic models: data, sampling}
The principles of data selection:
\begin{itemize}
    \item Conversational tasks
    \item Matching classes for English and Russian languages
\end{itemize}
In the training stage, the batch from each dataset is sampled with the probability proportional to this training dataset size (plain sampling).
\end{frame}

\begin{frame}{Encoder-agnostic models: data }
\begin{itemize}
    \item For \textbf{emotion} classification -- Russian CEDR dataset, collected from different web sources, and English go\_emotions dataset, collected from the Reddit comments
We used seven Ekman types for emotions: anger, fear, sadness, joy, surprise, disgust, and neutral. 
    \item For \textbf{sentiment}  classification -- English DynaSent(r1) dataset, which consists of the sentences occurring in dialogs, and Russian RuReviews dataset, which consists of the reviews from the large Russian electronic shop. We used three classes: positive, negative, and neutral. 
\end{itemize}
\end{frame}

\begin{frame}{Encoder-agnostic models: data}
\begin{itemize}
    \item For the \textbf{toxicity} classification -- Russian comment dataset from «Dvach» (RuToxic) and English comment dataset from  «Wikipedia» (Wiki Talk). We used two classes: toxic and not toxic.
    \item For the \textbf{intent and topic} classification -- MASSIVE dataset, which consists of the user phrases addressed to the dialog system. We have used this dataset in the Russian variant and in the English variant. Every phrase from this dataset belongs to one of the 60 topics and one of the 18 intents. 
\end{itemize}
\end{frame}



\begin{frame}{Encoder-agnostic models: comparison to the single-task, English language}
\begin{table}[htbp]
\centering
\caption{Accuracy on the English data for the encoder-agnostic transformer-based model. English cased models trained on English data. Mode S stands for single-task, and mode M stands for multi-task.}
\scalebox{0.7}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Mode}} & \multirow{2}{*}{\textbf{Average}} & Emotions & Sentiment & Toxicity & Intents & Topics \\
& & & 39.4k & 80.5k & 127.6k & 11.5k & 11.5k \\ \hline \hline
\textit{\multirow{2}{*}{distilbert}} & S & \textbf{82.9} & \textbf{70.3} & 74.7 & 91.5 & \textbf{87.4} & \textbf{91.0} \\ %\hline
 & M  & 82.1 & 67.7 & \textbf{75.2} & 90.6 & 86.3 & 90.8  \\ \hline
\textit{\multirow{2}{*}{bert}} & S & \textbf{83.9} & \textbf{71.2} & 76.1 & \textbf{93.2} & \textbf{87.9} & \textbf{91.3} \\ %\hline
 & M &  83.0 & 69.0 & \textbf{76.5} & 91.4 & 87.1 & 91.2 \\ \hline
 \end{tabular}}
 \end{table}
 \end{frame}




 
 \begin{frame}{Encoder-agnostic models: comparison to the single-task, Russian language}
\begin{table}[htbp]
\centering
\caption {Accuracy on the Russian data for the encoder-agnostic transformer-based model. Russian cased models trained on Russian data. Mode S stands for single-task, and mode M stands for multi-task. Averaged by 3 runs.} 
\scalebox{0.7}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Mode}} & \multirow{2}{*}{\textbf{Average}} & Emotions & Sentiment & Toxicity & Intents & Topics \\
& & & 6.5k & 82.6k & 93.3k & 11.5k & 11.5k \\ \hline \hline
\textit{\multirow{2}{*}{distilrubert}} & S & \textbf{86.9} & \textbf{82.2} & \textbf{77.9} & \textbf{97.1} & \textbf{86.7} & 90.4  \\
 & M & 86.3 & 81.0 & 77.7 & 96.9 & 85.2 & \textbf{90.7} \\ \hline
\textit{\multirow{2}{*}{rubert}} & S & \textbf{86.5} & \textbf{80.9} & \textbf{78.0} & \textbf{97.2} & \textbf{86.2} & 90.0  \\ 
 & M & 86.2 & 80.5 & 77.6 & 96.8 & 85.3 & \textbf{90.5} \\ \hline
 \end{tabular}}
 \end{table}

\end{frame}


\begin{frame}{Encoder-agnostic models: cross-lingual knowledge transfer}
\begin{table}[htbp]
\caption{Accuracy on the Russian data for the encoder-agnostic transformer-based model. Multilingual cased models, batch size 160, plain sampling. Mode S stands for single-task, and mode M stands for multi-task. In the 'Training data' column, RU stands for the Russian language, 'RU+EN' means that Russian and English data are merged by task, and 'RU $\oplus$ EN' means that Russian and English tasks are treated as separate tasks.}
\scalebox{0.48}{
\begin{tabular}{|c|c|c||c|c|c|c|c|c|} \hline
Model & \begin{tabular}[c]{@{}l@{}}Training\\data\end{tabular} & Mode & Average & \begin{tabular}[c]{@{}l@{}}Emotions\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sentiment\end{tabular} & \begin{tabular}[c]{@{}l@{}}Toxicity\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intents\end{tabular} & \begin{tabular}[c]{@{}l@{}}Topics\end{tabular} \\
\hline \hline
\textit{distilbert-mult} & RU & S & \textbf{84.7} & 77.4 & \textbf{77.7} & \textbf{96.7} & \textbf{83.5} & 88.1 \\ 
% \hline
\textit{distilbert-mult} & RU & M & 84.3 & \textbf{78.1} & 76.8 & 96.5 & 81.9 & \textbf{88.2}  \\ 
 \hline
\textit{distilbert-mult} & RU+EN & S & \textbf{85.2} & \textbf{78.9} & \textbf{77.4} & \textbf{96.8} & \textbf{84.7} & \textbf{88.4}  \\ 
% \hline
\textit{distilbert-mult} & RU+EN & M & 84.5 & 77.9 & 76.6 & 96.5/94.5 & 82.9/76.5 & \textbf{88.4}/87.2  \\ 
% \hline
\textit{distilbert-mult} & RU $\oplus$ EN & M & 84.4 & 77.6 & 76.8 & 96.5 & 82.4 & 88.3 \\ 
\hline
\textit{bert-mult} & RU & S & 84.7 & 76.6 & \textbf{77.8} & \textbf{96.9} & \textbf{83.9} & 88.4  \\ 
% \hline
\textit{bert-mult} & RU & M & \textbf{84.8} & \textbf{78.4} & 76.3 & 96.8 & 83.7 & \textbf{89.0}  \\ 
 \hline
\textit{bert-mult} & RU+EN & S & \textbf{85.6} & 78.9 & \textbf{77.6} & \textbf{96.9} & \textbf{85.0} & \textbf{89.4}  \\ 
% \hline
\textit{bert-mult} & RU+EN & M & 85.2 & \textbf{79.2} & 76.4 & 96.7 & 84.3 & \textbf{89.4} \\ 
% \hline
\textit{bert-mult} & RU $\oplus$ EN & M & 85.0 & 78.3 & 77.1 & 96.7 & 84.0 & 89.1  \\ \hline
\end{tabular}}
\end{table}

\end{frame}


\begin{frame}{Encoder-agnostic models: the effect of training size reduction}
\begin{table}[htbp]
\caption{Average accuracy on the Russian data for \textit{distilbert-base-multilingual-cased}, batch size 160, plain sampling. 'S' stands for single-task mode, 'M' stands for multi-task mode, 'RU share' means the share of Russian training data, 'RU' means training only on the given percentage of Russian data, 'RU+EN' means training on the given percentage of Russian data with added full-size English data.}
\scalebox{0.99}{
\begin{tabular}[baseline={(0,2.1)}]{|l||c|c|c|c|}
\hline
RU & S & M & S & M \\
share & RU & RU & RU+EN & RU+EN \\ % Sizes are different for RU and RU+EN, so I don't give them here
\hline
3 & 57.0 & \textbf{65.9} & \textbf{71.8} & 70.7 \\ 
% \hline
5 & 58.4 & \textbf{70.3} & \textbf{75.0} & 74.2\\ 
% \hline
10 & \textbf{75.7} & 75.2 & \textbf{77.9} & 77.4\\ 
% \hline
15 & \textbf{77.7} & 77.2 & \textbf{79.7} & 78.9\\ 
% \hline
20 & 78.4 & \textbf{79.0} & \textbf{80.6} & 80.1 \\ 
% \hline
25 & 79.5 & \textbf{79.6} & \textbf{81.4} & 80.9 \\ 
% \hline
50 & \textbf{82.5} & 82.3 & \textbf{83.2} & 82.8 \\ 
% \hline
100 & \textbf{84.4} & 84.3 & \textbf{85.2} & 84.4 \\ \hline
\end{tabular}}
\end{table}
\end{frame}

\begin{frame}{Encoder-agnostic models: the effect of training sample reduction by task}
\label{fig:thresholds_acc_ru}
\begin{minipage}{0.5\textwidth}
\begin{tikzpicture}[baseline={(0,2.1)}]%[scale=2]
\begin{axis}[xlabel = RU share,
ylabel = Average accuracy,
legend pos= south east,
xtick={0,1,2,3,4,5,6,7,8},
xticklabels={2,3,5,10,15,20,25,50,100},
ymin=50,ymax=90,
legend cell align={left},
legend style={nodes={scale=0.7, transform shape}}
]
\addplot[color=orange,dotted, mark=*] coordinates {
(1, 57.02)
(2, 58.379999999999995)
(3, 75.66666666666667)
(4, 77.7)
(5, 78.36666666666667)
(6, 79.53333333333333)
(7, 82.5)
(8, 84.39999999999999)
};
\addlegendentry{S accuracy (RU)}
\addplot[color=red,solid,mark=*] coordinates {
(1, 65.88)
(2, 70.3)
(3, 75.23333333333333)
(4, 77.23333333333333)
(5, 79.0)
(6, 79.60000000000001)
(7, 82.3)
(8, 84.33333333333333)
};
\addlegendentry{M accuracy (RU)}
\addplot[color=cyan,dashed, mark=*] coordinates {
(1, 70.73333333333333)
(2, 74.23333333333333)
(3, 77.39999999999999)
(4, 78.89999999999999)
(5, 80.13333333333334)
(6, 80.93333333333332)
(7, 82.83333333333333)
(8, 84.36666666666667)
};
\addlegendentry{S accuracy (RU+EN)}
\addplot[color=blue,dashed,mark=*] coordinates {
(1, 71.8)
(2, 74.96666666666667)
(3, 77.93333333333334)
(4, 79.66666666666667)
(5, 80.56666666666666)
(6, 81.36666666666666)
(7, 83.23333333333333)
(8, 85.16666666666667)
};
\addlegendentry{M accuracy (RU+EN)}
\end{axis}%
\end{tikzpicture}

\end{minipage}

\end{frame}


\begin{frame}{Encoder-agnostic models: the effect of training sample reduction by task}

\pgfplotsset{every tick label/.append style={font=\tiny}}
\begin{figure}[!ht]
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}[scale=1]
\begin{axis}[xlabel = Number of training samples,
ylabel = Accuracy,
title=Average accuracy,
legend pos= south east,
width=\textwidth,
% width=10cm,
% height=10cm,
% xmin=2,
% xmax=100,
xtick={0,1,2,3,4,5,6,7},
xticklabels={6k,10k,21k,31k,41k,51k,103k,205k},
ymin=50,ymax=90,
legend cell align={left},
legend style={nodes={scale=0.5, transform shape}}
]
\addplot[color=orange,dotted, mark=*] coordinates {
(0, 57.02)
(1, 58.379999999999995)
(2, 75.66666666666667)
(3, 77.7)
(4, 78.36666666666667)
(5, 79.53333333333333)
(6, 82.5)
(7, 84.39999999999999)
};
\addlegendentry{Single-task accuracy (RU)}
\addplot[color=red,solid,mark=*] coordinates {
(0, 65.88)
(1, 70.3)
(2, 75.23333333333333)
(3, 77.23333333333333)
(4, 79.0)
(5, 79.60000000000001)
(6, 82.3)
(7, 84.33333333333333)};
\addlegendentry{Multi-task accuracy (RU)}
\end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}[scale=1]
\begin{axis}[xlabel = Number of training samples,
ylabel = Accuracy,
title=Emotion classification,
legend pos= south east,
width=\textwidth,
% width=10cm,
% height=10cm,
% xmin=2,
% xmax=100,
xtick={0,1,2,3,4,5,6,7},
xticklabels={196, 327, 655, 983, 1311, 1639, 3278, 6557},
ymin=45,ymax=80,
legend cell align={left},
legend style={nodes={scale=0.5, transform shape}}
]
\addplot[color=orange,dotted, mark=*] coordinates {
(0, 49.14)
(1, 48.260000000000005)
(2, 64.5)
(3, 66.10000000000001)
(4, 64.26666666666667)
(5, 66.96666666666665)
(6, 73.96666666666667)
(7, 76.46666666666667)
};
\addlegendentry{Single-task accuracy (RU)}
\addplot[color=red,solid,mark=*] coordinates {
(0, 62.61999999999999)
(1, 64.82000000000001)
(2, 68.73333333333333)
(3, 70.73333333333333)
(4, 71.36666666666667)
(5, 72.3)
(6, 75.0)
(7, 78.13333333333334)};
\addlegendentry{Multi-task accuracy (RU)}
\end{axis}%
\end{tikzpicture}
\end{subfigure}
\end{figure}
\end{frame}

\begin{frame}{Encoder-agnostic models: the effect of training sample reduction by task}
\pgfplotsset{every tick label/.append style={font=\tiny}}
\begin{figure}
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}[scale=1]
\begin{axis}[xlabel = Number of training samples,
ylabel = Accuracy,
title=Sentiment classification,
legend pos= south east,
width=\textwidth,
% width=10cm,
% height=10cm,
% xmin=2,
% xmax=100,
xtick={0,1,2,3,4,5,6,7},
xticklabels={2k, 4k, 8k, 12k, 17k, 21k, 41k, 83k},
ymin=65,ymax=80,
legend cell align={left},
legend style={nodes={scale=0.5, transform shape}}
]
\addplot[color=orange,dotted, mark=*] coordinates {
(0, 69.46000000000001)
(1, 71.02000000000001)
(2, 73.3)
(3, 73.96666666666665)
(4, 74.36666666666666)
(5, 75.13333333333334)
(6, 76.36666666666667)
(7, 77.16666666666667)
};
\addlegendentry{Single-task accuracy (RU)}
\addplot[color=red,solid,mark=*] coordinates {
(0, 68.99999999999999)
(1, 70.14)
(2, 71.46666666666665)
(3, 71.7)
(4, 72.96666666666667)
(5, 72.66666666666667)
(6, 74.56666666666666)
(7, 76.23333333333333)};
\addlegendentry{Multi-task accuracy (RU)}
\end{axis}%
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}[scale=1]
\begin{axis}[xlabel = Number of training samples,
ylabel = Accuracy,
title=Toxicity classification,
legend pos= south east,
width=\textwidth,
% width=10cm,
% height=10cm,
% xmin=2,
% xmax=100,
xtick={0,1,2,3,4,5,6,7},
xticklabels={3k, 5k, 9k, 14k, 19k, 23k, 47k, 93k},
ymin=90,ymax=100,
legend cell align={left},
legend style={nodes={scale=0.5, transform shape}}
]
\addplot[color=orange,dotted, mark=*] coordinates {
(0, 91.50000000000001)
(1, 92.70000000000002)
(2, 93.93333333333334)
(3, 94.80000000000001)
(4, 95.06666666666666)
(5, 95.36666666666667)
(6, 96.13333333333333)
(7, 96.7)
};
\addlegendentry{Single-task accuracy (RU)}
\addplot[color=red,solid,mark=*] coordinates {
(0, 91.24000000000001)
(1, 92.6)
(2, 93.96666666666665)
(3, 94.60000000000001)
(4, 95.0)
(5, 95.33333333333333)
(6, 96.0)
(7, 96.5)};
\addlegendentry{Multi-task accuracy (RU)}
\end{axis}%
\end{tikzpicture}
\end{subfigure}
\end{figure}
\end{frame}

\begin{frame}{Encoder-agnostic models: the effect of training sample reduction by task}
\pgfplotsset{every tick label/.append style={font=\tiny}}
\begin{figure}
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}
\begin{axis}[xlabel = Number of training samples,
ylabel = Accuracy,
title=Intent classification,
legend pos= south east,
width=\textwidth,
% width=10cm,
% height=10cm,
% xmin=2,
% xmax=100,
xtick={0,1,2,3,4,5,6,7},
xticklabels={0.3k, 0.6k, 1.2k, 1.7k, 2.3k, 2.9k, 5.8k, 11.5k},
ymin=25,ymax=85,
legend cell align={left},
legend style={nodes={scale=0.5, transform shape}}
]
\addplot[color=orange,dotted, mark=*] coordinates {
(0, 38.9)
(1, 29.920000000000005)
(2, 67.7)
(3, 72.16666666666667)
(4, 75.33333333333333)
(5, 76.60000000000001)
(6, 79.96666666666668)
(7, 83.46666666666665)
};
\addlegendentry{Single-task accuracy (RU)}
\addplot[color=red,solid,mark=*] coordinates {
(0, 42.64)
(1, 52.98)
(2, 64.03333333333335)
(3, 68.6)
(4, 73.5)
(5, 73.83333333333333)
(6, 79.5)
(7, 82.36666666666667)};
\addlegendentry{Multi-task accuracy (RU)}
\end{axis}
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\begin{tikzpicture}
\begin{axis}[xlabel = Number of training samples,
ylabel = Accuracy,
title=Topic classification,
legend pos= south east,
width=\textwidth,
% width=10cm,
% height=10cm,
% xmin=2,
% xmax=100,
xtick={0,1,2,3,4,5,6,7},
xticklabels={0.3k, 0.6k, 1.2k, 1.7k, 2.3k, 2.9k, 5.8k, 11.5k},
ymin=35,ymax=90,
legend cell align={left},
legend style={nodes={scale=0.5, transform shape}}
]
\addplot[color=orange,dotted, mark=*] coordinates {
(0, 36.16)
(1, 50.06)
(2, 78.80000000000001)
(3, 81.56666666666666)
(4, 82.86666666666666)
(5, 83.60000000000001)
(6, 86.13333333333334)
(7, 88.2)
};
\addlegendentry{Single-task accuracy (RU)}
\addplot[color=red,solid,mark=*] coordinates {
(0, 42.64)
(1, 52.98)
(2, 64.03333333333335)
(3, 68.6)
(4, 73.5)
(5, 73.83333333333333)
(6, 79.5)
(7, 82.36666666666667)};
\addlegendentry{Multi-task accuracy (RU)}
\end{axis}%
\end{tikzpicture}
\end{subfigure}
\end{figure}
\end{frame}

\begin{frame}{Encoder-agnostic models: conclusions}
\begin{enumerate}
\item Multi-task encoder-agnostic models almost match single-task ones by their metrics. Such multi-task models can even overcome single-task models for tasks where the dataset is small, for example, if the other similar task for the multi-task model has a sufficiently larger dataset. 
\item In case of the very small data size, multi-task encoder-agnostic models start strongly overcoming single-task ones. This effect also depends on the data size for the task. 
\item Adding English data to Russian improves metrics of the multilingual models, and the smaller the Russian data size is, the larger an improvement is(it can be up to several percent). It is true also for single-task models, with any validation language.
\end{enumerate}
\end{frame}

\begin{frame}{Thanks for your attention}
\end{frame}
